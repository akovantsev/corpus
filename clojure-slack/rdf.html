<!DOCTYPE html>
<head><title>rdf</title><meta charset="UTF-8" /><style type="text/css">body {
    font-family: Monaco, Menlo, Consolas, "Courier New", sans-serif;
    font-size: 12px;
    margin: 15px;
}
h2 {text-align: center}
pre,
code {
    font-family: Monaco, Menlo, Consolas, "Courier New", monospace;
    color: #333;
    line-break: anywhere;
}
a {
    line-break: anywhere;
}
code {
    /*line-height: 1.2;*/
    white-space: normal;
    color: #c25;
    background-color: #f7f7f9;
    border: 1px solid #e1e1e8;
}
pre {
    margin: .5rem 0 .2rem;
    font-size: .75rem;
    /*line-height: 1.15rem;*/
    background: #fbfaf8;
    padding: .5rem;
    word-break: normal;
    display: block;
    border: 1px solid rgba(0, 0, 0, .15);
    white-space: pre-wrap;
    word-wrap: break-word;
    border-radius: 4px;
}
/*span {*/
z {
    white-space: pre-wrap;
}

d {padding-right: 8px;}
h {padding-right: 16px;}

w, r {
    margin-right: 10px;
    padding-right: 8px;
    text-align: right;
}

d,h,w,r {
    background-color: lavender;
    margin-top: 1px;
    display: inline-table; /* prevents doubleclick selection spillover into neighbour elements */
}

h:hover,
d:hover {
    text-decoration: underline;
    color: blue;
    cursor: row-resize;
}
d {padding-right: 8px;}
h {padding-right: 16px;}

z {display: block;}

z:target > r,
z:target > w,
z:target > d,
z:target > h {
    background-color: aquamarine
}

#filters-container {
    position: fixed;
    left: 50px;
    z-index: 1;
}
#filters-container > input {
    /*column:*/
    display: block;
    outline-color: aquamarine;
}

y {
    cursor: pointer;
    text-decoration: none;
    color: blue;
    position: absolute;
    left: 3px;
}

y:hover {
    text-decoration: underline;
    color: aquamarine;
}


</style></head><body><style id="css-username">w {min-width: 120px;} r {min-width: 152px;}</style><style id="css-text-filter"></style><style id="css-date-filter"></style><style id="css-filter-override"></style><p id="filters-container"><input id="text-filter" onkeyup="debtextfilter(this.value)" placeholder="filter text" type="string" /><button hidden="hidden" id="date-filter" onclick="cleardatefilter()"></button></p><h2>#rdf</h2><pre><i>generated UTC: 2023-02-13 19:07</i><i>
latest data: <a href="https://clojurians-log.clojureverse.org/rdf/2023-02-11">https://clojurians-log.clojureverse.org/rdf/2023-02-11</a></i><i>
messages: 1955</i>
pro tips:
* Double click on text to filter by it. (doubleclick + cmd-f for extra points).
* Click on date to keep day visible regardless of filter.
* Click on time to keep hour visible regardless of filter.</pre><script>const textFilterInput = document.getElementById("text-filter");
const dateFilterInput = document.getElementById("date-filter");
const filterTextStyle = document.getElementById("css-text-filter");
const filterDateStyle = document.getElementById("css-date-filter");
const filterStyleOverride = document.getElementById("css-filter-override");



function textFilter2(text) {
    var style = '';
    if (text.trim() !== '') {
        text.split(' ').forEach(function (t) {
            if (t !== '') {
                style = style + "z:not([t*='" + t + "' i]) {display: none; opacity: 0.6}";
            }
        });
    }
    filterTextStyle.innerHTML = style;
}
function textFilter(text) {
    filterTextStyle.innerHTML = "";
    if (text.trim() !== '') {
        var sections = document.getElementsByTagName("g");
        var sectionsArr = Array.prototype.slice.call(sections);
        sectionsArr.forEach(function (s){
            var sid = s.getAttribute("id");
            text.split(' ').forEach(function (t) {
                if (t !== '') {
                    filterTextStyle.innerHTML += "\ng#" + sid + " > z:not([t*='" + t + "' i]) {display: none; opacity: 0.6}";
                }
            });
        })
    }
}
function textFilter3(text) {
    console.time("text search");
    filterTextStyle.innerHTML = "";
    if (text.trim() !== '') {
        const re = new RegExp( text, "i");
        var sections = document.getElementsByTagName("g");
        var sectionsArr = Array.prototype.slice.call(sections);
        sectionsArr.forEach(function (w){
            var zs = w.getElementsByTagName ("z");
            var zsArr = Array.prototype.slice.call(zs);
            var ids = zsArr
                .filter(function (el) {
                    return !re.test(el.innerText);
                })
                .map(function (el){
                    return el.getAttribute("id");
                });
            if (ids) {
                filterTextStyle.innerHTML += "\n#" + ids.join(",#") + " {display: none; opacity: 0.6}";
            }
        })
    }
    console.timeEnd("text search");
}


function filterSelection (e) {
    let sel = document.getSelection();
    let txt = sel.toString();
    textFilterInput.value = (txt || "");
    textFilter(txt);
}
function debounce1(callback, delay) {
    let timeout;
    return function(arg) {
        clearTimeout(timeout);
        timeout = setTimeout(callback, delay, arg);
    }
}

function keyUp (e) {
    if (e.code === "KeyF" && e.ctrlKey) {
        let txt = document.getSelection().toString();
        if (txt.length > 0) {
            textFilterInput.value = (txt || "");
            textFilter(txt);
        }
    }
}


function showDateTimes (el, datestr, hourstr) {
    dateFilterInput.innerText = "clear: " + datestr + " " + hourstr;
    dateFilterInput.hidden = false;
    let ids = [];
    const clicked = el.parentElement;  // el = z#id/t
    ids.push(clicked.id);
    // var idbefore, idafter;
    let cursor = clicked;
    function nextCursor (cursor) {
        const id = (cursor && cursor.id);
        const d = (cursor && cursor.children[1]);
        const h = (cursor && cursor.children[2])
        if (id) {
            ids.push(id); //includes 1st next-id not matching date.
        } else {
            cursor = null;
        }
        if (d && (d.textContent === datestr) && (!hourstr || (h && h.textContent.startsWith(hourstr)))) {
        } else {
            cursor = null;
        }
        return cursor;
    }
    while (cursor) {
        cursor = nextCursor(cursor.previousElementSibling);
    }

    cursor = clicked;

    while (cursor) {
        cursor = nextCursor(cursor.nextElementSibling);
    }


    var style1 = '';
    var style2 = '';
    var content;
    if (datestr.includes(" ")) { //hour
        content = " *";
    } else {
        content = "**";
    }
    ids.forEach(function (id) {
        if (id) {
            style1 = style1 + ", #" + id;
            style2 = style2 + ", #" + id + " h:after";
        }
    })
    if (style1) {
        style1 = style1.substring(1) + " {display: block !important}";
    }
    if (style2) {
        style2 = style2.substring(1) + " {content: \"" + content + "\"; position: absolute}"
    }
    filterDateStyle.innerHTML = style1 + "\n" + style2;
}

// https://developer.mozilla.org/en-US/docs/Web/API/Element/scrollIntoView
function filterDay (el) {
    showDateTimes(el, el.textContent);
    el.scrollIntoView({behavior: "smooth", block: "center", inline: "center"});
}

function filterHours (el) {
    showDateTimes(el, el.previousElementSibling.textContent, el.textContent.substring(0, 2));
    el.scrollIntoView({behavior: "smooth", block: "center", inline: "center"});
}

function forceShowSelected (el) {
    var id = el.parentElement.id
    if (id) {
        window.location.hash = "#" + id;
        filterStyleOverride.innerHTML = ""
            + "#" + id
            + " {display: block !important;}"
            + "#" + id + " h:after"
            + " {content: \"  #\"; position: absolute}";

    } else {
        window.location.hash = "";
        filterStyleOverride.innerHTML = "";
    }
}


function cleardatefilter () {
    dateFilterInput.hidden = true;
    dateFilterInput.innerText = "";
    filterDateStyle.innerHTML = "";
}

const debtextfilter = debounce1(textFilter, 200);

function doubleClick (e) {
    if (e.target.tagName !== "INPUT"
        && e.target.tagName !== "D"
        && e.target.tagName !== "H") {

        filterSelection();
        e.target.scrollIntoView({behavior: "auto", block: "center", inline: "start"});
    }
}

function onclick(e) {
    if (e.target.tagName === "Y") {
        e.preventDefault();
        forceShowSelected(e.target);
    } else if (e.target.tagName === "D") {
        e.preventDefault();
        filterDay(e.target);
    } else if (e.target.tagName === "H") {
        e.preventDefault();
        filterHours(e.target);
    }
}

document.onclick = onclick;
document.ondblclick = doubleClick;
document.onkeyup = keyUp;
textFilterInput.focus();
</script><div><g id="s0"><z id="t1440435105" t="rickmoynihan Cool simple_smile"><y>#</y><d>2015-08-24</d><h>16:51</h><w>rickmoynihan</w>Cool <b>simple_smile</b></z><z id="t1440435113" t="joelkuiper simple_smile"><y>#</y><d>2015-08-24</d><h>16:51</h><w>joelkuiper</w><b>simple_smile</b></z><z id="t1440435132" t="rickmoynihan anyone we should invite?"><y>#</y><d>2015-08-24</d><h>16:52</h><w>rickmoynihan</w>anyone we should invite?</z><z id="t1440435146" t="rickmoynihan I think [:attrs {:href &quot;/_/_/users/U050CTFRT&quot;}] said he was interested..."><y>#</y><d>2015-08-24</d><h>16:52</h><w>rickmoynihan</w>I think <a>@malcolmsparks</a> said he was interested...</z><z id="t1440435152" t="joelkuiper there were a couple of other people in the discussion yesterday, we could invite those!"><y>#</y><d>2015-08-24</d><h>16:52</h><w>joelkuiper</w>there were a couple of other people in the discussion yesterday, we could invite those!</z><z id="t1440437060" t="rickmoynihan sounds good"><y>#</y><d>2015-08-24</d><h>17:24</h><w>rickmoynihan</w>sounds good</z><z id="t1440585867" t="joelkuiper hey 😛"><y>#</y><d>2015-08-26</d><h>10:44</h><w>joelkuiper</w>hey <b>😛</b></z><z id="t1440585882" t="rickmoynihan yo"><y>#</y><d>2015-08-26</d><h>10:44</h><w>rickmoynihan</w>yo</z><z id="t1440585904" t="joelkuiper So based on the feedback by [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] I’ll be releasing a new version of YeSPARQL that is able to consume the ResultSet iteratively"><y>#</y><d>2015-08-26</d><h>10:45</h><w>joelkuiper</w>So based on the feedback by <a>@rickmoynihan</a> I’ll be releasing a new version of YeSPARQL that is able to consume the ResultSet iteratively</z><z id="t1440585918" t="rickmoynihan Awesome! simple_smile"><y>#</y><d>2015-08-26</d><h>10:45</h><w>rickmoynihan</w>Awesome! <b>simple_smile</b></z><z id="t1440585937" t="joelkuiper We’re still debating some future ideas, for example doing type conversion to native Clojure data structures, but that’ll be work for another version simple_smile"><y>#</y><d>2015-08-26</d><h>10:45</h><w>joelkuiper</w>We’re still debating some future ideas, for example doing type conversion to native Clojure data structures, but that’ll be work for another version <b>simple_smile</b></z><z id="t1440585948" t="rickmoynihan I probably won&apos;t get much time to try YeSPARQL this week"><y>#</y><d>2015-08-26</d><h>10:45</h><w>rickmoynihan</w>I probably won&apos;t get much time to try YeSPARQL this week</z><z id="t1440586026" t="joelkuiper Me neither, I have some other stuff to do as well simple_smile but it’s probably good to sleep on it some more as well!"><y>#</y><d>2015-08-26</d><h>10:47</h><w>joelkuiper</w>Me neither, I have some other stuff to do as well <b>simple_smile</b> but it’s probably good to sleep on it some more as well!</z><z id="t1440586037" t="joelkuiper It’s fun, first Clojure library that has some attention simple_smile"><y>#</y><d>2015-08-26</d><h>10:47</h><w>joelkuiper</w>It’s fun, first Clojure library that has some attention <b>simple_smile</b></z><z id="t1440586042" t="joelkuiper (that I wrote)"><y>#</y><d>2015-08-26</d><h>10:47</h><w>joelkuiper</w>(that I wrote)</z><z id="t1440586183" t="joelkuiper https://github.com/joelkuiper/yesparql#results-processing"><y>#</y><d>2015-08-26</d><h>10:49</h><w>joelkuiper</w><a href="https://github.com/joelkuiper/yesparql#results-processing" target="_blank">https://github.com/joelkuiper/yesparql#results-processing</a></z><z id="t1440586247" t="rickmoynihan simple_smile"><y>#</y><d>2015-08-26</d><h>10:50</h><w>rickmoynihan</w><b>simple_smile</b></z><z id="t1440586270" t="rickmoynihan Glad you&apos;re enjoying it - its great to see some activity other Grafter in the clojure RDF space"><y>#</y><d>2015-08-26</d><h>10:51</h><w>rickmoynihan</w>Glad you&apos;re enjoying it - its great to see some activity other Grafter in the clojure RDF space</z><z id="t1440586312" t="rickmoynihan I definitely think we should try and work together / as much as we can -- to create a healthy eco-system"><y>#</y><d>2015-08-26</d><h>10:51</h><w>rickmoynihan</w>I definitely think we should try and work together / as much as we can -- to create a healthy eco-system</z><z id="t1440586357" t="rickmoynihan maybe we need the ring of clojure RDF simple_smile"><y>#</y><d>2015-08-26</d><h>10:52</h><w>rickmoynihan</w>maybe we need the ring of clojure RDF <b>simple_smile</b></z><z id="t1440586377" t="joelkuiper yeah it’s silly, I never really liked Semantic Web. But I’m working in the biomedical space, and I use of a lot ontologies (like UMLS, Human Phenotype, Cancer Ontology), mostly for annotation of database resources with OpenAnnotations … and I kinda enjoy it. There are so many things that seem to have been reinvented outside Semantic Web as well a couple of times, but the whole thing has a nice holistic feel to it"><y>#</y><d>2015-08-26</d><h>10:52</h><w>joelkuiper</w>yeah it’s silly, I never really liked Semantic Web. But I’m working in the biomedical space, and I use of a lot ontologies (like UMLS, Human Phenotype, Cancer Ontology), mostly for annotation of database resources with OpenAnnotations … and I kinda enjoy it. There are so many things that seem to have been reinvented outside Semantic Web as well a couple of times, but the whole thing has a nice holistic feel to it</z><z id="t1440586417" t="rickmoynihan Have you seen this btw: https://commonsrdf.incubator.apache.org/"><y>#</y><d>2015-08-26</d><h>10:53</h><w>rickmoynihan</w>Have you seen this btw: <a href="https://commonsrdf.incubator.apache.org/" target="_blank">https://commonsrdf.incubator.apache.org/</a></z><z id="t1440586449" t="joelkuiper yeah, we could discuss what that would look like. Stupid thing is that there are two competing libraries (sesame and jena), with different tradeoffs"><y>#</y><d>2015-08-26</d><h>10:54</h><w>joelkuiper</w>yeah, we could discuss what that would look like. Stupid thing is that there are two competing libraries (sesame and jena), with different tradeoffs</z><z id="t1440586451" t="rickmoynihan Its a project to help enable basic interop between sesame/jena"><y>#</y><d>2015-08-26</d><h>10:54</h><w>rickmoynihan</w>Its a project to help enable basic interop between sesame/jena</z><z id="t1440586462" t="joelkuiper hmm, no that sounds very useful"><y>#</y><d>2015-08-26</d><h>10:54</h><w>joelkuiper</w>hmm, no that sounds very useful</z><z id="t1440586476" t="joelkuiper is it a bit mature? We might piggy back on to off that for YeSPARQL/Grafter"><y>#</y><d>2015-08-26</d><h>10:54</h><w>joelkuiper</w>is it a bit mature? We might piggy back on to off that for YeSPARQL/Grafter</z><z id="t1440586482" t="rickmoynihan I don&apos;t think they support it yet -- but I think sesame 4 and the next JENA will"><y>#</y><d>2015-08-26</d><h>10:54</h><w>rickmoynihan</w>I don&apos;t think they support it yet -- but I think sesame 4 and the next JENA will</z><z id="t1440586509" t="joelkuiper awesome, Jena just released 3.0 (after a long while) … so it might take some time 😛"><y>#</y><d>2015-08-26</d><h>10:55</h><w>joelkuiper</w>awesome, Jena just released 3.0 (after a long while) … so it might take some time <b>😛</b></z><z id="t1440586515" t="rickmoynihan well I was thinking a minimal clojure wrapper over it would be nice"><y>#</y><d>2015-08-26</d><h>10:55</h><w>rickmoynihan</w>well I was thinking a minimal clojure wrapper over it would be nice</z><z id="t1440586527" t="rickmoynihan JENA 3 might support it - not sure"><y>#</y><d>2015-08-26</d><h>10:55</h><w>rickmoynihan</w>JENA 3 might support it - not sure</z><z id="t1440586531" t="joelkuiper Over CommonsRDF?"><y>#</y><d>2015-08-26</d><h>10:55</h><w>joelkuiper</w>Over CommonsRDF?</z><z id="t1440586551" t="joelkuiper I saw no mention in the release notes, might have missed it … although they’re not very good at documenting some times 😛"><y>#</y><d>2015-08-26</d><h>10:55</h><w>joelkuiper</w>I saw no mention in the release notes, might have missed it … although they’re not very good at documenting some times <b>😛</b></z><z id="t1440586560" t="rickmoynihan maybe not then"><y>#</y><d>2015-08-26</d><h>10:56</h><w>rickmoynihan</w>maybe not then</z><z id="t1440586577" t="rickmoynihan but it has both JENA and Sesame contributors working together on it"><y>#</y><d>2015-08-26</d><h>10:56</h><w>rickmoynihan</w>but it has both JENA and Sesame contributors working together on it</z><z id="t1440586588" t="rickmoynihan (I think)"><y>#</y><d>2015-08-26</d><h>10:56</h><w>rickmoynihan</w>(I think)</z><z id="t1440586613" t="rickmoynihan ok going to go quiet for a while - I need to do some real work simple_smile"><y>#</y><d>2015-08-26</d><h>10:56</h><w>rickmoynihan</w>ok going to go quiet for a while - I need to do some real work <b>simple_smile</b></z><z id="t1440586618" t="joelkuiper Right, so writing a little library that takes Common RDF representations and maps it to Clojure data structures would solve the Jena/Sesame interop nicely I think"><y>#</y><d>2015-08-26</d><h>10:56</h><w>joelkuiper</w>Right, so writing a little library that takes Common RDF representations and maps it to Clojure data structures would solve the Jena/Sesame interop nicely I think</z><z id="t1440586621" t="joelkuiper same here 😉"><y>#</y><d>2015-08-26</d><h>10:57</h><w>joelkuiper</w>same here <b>😉</b></z><z id="t1440586683" t="rickmoynihan I think thats a long term thing -- right now our best hope of doing that is probably to use some common protocols across both jena/sesame core types (and also the commons-rdf ones)"><y>#</y><d>2015-08-26</d><h>10:58</h><w>rickmoynihan</w>I think thats a long term thing -- right now our best hope of doing that is probably to use some common protocols across both jena/sesame core types (and also the commons-rdf ones)</z><z id="t1440586718" t="rickmoynihan not sure on the details though -- might not achieve much in practice..."><y>#</y><d>2015-08-26</d><h>10:58</h><w>rickmoynihan</w>not sure on the details though -- might not achieve much in practice...</z><z id="t1440586736" t="joelkuiper well we could re-use that library across YeSPARQL and Grafter"><y>#</y><d>2015-08-26</d><h>10:58</h><w>joelkuiper</w>well we could re-use that library across YeSPARQL and Grafter</z><z id="t1440586739" t="joelkuiper that’s a win I’d say"><y>#</y><d>2015-08-26</d><h>10:58</h><w>joelkuiper</w>that’s a win I’d say</z><z id="t1440586759" t="joelkuiper and if Jena, Sesame switch to CommonsRDF it will only make it easier I guess"><y>#</y><d>2015-08-26</d><h>10:59</h><w>joelkuiper</w>and if Jena, Sesame switch to CommonsRDF it will only make it easier I guess</z><z id="t1440587068" t="joelkuiper [announcement] YeSPARQL 0.1.5 https://github.com/joelkuiper/yesparql , see https://github.com/joelkuiper/yesparql#results-processing for the most notable change"><y>#</y><d>2015-08-26</d><h>11:04</h><w>joelkuiper</w>[announcement] YeSPARQL 0.1.5 <a href="https://github.com/joelkuiper/yesparql" target="_blank">https://github.com/joelkuiper/yesparql</a>, see <a href="https://github.com/joelkuiper/yesparql#results-processing" target="_blank">https://github.com/joelkuiper/yesparql#results-processing</a> for the most notable change</z><z id="t1440595140" t="jamesaoverton [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] : Thanks for the link to Apache Commons RDF. First I’d heard of it."><y>#</y><d>2015-08-26</d><h>13:19</h><w>jamesaoverton</w><a>@rickmoynihan</a>: Thanks for the link to Apache Commons RDF. First I’d heard of it.</z><z id="t1440595621" t="rickmoynihan n/p [:attrs {:href &quot;/_/_/users/U09DVUASJ&quot;}] -- not really sure how useful it is yet though - as neither sesame or jena appear to actually implement those interfaces yet ... more something for the future simple_smile"><y>#</y><d>2015-08-26</d><h>13:27</h><w>rickmoynihan</w>n/p <a>@jamesaoverton</a> -- not really sure how useful it is yet though - as neither sesame or jena appear to actually implement those interfaces yet ... more something for the future <b>simple_smile</b></z><z id="t1440600340" t="joelkuiper How would grafter parse a https://www.w3.org/1999/02/22-rdf-syntax-ns#langString to a Clojure object?"><y>#</y><d>2015-08-26</d><h>14:45</h><w>joelkuiper</w>How would grafter parse a <a href="https://www.w3.org/1999/02/22-rdf-syntax-ns#langString" target="_blank">https://www.w3.org/1999/02/22-rdf-syntax-ns#langString</a> to a Clojure object?</z><z id="t1440600354" t="joelkuiper something like {:value … :lang …} ?"><y>#</y><d>2015-08-26</d><h>14:45</h><w>joelkuiper</w>something like {:value … :lang …} ?</z><z id="t1440600609" t="jamesaoverton That’s the shape used in EDN-LD: https://github.com/ontodev/edn-ld/blob/master/test/edn_ld/core_test.clj#L74"><y>#</y><d>2015-08-26</d><h>14:50</h><w>jamesaoverton</w>That’s the shape used in EDN-LD: <a href="https://github.com/ontodev/edn-ld/blob/master/test/edn_ld/core_test.clj#L74" target="_blank">https://github.com/ontodev/edn-ld/blob/master/test/edn_ld/core_test.clj#L74</a></z><z id="t1440600733" t="jamesaoverton … which is intentionally similar to JSON-LD."><y>#</y><d>2015-08-26</d><h>14:52</h><w>jamesaoverton</w>… which is intentionally similar to JSON-LD.</z><z id="t1440601118" t="joelkuiper hmm, may stick to that then"><y>#</y><d>2015-08-26</d><h>14:58</h><w>joelkuiper</w>hmm, may stick to that then</z><z id="t1440601249" t="joelkuiper what would you prefer to return as an URI, a java.net.URI or just a string?"><y>#</y><d>2015-08-26</d><h>15:00</h><w>joelkuiper</w>what would you prefer to return as an URI, a java.net.URI or just a string?</z><z id="t1440601322" t="jamesaoverton I work with OWL 2 a lot, which uses IRIs. The OWLAPI has its own IRI class: http://owlapi.sourceforge.net/javadoc/org/semanticweb/owlapi/model/IRI.html"><y>#</y><d>2015-08-26</d><h>15:02</h><w>jamesaoverton</w>I work with OWL 2 a lot, which uses IRIs. The OWLAPI has its own IRI class: <a href="http://owlapi.sourceforge.net/javadoc/org/semanticweb/owlapi/model/IRI.html" target="_blank">http://owlapi.sourceforge.net/javadoc/org/semanticweb/owlapi/model/IRI.html</a></z><z id="t1440601365" t="jamesaoverton Instead of choosing between java.net.URI and that IRI class, I just stick with strings."><y>#</y><d>2015-08-26</d><h>15:02</h><w>jamesaoverton</w>Instead of choosing between java.net.URI and that IRI class, I just stick with strings.</z><z id="t1440601384" t="jamesaoverton So, that’s the tradeoff I’ve chosen."><y>#</y><d>2015-08-26</d><h>15:03</h><w>jamesaoverton</w>So, that’s the tradeoff I’ve chosen.</z><z id="t1440601395" t="joelkuiper hmm, makes sense"><y>#</y><d>2015-08-26</d><h>15:03</h><w>joelkuiper</w>hmm, makes sense</z><z id="t1440601432" t="jamesaoverton I’ll be interested to hear how Grafter does it."><y>#</y><d>2015-08-26</d><h>15:03</h><w>jamesaoverton</w>I’ll be interested to hear how Grafter does it.</z><z id="t1440610611" t="joelkuiper This will sounds really stupid … but how do you get the Quads from a Jena Model? I’m iterating over Statements, but I have no idea how to get the graph names …"><y>#</y><d>2015-08-26</d><h>17:36</h><w>joelkuiper</w>This will sounds really stupid … but how do you get the Quads from a Jena Model? I’m iterating over Statements, but I have no idea how to get the graph names …</z><z id="t1440611047" t="jamesaoverton I think that com.hp.hpl.jena.rdf.model.Model is just a set of triples. So you’ll also need the name of graph that the model represents to make a quad."><y>#</y><d>2015-08-26</d><h>17:44</h><w>jamesaoverton</w>I think that com.hp.hpl.jena.rdf.model.Model is just a set of triples. So you’ll also need the name of graph that the model represents to make a quad.</z><z id="t1440611166" t="jamesaoverton To deal with multiple graphs, you use com.hp.hpl.jena.query.Dataset which contains a bunch of Models."><y>#</y><d>2015-08-26</d><h>17:46</h><w>jamesaoverton</w>To deal with multiple graphs, you use com.hp.hpl.jena.query.Dataset which contains a bunch of Models.</z><z id="t1440611334" t="jamesaoverton And remember that Jena has two main interfaces for triples: Model and Graph. Likewise there are two main interfaces for quads: Dataset and DatasetGraph. It’s easy to convert between the pairs of interfaces. DatasetGraph explicitly has quads."><y>#</y><d>2015-08-26</d><h>17:48</h><w>jamesaoverton</w>And remember that Jena has two main interfaces for triples: Model and Graph. Likewise there are two main interfaces for quads: Dataset and DatasetGraph. It’s easy to convert between the pairs of interfaces. DatasetGraph explicitly has quads.</z><z id="t1440611519" t="joelkuiper right but if I do an execConstruct on a QueryExecution I get a Model, can I get the graphs from that?"><y>#</y><d>2015-08-26</d><h>17:51</h><w>joelkuiper</w>right but if I do an execConstruct on a QueryExecution I get a Model, can I get the graphs from that?</z><z id="t1440611550" t="joelkuiper Or will it do something clever and return something else when the query has Quads? (like a Dataset?)"><y>#</y><d>2015-08-26</d><h>17:52</h><w>joelkuiper</w>Or will it do something clever and return something else when the query has Quads? (like a Dataset?)</z><z id="t1440611682" t="jamesaoverton Does this help? https://jena.apache.org/documentation/query/construct-quad.html"><y>#</y><d>2015-08-26</d><h>17:54</h><w>jamesaoverton</w>Does this help? <a href="https://jena.apache.org/documentation/query/construct-quad.html" target="_blank">https://jena.apache.org/documentation/query/construct-quad.html</a></z><z id="t1440611725" t="jamesaoverton I don’t see QueryExecution.execConstructDataset() in the JavaDocs..."><y>#</y><d>2015-08-26</d><h>17:55</h><w>jamesaoverton</w>I don’t see QueryExecution.execConstructDataset() in the JavaDocs...</z><z id="t1440611741" t="joelkuiper no, funnily neither execConstructQuads"><y>#</y><d>2015-08-26</d><h>17:55</h><w>joelkuiper</w>no, funnily neither execConstructQuads</z><z id="t1440611759" t="joelkuiper it’s weird though right, if I run execConstruct I would persume I’d be able to get quads somehow"><y>#</y><d>2015-08-26</d><h>17:55</h><w>joelkuiper</w>it’s weird though right, if I run execConstruct I would persume I’d be able to get quads somehow</z><z id="t1440611797" t="jamesaoverton Do those methods run? Are they just undocumented?"><y>#</y><d>2015-08-26</d><h>17:56</h><w>jamesaoverton</w>Do those methods run? Are they just undocumented?</z><z id="t1440611802" t="joelkuiper unsure"><y>#</y><d>2015-08-26</d><h>17:56</h><w>joelkuiper</w>unsure</z><z id="t1440611829" t="joelkuiper hang on, I’ll try"><y>#</y><d>2015-08-26</d><h>17:57</h><w>joelkuiper</w>hang on, I’ll try</z><z id="t1440611842" t="joelkuiper if execConstructDataset works I assume I can get quads from that somehow"><y>#</y><d>2015-08-26</d><h>17:57</h><w>joelkuiper</w>if execConstructDataset works I assume I can get quads from that somehow</z><z id="t1440611945" t="jamesaoverton The only thing I’ve done in the past is run UpdateProcessor.execute() on a DatasetGraph, then grab the stuff I want from the modified DatasetGraph. It works fine, but I don’t think it’s what you want."><y>#</y><d>2015-08-26</d><h>17:59</h><w>jamesaoverton</w>The only thing I’ve done in the past is run UpdateProcessor.execute() on a DatasetGraph, then grab the stuff I want from the modified DatasetGraph. It works fine, but I don’t think it’s what you want.</z><z id="t1440612112" t="joelkuiper hmm looks like they’re not in 3.0"><y>#</y><d>2015-08-26</d><h>18:01</h><w>joelkuiper</w>hmm looks like they’re not in 3.0</z><z id="t1440612116" t="joelkuiper seems like a recent addition"><y>#</y><d>2015-08-26</d><h>18:01</h><w>joelkuiper</w>seems like a recent addition</z><z id="t1440612550" t="joelkuiper there’s getDataset on QueryExecution, but I’m not sure how that works"><y>#</y><d>2015-08-26</d><h>18:09</h><w>joelkuiper</w>there’s getDataset on QueryExecution, but I’m not sure how that works</z><z id="t1440612636" t="jamesaoverton My guess would be that it gets you back the Dataset that you specified in QueryExecutionFactory.create()."><y>#</y><d>2015-08-26</d><h>18:10</h><w>jamesaoverton</w>My guess would be that it gets you back the Dataset that you specified in QueryExecutionFactory.create().</z><z id="t1440612829" t="joelkuiper right, so that won’t work for remote SPARQL endpoints"><y>#</y><d>2015-08-26</d><h>18:13</h><w>joelkuiper</w>right, so that won’t work for remote SPARQL endpoints</z><z id="t1440612850" t="joelkuiper so no way of getting quads from a SPARQL construct in Jena 3.0 😛"><y>#</y><d>2015-08-26</d><h>18:14</h><w>joelkuiper</w>so no way of getting quads from a SPARQL construct in Jena 3.0 <b>😛</b></z><z id="t1440614610" t="joelkuiper https://stackoverflow.com/questions/32234268/quads-from-a-construct-query-in-jena"><y>#</y><d>2015-08-26</d><h>18:43</h><w>joelkuiper</w><a href="https://stackoverflow.com/questions/32234268/quads-from-a-construct-query-in-jena" target="_blank">https://stackoverflow.com/questions/32234268/quads-from-a-construct-query-in-jena</a></z><z id="t1440614931" t="joelkuiper Initial try of getting type conversion to work in YeSPARQL"><y>#</y><d>2015-08-26</d><h>18:48</h><w>joelkuiper</w>Initial try of getting type conversion to work in YeSPARQL</z><z id="t1440614932" t="joelkuiper https://github.com/joelkuiper/yesparql/blob/feature/type-conversion/src/yesparql/sparql.clj#L110-L174"><y>#</y><d>2015-08-26</d><h>18:48</h><w>joelkuiper</w><a href="https://github.com/joelkuiper/yesparql/blob/feature/type-conversion/src/yesparql/sparql.clj#L110-L174" target="_blank">https://github.com/joelkuiper/yesparql/blob/feature/type-conversion/src/yesparql/sparql.clj#L110-L174</a></z><z id="t1440614939" t="joelkuiper unfortunately no quads 😉"><y>#</y><d>2015-08-26</d><h>18:48</h><w>joelkuiper</w>unfortunately no quads <b>😉</b></z><z id="t1440615200" t="quoll can always just use a SELECT"><y>#</y><d>2015-08-26</d><h>18:53</h><w>quoll</w>can always just use a SELECT</z><z id="t1440615397" t="joelkuiper sure, but that’s kinda hacky 😛"><y>#</y><d>2015-08-26</d><h>18:56</h><w>joelkuiper</w>sure, but that’s kinda hacky <b>😛</b></z><z id="t1440615694" t="joelkuiper seems like it’s being worked on, just a matter of waiting I guess"><y>#</y><d>2015-08-26</d><h>19:01</h><w>joelkuiper</w>seems like it’s being worked on, just a matter of waiting I guess</z><z id="t1440634010" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U0508P8EK&quot;}] : this is actually going to change - currently Grafter parses a lang string into a reified object - you can str it to get the string but if you want the tag out you have to (.getLanguage (-&gt;sesame-rdf-type my-lang-string-obj)) ... This bit is a bit broken - and we&apos;ve had a ticket to fix it for a while... it&apos;s a pretty simple fix though... The plan is to implement a Literal record type -- so basically a map like [:attrs {:href &quot;/_/_/users/U09DVUASJ&quot;}] says -- but with some polymorphic benefits that ensure it can coerce to the sesame (and maybe oneday jena) types properly... it&apos;ll have the string itself, the URI type and if its a string have a language keyword set e.g. :en /`:fr` (we use keywords for language tags already and it works well). Right now you can build lang strings with the (s &quot;hola&quot; :es)"><y>#</y><d>2015-08-27</d><h>00:06</h><w>rickmoynihan</w><a>@joelkuiper</a>: this is actually going to change - currently Grafter parses a lang string into a reified object - you can str it to get the string but if you want the tag out you have to <code>(.getLanguage (-&gt;sesame-rdf-type my-lang-string-obj))</code> ...  This bit is a bit broken - and we&apos;ve had a ticket to fix it for a while... it&apos;s a pretty simple fix though... 

The plan is to implement a <code>Literal</code> record type -- so basically a map like <a>@jamesaoverton</a> says -- but with some polymorphic benefits that ensure it can coerce to the sesame (and maybe oneday jena) types properly... it&apos;ll have the string itself, the URI type and if its a string have a language keyword set e.g. <code>:en</code>/`:fr` (we use keywords for language tags already and it works well).

Right now you can build lang strings with the <code>(s &quot;hola&quot; :es)</code></z><z id="t1440634183" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U0508P8EK&quot;}] : [:attrs {:href &quot;/_/_/users/U09DVUASJ&quot;}] : just reading your discussion -- remember SPARQL 1.1 doesn&apos;t really have FULL support for quads... i.e. you can&apos;t CONSTRUCT a quad... the pattern in the construct is the GRAPH... I personally think this is a real shame, as there are quad serialisation formats (e.g trig/trix etc...). This might be why you can&apos;t just get quads from a model"><y>#</y><d>2015-08-27</d><h>00:09</h><w>rickmoynihan</w><a>@joelkuiper</a>: <a>@jamesaoverton</a>: just reading your discussion -- remember SPARQL 1.1 doesn&apos;t really have FULL support for quads... i.e. you can&apos;t <code>CONSTRUCT</code> a quad... the pattern in the construct is the GRAPH... I personally think this is a real shame, as there are quad serialisation formats (e.g trig/trix etc...).  This might be why you can&apos;t just get quads from a model</z><z id="t1440634598" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U0508P8EK&quot;}] : just looking at your type coercion code -- Grafter also has both a Triple record and a Quad record... And I consider this a mistake... one we&apos;re going to undo I think you really only want (defrecord Quad [s p o g]) - then create a constructor function for triple which returns a Quad with a nil :g . Otherwise you&apos;ll get into a load of bother where #Quad { :s 1 :p 1 :o 1 :g nil } is not= to #Triple {:s 1 :p 1 :o 1}"><y>#</y><d>2015-08-27</d><h>00:16</h><w>rickmoynihan</w><a>@joelkuiper</a>: just looking at your type coercion code -- Grafter also has both a Triple record and a Quad record... And I consider this a mistake... one  we&apos;re going to undo

I think you really only want <code>(defrecord Quad [s p o g])</code> - then create a constructor function for triple which returns a Quad with a nil <code>:g</code>.  Otherwise you&apos;ll get into a load of bother where <code>#Quad { :s 1 :p 1 :o 1 :g nil }</code> is <code>not=</code> to <code>#Triple {:s 1 :p 1 :o 1}</code></z><z id="t1440634622" t="rickmoynihan simply because they&apos;re different types"><y>#</y><d>2015-08-27</d><h>00:17</h><w>rickmoynihan</w>simply because they&apos;re different types</z><z id="t1440635184" t="rickmoynihan obviously its easy enough to resolve but its a small pain... - yes there are still problems with this model where RDF semantics don&apos;t map directly onto clojure value semantics... However we recently had a discussion on the sesame developers mailing list where we convinced the core committer to change sesame&apos;s policy to use value semantics when testing equality - rather than RDF style equality where a quad of :s1 :p1 :o1 :g1 .equals a triple of :s1 :p1 :o1 . This should be coming in a future release... Not sure what Jena&apos;s policy is here"><y>#</y><d>2015-08-27</d><h>00:26</h><w>rickmoynihan</w>obviously its easy enough to resolve but its a small pain... - yes there are still problems with this model where RDF semantics don&apos;t map directly onto clojure value semantics... However we recently had a discussion on the sesame developers mailing list where we convinced the core committer to change sesame&apos;s policy to use value semantics when testing equality - rather than RDF style equality where a quad of <code>:s1 :p1 :o1 :g1</code> <code>.equals</code> a triple of <code>:s1 :p1 :o1</code>.  This should be coming in a future release... Not sure what Jena&apos;s policy is here</z><z id="t1440635191" t="rickmoynihan https://groups.google.com/forum/?hl=en&amp;amp;fromgroups#!searchin/sesame-devel/value$20semantics/sesame-devel/GEq5K-dTmM4/Y995GvkoAZsJ"><y>#</y><d>2015-08-27</d><h>00:26</h><w>rickmoynihan</w><a href="https://groups.google.com/forum/?hl=en&amp;amp;fromgroups#!searchin/sesame-devel/value$20semantics/sesame-devel/GEq5K-dTmM4/Y995GvkoAZsJ" target="_blank">https://groups.google.com/forum/?hl=en&amp;amp;fromgroups#!searchin/sesame-devel/value$20semantics/sesame-devel/GEq5K-dTmM4/Y995GvkoAZsJ</a></z><z id="t1440635749" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U09DVUASJ&quot;}] : early on in the first version of grafter - because I initially wanted a terse syntax for expressing triple patterns I also chose to represent URI&apos;s as strings - as URI&apos;s are the primary data type - in Grafter string literals have to built with the s function. - Again this is something I&apos;m going to change -- raw java strings should probably not automatically coerce into RDF - or if they do they should do so to RDF strings in the default language... any java URI type you might reasonably use should probably be made to work."><y>#</y><d>2015-08-27</d><h>00:35</h><w>rickmoynihan</w><a>@jamesaoverton</a>: early on in the first version of grafter - because I initially wanted a terse syntax for expressing triple patterns I also chose to represent URI&apos;s as strings - as URI&apos;s are the primary data type - in Grafter string literals have to built with the <code>s</code> function. - Again this is something I&apos;m going to change -- raw java strings should probably not automatically coerce into RDF - or if they do they should do so to RDF strings in the default language... any java URI type you might reasonably use should probably be made to work.</z><z id="t1440635751" t="rickmoynihan https://github.com/Swirrl/grafter-url"><y>#</y><d>2015-08-27</d><h>00:35</h><w>rickmoynihan</w><a href="https://github.com/Swirrl/grafter-url" target="_blank">https://github.com/Swirrl/grafter-url</a></z><z id="t1440636154" t="jamesaoverton Yeah, well… It’s something I’ve thought a lot about, and in the end I really like working with plain, literal EDN data everywhere I can. For what I do, IRIs are opaque, and I don’t need to get their protocol or query params. So I end up using strings for IRI, keywords for CURIEs/QNames, and maps for Literals."><y>#</y><d>2015-08-27</d><h>00:42</h><w>jamesaoverton</w>Yeah, well… It’s something I’ve thought a lot about, and in the end I really like working with plain, literal EDN data everywhere I can. For what I do, IRIs are opaque, and I don’t need to get their protocol or query params. So I end up using strings for IRI, keywords for CURIEs/QNames, and maps for Literals.</z><z id="t1440636203" t="jamesaoverton I work with EDN as long as possible, and only convert to other formats at the very end."><y>#</y><d>2015-08-27</d><h>00:43</h><w>jamesaoverton</w>I work with EDN as long as possible, and only convert to other formats at the very end.</z><z id="t1440636385" t="rickmoynihan yes - records can add noise - but I think you can actually override print-method to print them shorter e.g. you might be able to do #URI &quot;&quot;"><y>#</y><d>2015-08-27</d><h>00:46</h><w>rickmoynihan</w>yes - records can add noise - but I think you can actually override print-method to print them shorter e.g. you might be able to do <code>#URI &quot;&quot;</code></z><z id="t1440636575" t="jamesaoverton Then you need to provide a reader function to cast the string to that type. I’m glad EDN has typed literals, but I haven’t found that they’re worth the hassle."><y>#</y><d>2015-08-27</d><h>00:49</h><w>jamesaoverton</w>Then you need to provide a reader function to cast the string to that type. I’m glad EDN has typed literals, but I haven’t found that they’re worth the hassle.</z><z id="t1440636614" t="rickmoynihan yes I know - it definitely adds some friction"><y>#</y><d>2015-08-27</d><h>00:50</h><w>rickmoynihan</w>yes I know - it definitely adds some friction</z><z id="t1440636637" t="jamesaoverton I think that Transit has a native URI type, which would be more convenient."><y>#</y><d>2015-08-27</d><h>00:50</h><w>jamesaoverton</w>I think that Transit has a native URI type, which would be more convenient.</z><z id="t1440636681" t="rickmoynihan ooo interesting"><y>#</y><d>2015-08-27</d><h>00:51</h><w>rickmoynihan</w>ooo interesting</z><z id="t1440636689" t="jamesaoverton Yeah: https://github.com/cognitect/transit-clj"><y>#</y><d>2015-08-27</d><h>00:51</h><w>jamesaoverton</w>Yeah: <a href="https://github.com/cognitect/transit-clj" target="_blank">https://github.com/cognitect/transit-clj</a></z><z id="t1440636992" t="rickmoynihan what exactly do you use edn-ld for [:attrs {:href &quot;/_/_/users/U09DVUASJ&quot;}] ?"><y>#</y><d>2015-08-27</d><h>00:56</h><w>rickmoynihan</w>what exactly do you use edn-ld for <a>@jamesaoverton</a> ?</z><z id="t1440637101" t="jamesaoverton The library itself is a recent refactoring of some patterns I’ve developed over the last three years. So I’ve only used that particular code in a few projects so far, but I’ve used its predecessors in a larger number of projects."><y>#</y><d>2015-08-27</d><h>00:58</h><w>jamesaoverton</w>The library itself is a recent refactoring of some patterns I’ve developed over the last three years. So I’ve only used that particular code in a few projects so far, but I’ve used its predecessors in a larger number of projects.</z><z id="t1440637137" t="jamesaoverton And although I’m allowed to share those other projects, I’ve never had the time to clean them up and put them on GitHub..."><y>#</y><d>2015-08-27</d><h>00:58</h><w>jamesaoverton</w>And although I’m allowed to share those other projects, I’ve never had the time to clean them up and put them on GitHub...</z><z id="t1440637195" t="jamesaoverton But this is an example of some of the stuff that I do: https://github.com/jamesaoverton/MRO"><y>#</y><d>2015-08-27</d><h>00:59</h><w>jamesaoverton</w>But this is an example of some of the stuff that I do: <a href="https://github.com/jamesaoverton/MRO" target="_blank">https://github.com/jamesaoverton/MRO</a></z><z id="t1440637258" t="rickmoynihan cool"><y>#</y><d>2015-08-27</d><h>01:00</h><w>rickmoynihan</w>cool</z><z id="t1440637262" t="jamesaoverton The Clojure code takes a table from an SQL database that contains a very dense representation of MHC class restrictions, AKA some biology stuff."><y>#</y><d>2015-08-27</d><h>01:01</h><w>jamesaoverton</w>The Clojure code takes a table from an SQL database that contains a very dense representation of MHC class restrictions, AKA some biology stuff.</z><z id="t1440637332" t="jamesaoverton The goal is to convert that table into an OWL ontology. The ontology has several branches, with specific relationships."><y>#</y><d>2015-08-27</d><h>01:02</h><w>jamesaoverton</w>The goal is to convert that table into an OWL ontology. The ontology has several branches, with specific relationships.</z><z id="t1440637372" t="jamesaoverton There’s an Excel spreadsheet that specifies templates for different branches at different levels."><y>#</y><d>2015-08-27</d><h>01:02</h><w>jamesaoverton</w>There’s an Excel spreadsheet that specifies templates for different branches at different levels.</z><z id="t1440637405" t="jamesaoverton Then I read the source table and the template table, and zip them together into a sequence of maps defining OWL classes."><y>#</y><d>2015-08-27</d><h>01:03</h><w>jamesaoverton</w>Then I read the source table and the template table, and zip them together into a sequence of maps defining OWL classes.</z><z id="t1440637425" t="jamesaoverton Finally, I convert that EDN data into RDFXML file."><y>#</y><d>2015-08-27</d><h>01:03</h><w>jamesaoverton</w>Finally, I convert that EDN data into RDFXML file.</z><z id="t1440637475" t="rickmoynihan what makes it EDN, rather than just CLJ? simple_smile"><y>#</y><d>2015-08-27</d><h>01:04</h><w>rickmoynihan</w>what makes it EDN, rather than just CLJ? <b>simple_smile</b></z><z id="t1440637497" t="jamesaoverton There are really two parts. The first is ripping the source table into a number of branch-specific tables. Then I use a Java tool I wrote ROBOT to convert those tables to OWL."><y>#</y><d>2015-08-27</d><h>01:04</h><w>jamesaoverton</w>There are really two parts. The first is ripping the source table into a number of branch-specific tables. Then I use a Java tool I wrote ROBOT to convert those tables to OWL.</z><z id="t1440637518" t="jamesaoverton It’s not the best example, but it’s on GitHub."><y>#</y><d>2015-08-27</d><h>01:05</h><w>jamesaoverton</w>It’s not the best example, but it’s on GitHub.</z><z id="t1440637556" t="jamesaoverton To answer your question: I’m pretty convinced by this &quot;Data are better than Functions, are better than Macros” thing that Clojure people talk about."><y>#</y><d>2015-08-27</d><h>01:05</h><w>jamesaoverton</w>To answer your question: I’m pretty convinced by this &quot;Data are better than Functions, are better than Macros” thing that Clojure people talk about.</z><z id="t1440637618" t="jamesaoverton The MRO project doesn’t use the EDN-LD library because it’s for OWL and not just RDF. I haven’t figured out a general way to describe OWL in EDN, but I’ve been talking to Phil Lord about it."><y>#</y><d>2015-08-27</d><h>01:06</h><w>jamesaoverton</w>The MRO project doesn’t use the EDN-LD library because it’s for OWL and not just RDF. I haven’t figured out a general way to describe OWL in EDN, but I’ve been talking to Phil Lord about it.</z><z id="t1440637646" t="rickmoynihan yeah Phil and I have spoken in the past too"><y>#</y><d>2015-08-27</d><h>01:07</h><w>rickmoynihan</w>yeah Phil and I have spoken in the past too</z><z id="t1440637768" t="rickmoynihan what in the MRO example is data?"><y>#</y><d>2015-08-27</d><h>01:09</h><w>rickmoynihan</w>what in the MRO example is data?</z><z id="t1440637786" t="rickmoynihan thats not functions/macros/ just general clojure"><y>#</y><d>2015-08-27</d><h>01:09</h><w>rickmoynihan</w>thats not functions/macros/ just general clojure</z><z id="t1440637845" t="jamesaoverton The source table from SQL, and the Excel spreadsheet under src/mro . Those are converted to all the branch-specific CSV files at the top level."><y>#</y><d>2015-08-27</d><h>01:10</h><w>jamesaoverton</w>The source table from SQL, and the Excel spreadsheet under <code>src/mro</code>. Those are converted to all the branch-specific CSV files at the top level.</z><z id="t1440637941" t="rickmoynihan sorry I was meaning where is the data - in the EDN-LD Data &gt; Functions &gt; Macros, sense - presumably by that you meant that EDN-LD represents transformations by clojure data? Not symbols/functions/macros"><y>#</y><d>2015-08-27</d><h>01:12</h><w>rickmoynihan</w>sorry I was meaning where is the data - in the EDN-LD Data &gt; Functions &gt; Macros, sense - presumably by that you meant that EDN-LD represents transformations by clojure data?  Not symbols/functions/macros</z><z id="t1440638003" t="jamesaoverton The previous version of the MRO code had a separate function for each level of each branch."><y>#</y><d>2015-08-27</d><h>01:13</h><w>jamesaoverton</w>The previous version of the MRO code had a separate function for each level of each branch.</z><z id="t1440638090" t="jamesaoverton EDN-LD is mostly just conventions for representing RDF in EDN, and then some functions for working with those representations."><y>#</y><d>2015-08-27</d><h>01:14</h><w>jamesaoverton</w>EDN-LD is mostly just conventions for representing RDF in EDN, and then some functions for working with those representations.</z><z id="t1440638104" t="rickmoynihan and now you have a map - essentially in place of a cond ?"><y>#</y><d>2015-08-27</d><h>01:15</h><w>rickmoynihan</w>and now you have a map - essentially in place of a <code>cond</code>?</z><z id="t1440638216" t="jamesaoverton In the MRO example, there’s a sequence of maps representing templates, and a sequence of maps from the source table (SQL). Then the smarts are in the apply-template function, that applies each template to each row of the source table."><y>#</y><d>2015-08-27</d><h>01:16</h><w>jamesaoverton</w>In the MRO example, there’s a sequence of maps representing templates, and a sequence of maps from the source table (SQL). Then the smarts are in the <code>apply-template</code> function, that applies each template to each row of the source table.</z><z id="t1440638238" t="jamesaoverton So there’s a smaller number of higher-level functions, in the end, and I find it easier to reason about."><y>#</y><d>2015-08-27</d><h>01:17</h><w>jamesaoverton</w>So there’s a smaller number of higher-level functions, in the end, and I find it easier to reason about.</z><z id="t1440638281" t="rickmoynihan for what its worth - your MRO code seems broadly similar to grafter pipelines... In that you have a sequence of rows which you effectively process in row form... and then templatize. Is that fair?"><y>#</y><d>2015-08-27</d><h>01:18</h><w>rickmoynihan</w>for what its worth - your MRO code seems broadly similar to grafter pipelines...  In that you have a sequence of rows which you effectively process in row form... and then templatize.  Is that fair?</z><z id="t1440638293" t="rickmoynihan oh sorry you just said that"><y>#</y><d>2015-08-27</d><h>01:18</h><w>rickmoynihan</w>oh sorry you just said that</z><z id="t1440638320" t="jamesaoverton I agree with that."><y>#</y><d>2015-08-27</d><h>01:18</h><w>jamesaoverton</w>I agree with that.</z><z id="t1440638333" t="rickmoynihan Grafter basically works the same"><y>#</y><d>2015-08-27</d><h>01:18</h><w>rickmoynihan</w>Grafter basically works the same</z><z id="t1440638428" t="jamesaoverton In the MRO case, the Clojure code is table-to-table, then ROBOT (my Java tool) is used for the table-to-OWL part."><y>#</y><d>2015-08-27</d><h>01:20</h><w>jamesaoverton</w>In the MRO case, the Clojure code is table-to-table, then ROBOT (my Java tool) is used for the table-to-OWL part.</z><z id="t1440638460" t="jamesaoverton At the end of the day, pretty much all the code I write is a pipeline. :^)"><y>#</y><d>2015-08-27</d><h>01:21</h><w>jamesaoverton</w>At the end of the day, pretty much all the code I write is a pipeline. :^)</z><z id="t1440638490" t="rickmoynihan cool"><y>#</y><d>2015-08-27</d><h>01:21</h><w>rickmoynihan</w>cool</z><z id="t1440638506" t="rickmoynihan same for a lot of the stuff we do"><y>#</y><d>2015-08-27</d><h>01:21</h><w>rickmoynihan</w>same for a lot of the stuff we do</z><z id="t1440638518" t="jamesaoverton Some day I’ll publish a cleaner example :^)"><y>#</y><d>2015-08-27</d><h>01:21</h><w>jamesaoverton</w>Some day I’ll publish a cleaner example :^)</z><z id="t1440638522" t="rickmoynihan that and tools around them"><y>#</y><d>2015-08-27</d><h>01:22</h><w>rickmoynihan</w>that and tools around them</z><z id="t1440638526" t="rickmoynihan lol - same"><y>#</y><d>2015-08-27</d><h>01:22</h><w>rickmoynihan</w>lol - same</z><z id="t1440638561" t="jamesaoverton You made a good point about Quad equality above. I’ll think more about that."><y>#</y><d>2015-08-27</d><h>01:22</h><w>jamesaoverton</w>You made a good point about Quad equality above. I’ll think more about that.</z><z id="t1440638575" t="jamesaoverton It was good talking, but I’ve got to go now."><y>#</y><d>2015-08-27</d><h>01:22</h><w>jamesaoverton</w>It was good talking, but I’ve got to go now.</z><z id="t1440638587" t="jamesaoverton Later!"><y>#</y><d>2015-08-27</d><h>01:23</h><w>jamesaoverton</w>Later!</z><z id="t1440638601" t="rickmoynihan cool"><y>#</y><d>2015-08-27</d><h>01:23</h><w>rickmoynihan</w>cool</z><z id="t1440638602" t="rickmoynihan night"><y>#</y><d>2015-08-27</d><h>01:23</h><w>rickmoynihan</w>night</z><z id="t1440663840" t="joelkuiper so as far as I’m aware off there’s no real way to use SPARQL 1.1 to get Quads, but there might be in the future, so I’ll just leave it nil I guess."><y>#</y><d>2015-08-27</d><h>08:24</h><w>joelkuiper</w>so as far as I’m aware off there’s no real way to use SPARQL 1.1 to get Quads, but there might be in the future, so I’ll just leave it nil I guess.</z><z id="t1440663948" t="joelkuiper As far as type/data coercion … well I don’t really want to invent another class/type model for RDF. So I’ve chosen to represent results/triples as simple maps and records with strings for URI’s and json-ld-isch maps as best I can for the rest. If that’s not your cup of tea you can always just use the Jena objects 😉 and forget about the lazy-seq stuff 😛"><y>#</y><d>2015-08-27</d><h>08:25</h><w>joelkuiper</w>As far as type/data coercion … well I don’t  really want to invent another class/type model for RDF. So I’ve chosen to represent results/triples as simple maps and records with strings for URI’s and json-ld-isch maps as best I can for the rest. If that’s not your cup of tea you can always just use the Jena objects <b>😉</b> and forget about the lazy-seq stuff <b>😛</b></z><z id="t1440664095" t="joelkuiper If commonsRDF solves this problem I might consider implementing that, but for now it’s just too much of a mess to match the RDF semantics to Clojure, and the simplest thing I could think of was {:type “typeURI” :lang “@lang” :value “Jena coerced POJO”}"><y>#</y><d>2015-08-27</d><h>08:28</h><w>joelkuiper</w>If commonsRDF solves this problem I might consider implementing that, but for now it’s just too much of a mess to match the RDF semantics to Clojure, and the simplest thing I could think of was {:type “typeURI” :lang “@lang” :value “Jena coerced POJO”}</z><z id="t1440664141" t="joelkuiper or a string for uri, I may consider wrapping that in a java.net.URI. though, bit unsure still"><y>#</y><d>2015-08-27</d><h>08:29</h><w>joelkuiper</w>or a string for uri, I may consider wrapping that in a java.net.URI. though, bit unsure still</z><z id="t1440665806" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U0508P8EK&quot;}] : I&apos;d be tempted to go with a record for Quads and Literals... it makes writing and extending coercions easier (admitedly you can use a multimethod for this too -- but you&apos;ll probably just end up dispatching on type anyway (and you can always use a multimethod on a record too if you want)... Also multimethod dispatch is quite a bit slower than record dispatch... and you&apos;ll probably end up dispatching on millions of quads"><y>#</y><d>2015-08-27</d><h>08:56</h><w>rickmoynihan</w><a>@joelkuiper</a>: I&apos;d be tempted to go with a record for Quads and Literals... it makes writing and extending coercions easier (admitedly you can use a multimethod for this too -- but you&apos;ll probably just end up dispatching on type anyway (and you can always use a multimethod on a record too if you want)...  Also multimethod dispatch is quite a bit slower than record dispatch... and you&apos;ll probably end up dispatching on millions of quads</z><z id="t1440665826" t="rickmoynihan when users come to process results"><y>#</y><d>2015-08-27</d><h>08:57</h><w>rickmoynihan</w>when users come to process results</z><z id="t1440665887" t="joelkuiper well, Triples 😛 since there’s no real way of getting Quads 😉"><y>#</y><d>2015-08-27</d><h>08:58</h><w>joelkuiper</w>well, Triples <b>😛</b> since there’s no real way of getting Quads <b>😉</b></z><z id="t1440665944" t="joelkuiper So a Literal of [type, value, lang] -&gt; [String, Object, Keyword] or something?"><y>#</y><d>2015-08-27</d><h>08:59</h><w>joelkuiper</w>So a Literal of [type, value, lang] -&gt; [String, Object, Keyword] or something?</z><z id="t1440666110" t="rickmoynihan type =&gt; String, value =&gt; String, lang =&gt; Keyword"><y>#</y><d>2015-08-27</d><h>09:01</h><w>rickmoynihan</w>type =&gt; String, value =&gt; String, lang =&gt; Keyword</z><z id="t1440666167" t="joelkuiper why value as a string?"><y>#</y><d>2015-08-27</d><h>09:02</h><w>joelkuiper</w>why value as a string?</z><z id="t1440666182" t="rickmoynihan there might not be a way to query for a Quad -- but I think on the processing side it makes sense to have a quad -- because you can set it to non-nil yourself and serialise nquads etc easier"><y>#</y><d>2015-08-27</d><h>09:03</h><w>rickmoynihan</w>there might not be a way to query for a Quad -- but I think on the processing side it makes sense to have a quad -- because you can set it to non-nil yourself and serialise nquads etc easier</z><z id="t1440666205" t="joelkuiper Jena has excellent support for making sense of a lot of the XSD types into java objects"><y>#</y><d>2015-08-27</d><h>09:03</h><w>joelkuiper</w>Jena has excellent support for making sense of a lot of the XSD types into java objects</z><z id="t1440666235" t="rickmoynihan ahh ok sorry - by Object you mean Integer/Float/Double/Date etc..."><y>#</y><d>2015-08-27</d><h>09:03</h><w>rickmoynihan</w>ahh ok sorry - by Object you mean Integer/Float/Double/Date etc...</z><z id="t1440666237" t="joelkuiper that’s a fair point"><y>#</y><d>2015-08-27</d><h>09:03</h><w>joelkuiper</w>that’s a fair point</z><z id="t1440666238" t="rickmoynihan then yes I agree"><y>#</y><d>2015-08-27</d><h>09:03</h><w>rickmoynihan</w>then yes I agree</z><z id="t1440666242" t="joelkuiper yep simple_smile"><y>#</y><d>2015-08-27</d><h>09:04</h><w>joelkuiper</w>yep <b>simple_smile</b></z><z id="t1440666254" t="rickmoynihan definitely coerce the types out where you can"><y>#</y><d>2015-08-27</d><h>09:04</h><w>rickmoynihan</w>definitely coerce the types out where you can</z><z id="t1440666263" t="rickmoynihan but where you can&apos;t you&apos;ll need to fall back to string"><y>#</y><d>2015-08-27</d><h>09:04</h><w>rickmoynihan</w>but where you can&apos;t you&apos;ll need to fall back to string</z><z id="t1440666269" t="joelkuiper right, that’s what I do now"><y>#</y><d>2015-08-27</d><h>09:04</h><w>joelkuiper</w>right, that’s what I do now</z><z id="t1440666284" t="rickmoynihan thats what we&apos;re doing with grafter"><y>#</y><d>2015-08-27</d><h>09:04</h><w>rickmoynihan</w>thats what we&apos;re doing with grafter</z><z id="t1440666323" t="joelkuiper yeah I saw that simple_smile"><y>#</y><d>2015-08-27</d><h>09:05</h><w>joelkuiper</w>yeah I saw that <b>simple_smile</b></z><z id="t1440666326" t="rickmoynihan did you read the stuff I wrote here last about Triple/Quad equality etc?"><y>#</y><d>2015-08-27</d><h>09:05</h><w>rickmoynihan</w>did you read the stuff I wrote here last about Triple/Quad equality etc?</z><z id="t1440666358" t="joelkuiper yup, interesting stuff; I’ll probably change it to Quad for those reasons. Makes sense"><y>#</y><d>2015-08-27</d><h>09:05</h><w>joelkuiper</w>yup, interesting stuff; I’ll probably change it to Quad for those reasons. Makes sense</z><z id="t1440666379" t="rickmoynihan Its definitely a trade off -- but I think its the better one"><y>#</y><d>2015-08-27</d><h>09:06</h><w>rickmoynihan</w>Its definitely a trade off -- but I think its the better one</z><z id="t1440666418" t="joelkuiper could also just use a map I guess"><y>#</y><d>2015-08-27</d><h>09:06</h><w>joelkuiper</w>could also just use a map I guess</z><z id="t1440666521" t="rickmoynihan yes but it&apos;ll have the same issues -- i.e. (= {:s :s1 :p :p1 ::o o1 :g nil} {:s :s1 :p :p1 ::o o1}) =&gt; false"><y>#</y><d>2015-08-27</d><h>09:08</h><w>rickmoynihan</w>yes but it&apos;ll have the same issues -- i.e. <code>(= {:s :s1 :p :p1 ::o o1 :g nil} {:s :s1 :p :p1 ::o o1}) =&gt; false</code></z><z id="t1440666573" t="joelkuiper yeah, that’s true. it’s a silly problem 😛"><y>#</y><d>2015-08-27</d><h>09:09</h><w>joelkuiper</w>yeah, that’s true. it’s a silly problem <b>😛</b></z><z id="t1440666601" t="rickmoynihan its not a big deal - its just annoying -- and can cause hard to find bugs"><y>#</y><d>2015-08-27</d><h>09:10</h><w>rickmoynihan</w>its not a big deal - its just annoying -- and can cause hard to find bugs</z><z id="t1440666703" t="joelkuiper it’s one of those things that would be easy enough to solve with a custom Equals method though"><y>#</y><d>2015-08-27</d><h>09:11</h><w>joelkuiper</w>it’s one of those things that would be easy enough to solve with a custom Equals method though</z><z id="t1440666927" t="rickmoynihan yes but I think its more pragmatic to retain value semantics"><y>#</y><d>2015-08-27</d><h>09:15</h><w>rickmoynihan</w>yes but I think its more pragmatic to retain value semantics</z><z id="t1440666950" t="rickmoynihan even in java"><y>#</y><d>2015-08-27</d><h>09:15</h><w>rickmoynihan</w>even in java</z><z id="t1440667071" t="joelkuiper I’ve gone back and forth on that topic in Java projects; either can create hard to find bugs, especially if done inconsistently across developers 😛"><y>#</y><d>2015-08-27</d><h>09:17</h><w>joelkuiper</w>I’ve gone back and forth on that topic in Java projects; either can create hard to find bugs, especially if done inconsistently across developers <b>😛</b></z><z id="t1440667549" t="rickmoynihan yeah it definitely depends on what you&apos;re doing"><y>#</y><d>2015-08-27</d><h>09:25</h><w>rickmoynihan</w>yeah it definitely depends on what you&apos;re doing</z><z id="t1440667570" t="rickmoynihan but I think programming with values is generally better"><y>#</y><d>2015-08-27</d><h>09:26</h><w>rickmoynihan</w>but I think programming with values is generally better</z><z id="t1440670911" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U0508P8EK&quot;}] : any reason to use &quot;@en&quot; strings rather than :en keywords for language tags - (I know obviously that SPARQL and various serialisations represent them that way..."><y>#</y><d>2015-08-27</d><h>10:21</h><w>rickmoynihan</w><a>@joelkuiper</a>: any reason to use <code>&quot;@en&quot;</code> strings rather than <code>:en</code> keywords for language tags - (I know obviously that SPARQL and various serialisations represent them that way...</z><z id="t1440670945" t="rickmoynihan keywords share memory when you have lots of them"><y>#</y><d>2015-08-27</d><h>10:22</h><w>rickmoynihan</w>keywords share memory when you have lots of them</z><z id="t1440674085" t="joelkuiper no strong opinion, it’s closer to JSON-LD"><y>#</y><d>2015-08-27</d><h>11:14</h><w>joelkuiper</w>no strong opinion, it’s closer to JSON-LD</z><z id="t1440674086" t="joelkuiper which is nice"><y>#</y><d>2015-08-27</d><h>11:14</h><w>joelkuiper</w>which is nice</z><z id="t1440675202" t="joelkuiper switched it to keywords 😉 , probably the last I’ll work on it for the week at least!"><y>#</y><d>2015-08-27</d><h>11:33</h><w>joelkuiper</w>switched it to keywords <b>😉</b>, probably the last I’ll work on it for the week at least!</z><z id="t1440684124" t="rickmoynihan cool"><y>#</y><d>2015-08-27</d><h>14:02</h><w>rickmoynihan</w>cool</z><z id="t1440686708" t="joelkuiper https://github.com/joelkuiper/yesparql/tree/feature/type-conversion#a-note-on-lazyness simple_smile"><y>#</y><d>2015-08-27</d><h>14:45</h><w>joelkuiper</w><a href="https://github.com/joelkuiper/yesparql/tree/feature/type-conversion#a-note-on-lazyness" target="_blank">https://github.com/joelkuiper/yesparql/tree/feature/type-conversion#a-note-on-lazyness</a> <b>simple_smile</b></z><z id="t1440688094" t="quoll I want to think on it some more, but I agree that we should have: (not= {:s :s1 :p :p1 :o o1 :g nil} {:s :s1 :p :p1 :o o1})"><y>#</y><d>2015-08-27</d><h>15:08</h><w>quoll</w>I want to think on it some more, but I agree that we should have:
<code>(not= {:s :s1 :p :p1 :o o1 :g nil} {:s :s1 :p :p1 :o o1})</code></z><z id="t1440688106" t="quoll rather than a custom = function, I’d like to see another function that explicitly calls out that it’s handling some kind of equivalence instead"><y>#</y><d>2015-08-27</d><h>15:08</h><w>quoll</w>rather than a custom <code>=</code> function, I’d like to see another function that explicitly calls out that it’s handling some kind of equivalence instead</z><z id="t1440688151" t="quoll such as: (equiv {:s :s1 :p :p1 :o o1 :g nil} {:s :s1 :p :p1 :o o1})"><y>#</y><d>2015-08-27</d><h>15:09</h><w>quoll</w>such as: <code>(equiv {:s :s1 :p :p1 :o o1 :g nil} {:s :s1 :p :p1 :o o1})</code></z><z id="t1440691066" t="rickmoynihan I personally think its better to have one type - even if it has a nil field a lot of the time instead of two - for essentially the same thing"><y>#</y><d>2015-08-27</d><h>15:57</h><w>rickmoynihan</w>I personally think its better to have one type - even if it has a nil field a lot of the time instead of two - for essentially the same thing</z><z id="t1440691141" t="rickmoynihan I think its a good idea to have a custom equivalence function that implements RDF semantics"><y>#</y><d>2015-08-27</d><h>15:59</h><w>rickmoynihan</w>I think its a good idea to have a custom equivalence function that implements RDF semantics</z><z id="t1440691193" t="rickmoynihan so the not= &apos;s case won&apos;t arise in normal usage"><y>#</y><d>2015-08-27</d><h>15:59</h><w>rickmoynihan</w>so the <code>not=</code>&apos;s case won&apos;t arise in normal usage</z><z id="t1440691224" t="quoll on the second point, yes. Clojure needs to have = semantics that are separate to what is needed for RDF"><y>#</y><d>2015-08-27</d><h>16:00</h><w>quoll</w>on the second point, yes. Clojure needs to have = semantics that are separate to what is needed for RDF</z><z id="t1440691283" t="quoll for instance, I want to be able to say things like: (matches {:s s1 :p p1 :o o1} {:s s1 :p p1 :o o1 :g g1})"><y>#</y><d>2015-08-27</d><h>16:01</h><w>quoll</w>for instance, I want to be able to say things like:  <code>(matches {:s s1 :p p1 :o o1} {:s s1 :p p1 :o o1 :g g1})</code></z><z id="t1440691323" t="quoll because the triple in the first arg does match the triple-in-a-graph found in the second arg"><y>#</y><d>2015-08-27</d><h>16:02</h><w>quoll</w>because the triple in the first arg does match the triple-in-a-graph found in the second arg</z><z id="t1440691361" t="rickmoynihan quol - I think the best thing is to have a Quad record -- with a triple constructor - that essentially returns you a nil in the :g"><y>#</y><d>2015-08-27</d><h>16:02</h><w>rickmoynihan</w>quol - I think the best thing is to have a Quad record -- with a triple constructor - that essentially returns you a <code>nil</code> in the <code>:g</code></z><z id="t1440691395" t="rickmoynihan so (matches (triple :s1 :p1 :o1) (quad :s1 :p1 :o1 :g1) =&gt; true"><y>#</y><d>2015-08-27</d><h>16:03</h><w>rickmoynihan</w>so <code>(matches (triple :s1 :p1 :o1) (quad :s1 :p1 :o1 :g1) =&gt; true</code></z><z id="t1440691409" t="quoll it’ll depend on usage. I’ve never needed quads, except when storing multiple graphs in a single file"><y>#</y><d>2015-08-27</d><h>16:03</h><w>quoll</w>it’ll depend on usage. I’ve never needed quads, except when storing multiple graphs in a single file</z><z id="t1440691425" t="quoll I’m a “triples” person myself simple_smile"><y>#</y><d>2015-08-27</d><h>16:03</h><w>quoll</w>I’m a “triples” person myself <b>simple_smile</b></z><z id="t1440691479" t="rickmoynihan we use both 50/50 - one representation simplifies things for everyone... if you don&apos;t care about the nil :g - you don&apos;t need to..."><y>#</y><d>2015-08-27</d><h>16:04</h><w>rickmoynihan</w>we use both 50/50 - one representation simplifies things for everyone... if you don&apos;t care about the nil <code>:g</code> - you don&apos;t need to...</z><z id="t1440691495" t="quoll when I say “storing”, I also mean “loading”, since you get quads back when you read, and they need to go to various graphs"><y>#</y><d>2015-08-27</d><h>16:04</h><w>quoll</w>when I say “storing”, I also mean “loading”, since you get quads back when you read, and they need to go to various graphs</z><z id="t1440691500" t="rickmoynihan the Quad record will seamlessly coerce into a sesame/jena triple/quad resepectively"><y>#</y><d>2015-08-27</d><h>16:05</h><w>rickmoynihan</w>the <code>Quad</code> record will seamlessly coerce into a sesame/jena triple/quad resepectively</z><z id="t1440691582" t="rickmoynihan yes -- we use quads a lot -- because most of our work is writing pipelines that generate RDF... and we usually want to derive the graph from the data we&apos;re loading in"><y>#</y><d>2015-08-27</d><h>16:06</h><w>rickmoynihan</w>yes -- we use quads a lot -- because most of our work is writing pipelines that generate RDF... and we usually want to derive the graph from the data we&apos;re loading in</z><z id="t1440691612" t="quoll and you’re working with multiple graphs at once?"><y>#</y><d>2015-08-27</d><h>16:06</h><w>quoll</w>and you’re working with multiple graphs at once?</z><z id="t1440691616" t="rickmoynihan yes"><y>#</y><d>2015-08-27</d><h>16:06</h><w>rickmoynihan</w>yes</z><z id="t1440691641" t="rickmoynihan the fact you can&apos;t in other tools is one reason we created grafter"><y>#</y><d>2015-08-27</d><h>16:07</h><w>rickmoynihan</w>the fact you can&apos;t in other tools is one reason we created grafter</z><z id="t1440691663" t="rickmoynihan we have tens of thousands of graphs"><y>#</y><d>2015-08-27</d><h>16:07</h><w>rickmoynihan</w>we have tens of thousands of graphs</z><z id="t1440691676" t="quoll ah. You’re one of those simple_smile"><y>#</y><d>2015-08-27</d><h>16:07</h><w>quoll</w>ah. You’re one of those <b>simple_smile</b></z><z id="t1440691724" t="rickmoynihan we manage lots of data for many customers"><y>#</y><d>2015-08-27</d><h>16:08</h><w>rickmoynihan</w>we manage lots of data for many customers</z><z id="t1440691763" t="rickmoynihan so a lot of the time its out of our hands"><y>#</y><d>2015-08-27</d><h>16:09</h><w>rickmoynihan</w>so a lot of the time its out of our hands</z><z id="t1440691809" t="rickmoynihan graphs are also very useful for managing data"><y>#</y><d>2015-08-27</d><h>16:10</h><w>rickmoynihan</w>graphs are also very useful for managing data</z><z id="t1440691819" t="quoll most RDF stores are optimized around triples, and then group statements into graphs. Those that treat graphs as an equal part of the quad take a small performance hit, and it often seems unjustified given that SPARQL treats graphs so differently"><y>#</y><d>2015-08-27</d><h>16:10</h><w>quoll</w>most RDF stores are optimized around triples, and then group statements into graphs. Those that treat graphs as an equal part of the quad take a small performance hit, and it often seems unjustified given that SPARQL treats graphs so differently</z><z id="t1440691834" t="quoll yes, I completely agree that graphs are great that way"><y>#</y><d>2015-08-27</d><h>16:10</h><w>quoll</w>yes, I completely agree that graphs are great that way</z><z id="t1440691914" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] : having used fuseki, sesame, stardog, bigdata and graphdb/owlim I can say that statements not true in my experience"><y>#</y><d>2015-08-27</d><h>16:11</h><w>rickmoynihan</w><a>@quoll</a>: having used fuseki, sesame, stardog, bigdata and graphdb/owlim I can say that statements not true in my experience</z><z id="t1440691959" t="rickmoynihan on many stores you have to use graphs to get acceptable performance"><y>#</y><d>2015-08-27</d><h>16:12</h><w>rickmoynihan</w>on many stores you have to use graphs to get acceptable performance</z><z id="t1440692039" t="quoll I may not have been clear in what I was trying to say"><y>#</y><d>2015-08-27</d><h>16:13</h><w>quoll</w>I may not have been clear in what I was trying to say</z><z id="t1440692045" t="rickmoynihan I agree thats its unfortunate SPARQL only half implements graphs though"><y>#</y><d>2015-08-27</d><h>16:14</h><w>rickmoynihan</w>I agree thats its unfortunate SPARQL only half implements graphs though</z><z id="t1440692247" t="quoll when RDF stores are storing data on disk, many of them will use a scheme that is based around subject/predicate/object. Graphs then get implemented as a separate structure (e.g. separate index files, or an index that refers to statements as a group, but not allowing arbitrary selection of subject/predicate/object/graph as single step index lookups)."><y>#</y><d>2015-08-27</d><h>16:17</h><w>quoll</w>when RDF stores are storing data on disk, many of them will use a scheme that is based around subject/predicate/object. Graphs then get implemented as a separate structure (e.g. separate index files, or an index that refers to statements as a group, but not allowing arbitrary selection of subject/predicate/object/graph as single step index lookups).</z><z id="t1440692269" t="quoll Some stores do allow arbitrary lookup for quads"><y>#</y><d>2015-08-27</d><h>16:17</h><w>quoll</w>Some stores do allow arbitrary lookup for quads</z><z id="t1440692278" t="quoll but then SPARQL hamstrings it"><y>#</y><d>2015-08-27</d><h>16:17</h><w>quoll</w>but then SPARQL hamstrings it</z><z id="t1440692366" t="quoll I mean, you can still work with it, but SPARQL presumes that you’ll be selecting only a couple of graphs, and working with triples from them. The syntax gets messier if you treat graphs as just another element of the quad"><y>#</y><d>2015-08-27</d><h>16:19</h><w>quoll</w>I mean, you can still work with it, but SPARQL presumes that you’ll be selecting only a couple of graphs, and working with triples from them. The syntax gets messier if you treat graphs as just another element of the quad</z><z id="t1440692397" t="quoll ironically, the stores that index symmetrically on the quad can handle the operations just fine. It’s SPARQL syntax that gets in the way"><y>#</y><d>2015-08-27</d><h>16:19</h><w>quoll</w>ironically, the stores that index symmetrically on the quad can handle the operations just fine. It’s SPARQL syntax that gets in the way</z><z id="t1440692425" t="quoll but because of this bias, many stores don’t index symmetrically around the quad"><y>#</y><d>2015-08-27</d><h>16:20</h><w>quoll</w>but because of this bias, many stores don’t index symmetrically around the quad</z><z id="t1440692454" t="quoll that’s usually OK, because many applications don’t ask for lots of graphs like that"><y>#</y><d>2015-08-27</d><h>16:20</h><w>quoll</w>that’s usually OK, because many applications don’t ask for lots of graphs like that</z><z id="t1440692470" t="quoll but some do…. hence my statement that you’re “one of those” simple_smile"><y>#</y><d>2015-08-27</d><h>16:21</h><w>quoll</w>but some do…. hence my statement that you’re “one of those” <b>simple_smile</b></z><z id="t1440692510" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] : yes you&apos;re right -- sorry was missunderstanding what you were saying... Yes that&apos;s definitely true... Graph performance can be spotty on some stores... I know - because we have some automatically generated queries which have well over 1000 graph clauses"><y>#</y><d>2015-08-27</d><h>16:21</h><w>rickmoynihan</w><a>@quoll</a>: yes you&apos;re right -- sorry was missunderstanding what you were saying...

Yes that&apos;s definitely true... Graph performance can be spotty on some stores... I know - because we have some automatically generated queries which have  well over 1000 graph clauses</z><z id="t1440692601" t="rickmoynihan but we actually sell a linked data management platform -- so its unavoidable -- we frequently push the limits and assumptions of every triple store"><y>#</y><d>2015-08-27</d><h>16:23</h><w>rickmoynihan</w>but we actually sell a linked data management platform -- so its unavoidable -- we frequently push the limits and assumptions of every triple store</z><z id="t1440692660" t="quoll I can’t recall now which stores index symmetrically around quads. I know ours does, but it’s in dire need of some love, and doesn’t even handle SPARQL 1.1 (i.e. indexing is great, but query/update functionality is not)"><y>#</y><d>2015-08-27</d><h>16:24</h><w>quoll</w>I can’t recall now which stores index symmetrically around quads. I know ours does, but it’s in dire need of some love, and doesn’t even handle SPARQL 1.1 (i.e. indexing is great, but query/update functionality is not)</z><z id="t1440692821" t="quoll I think that the default indexing in Jena is symmetric"><y>#</y><d>2015-08-27</d><h>16:27</h><w>quoll</w>I think that the default indexing in Jena is symmetric</z><z id="t1440692839" t="quoll I should ask Mike about Stardog though"><y>#</y><d>2015-08-27</d><h>16:27</h><w>quoll</w>I should ask Mike about Stardog though</z><z id="t1440692876" t="quoll I’ve never contributed to the internals of Stardog (for obvious reasons). And the Clojure adapter was just a client"><y>#</y><d>2015-08-27</d><h>16:27</h><w>quoll</w>I’ve never contributed to the internals of Stardog (for obvious reasons). And the Clojure adapter was just a client</z><z id="t1440692889" t="rickmoynihan I&apos;m guessing stardog does"><y>#</y><d>2015-08-27</d><h>16:28</h><w>rickmoynihan</w>I&apos;m guessing stardog does</z><z id="t1440692904" t="quoll I thought it did"><y>#</y><d>2015-08-27</d><h>16:28</h><w>quoll</w>I thought it did</z><z id="t1440692912" t="quoll I can ask… hang on"><y>#</y><d>2015-08-27</d><h>16:28</h><w>quoll</w>I can ask… hang on</z><z id="t1440692939" t="rickmoynihan what store do you work on?"><y>#</y><d>2015-08-27</d><h>16:28</h><w>rickmoynihan</w>what store do you work on?</z><z id="t1440692985" t="quoll Mulgara"><y>#</y><d>2015-08-27</d><h>16:29</h><w>quoll</w>Mulgara</z><z id="t1440692991" t="quoll or rather… I did"><y>#</y><d>2015-08-27</d><h>16:29</h><w>quoll</w>or rather… I did</z><z id="t1440693000" t="quoll I’ve been busy 😕"><y>#</y><d>2015-08-27</d><h>16:30</h><w>quoll</w>I’ve been busy <b>😕</b></z><z id="t1440693050" t="rickmoynihan ahh yes I&apos;ve been to this site before! simple_smile"><y>#</y><d>2015-08-27</d><h>16:30</h><w>rickmoynihan</w>ahh yes I&apos;ve been to this site before! <b>simple_smile</b></z><z id="t1440693088" t="quoll Well… busy life, plus the fact that I’d been on it for over a decade. I’ve been trying new things lately"><y>#</y><d>2015-08-27</d><h>16:31</h><w>quoll</w>Well… busy life, plus the fact that I’d been on it for over a decade. I’ve been trying new things lately</z><z id="t1440693132" t="rickmoynihan ahh you&apos;re the guy that implemented an RDF store on Datomic... I had that same thought the moment Rich released it... How did it go?"><y>#</y><d>2015-08-27</d><h>16:32</h><w>rickmoynihan</w>ahh you&apos;re the guy that implemented an RDF store on Datomic... I had that same thought the moment Rich released it...

How did it go?</z><z id="t1440693163" t="quoll it’s been good, though I put it aside for other stuff. I’m trying to pick it back up again actually"><y>#</y><d>2015-08-27</d><h>16:32</h><w>quoll</w>it’s been good, though I put it aside for other stuff. I’m trying to pick it back up again actually</z><z id="t1440693185" t="quoll Datomic is implemented in a very similar way to Mulgara’s indexes (persistent trees), so it seemed natural to me"><y>#</y><d>2015-08-27</d><h>16:33</h><w>quoll</w>Datomic is implemented in a very similar way to Mulgara’s indexes (persistent trees), so it seemed natural to me</z><z id="t1440693233" t="quoll OK, Al doesn’t know. He said I should ask Mike directly simple_smile"><y>#</y><d>2015-08-27</d><h>16:33</h><w>quoll</w>OK, Al doesn’t know. He said I should ask Mike directly <b>simple_smile</b></z><z id="t1440693281" t="quoll Mike is fun to talk to about this stuff, but I only have him on email, not IM simple_smile"><y>#</y><d>2015-08-27</d><h>16:34</h><w>quoll</w>Mike is fun to talk to about this stuff, but I only have him on email, not IM <b>simple_smile</b></z><z id="t1440693333" t="rickmoynihan Yes Mike and I have exchanged emails...they have a gitter channel now"><y>#</y><d>2015-08-27</d><h>16:35</h><w>rickmoynihan</w>Yes Mike and I have exchanged emails...they have a gitter channel now</z><z id="t1440693433" t="rickmoynihan what datomic schema does kiara use?"><y>#</y><d>2015-08-27</d><h>16:37</h><w>rickmoynihan</w>what datomic schema does kiara use?</z><z id="t1440693461" t="rickmoynihan does it implement a schema for triples/literals - or does it somehow use vocabularies for a datomic schema?"><y>#</y><d>2015-08-27</d><h>16:37</h><w>rickmoynihan</w>does it implement a schema for triples/literals - or does it somehow use vocabularies for a datomic schema?</z><z id="t1440693573" t="quoll literals are done in 2 ways"><y>#</y><d>2015-08-27</d><h>16:39</h><w>quoll</w>literals are done in 2 ways</z><z id="t1440693667" t="quoll if they’re simple text or using one of a few xsd datatypes then they’re stored as native values (strings, longs, doubles, floats, dates, URIs)"><y>#</y><d>2015-08-27</d><h>16:41</h><w>quoll</w>if they’re simple text or using one of a few xsd datatypes then they’re stored as native values (strings, longs, doubles, floats, dates, URIs)</z><z id="t1440693729" t="quoll anything else, and they become a structure with properties for value (a string) and datatype (a URI, since there aren’t any IRIs in xsd datatypes)"><y>#</y><d>2015-08-27</d><h>16:42</h><w>quoll</w>anything else, and they become a structure with properties for value (a string) and datatype (a URI, since there aren’t any IRIs in xsd datatypes)</z><z id="t1440693778" t="quoll RDF properties get scanned for the values that they refer to, and the most general type required is found"><y>#</y><d>2015-08-27</d><h>16:42</h><w>quoll</w>RDF properties get scanned for the values that they refer to, and the most general type required is found</z><z id="t1440693838" t="quoll this is because if you have a property of my:value and it refers to a xsd:long, then it’s a very rare schema that requires that property to also refer to a string, or something else"><y>#</y><d>2015-08-27</d><h>16:43</h><w>quoll</w>this is because if you have a property of my:value and it refers to a xsd:long, then it’s a very rare schema that requires that property to also refer to a string, or something else</z><z id="t1440693892" t="rickmoynihan yes I&apos;d say thats a fair assumption"><y>#</y><d>2015-08-27</d><h>16:44</h><w>rickmoynihan</w>yes I&apos;d say thats a fair assumption</z><z id="t1440693910" t="quoll but if that DOES happen, then the type for the property in the Datomic schema is set to refer to a structure, and that structure then refers to the final value, using different property names for each type"><y>#</y><d>2015-08-27</d><h>16:45</h><w>quoll</w>but if that DOES happen, then the type for the property in the Datomic schema is set to refer to a structure, and that structure then refers to the final value, using different property names for each type</z><z id="t1440693923" t="quoll that’s a corner case, but it makes querying more complex 😕"><y>#</y><d>2015-08-27</d><h>16:45</h><w>quoll</w>that’s a corner case, but it makes querying more complex <b>😕</b></z><z id="t1440693933" t="rickmoynihan no shit simple_smile"><y>#</y><d>2015-08-27</d><h>16:45</h><w>rickmoynihan</w>no shit <b>simple_smile</b></z><z id="t1440693936" t="quoll 😄"><y>#</y><d>2015-08-27</d><h>16:45</h><w>quoll</w><b>😄</b></z><z id="t1440693972" t="quoll I think I need to change how subjects work though"><y>#</y><d>2015-08-27</d><h>16:46</h><w>quoll</w>I think I need to change how subjects work though</z><z id="t1440694006" t="rickmoynihan whats the performance on datomic like?"><y>#</y><d>2015-08-27</d><h>16:46</h><w>rickmoynihan</w>whats the performance on datomic like?</z><z id="t1440694028" t="rickmoynihan is there any hope of it being competitive?"><y>#</y><d>2015-08-27</d><h>16:47</h><w>rickmoynihan</w>is there any hope of it being competitive?</z><z id="t1440694045" t="quoll for now, if they’re IRIs then I convert to QNames (ruthlessly, if necessary) simple_smile then convert the QNames to keywords and use those as the entity IDs. This works, but it uses RAM."><y>#</y><d>2015-08-27</d><h>16:47</h><w>quoll</w>for now, if they’re IRIs then I convert to QNames (ruthlessly, if necessary) <b>simple_smile</b> then convert the QNames to keywords and use those as the entity IDs. This works, but it uses RAM.</z><z id="t1440694072" t="quoll I have not pushed it to big datasets yet"><y>#</y><d>2015-08-27</d><h>16:47</h><w>quoll</w>I have not pushed it to big datasets yet</z><z id="t1440694120" t="quoll Most of the big sets are in RDF/XML (which I despise), and I really want to avoid Jena (I love those guys, but Jena is bloated), so I’ve started on an RDF/XML parser in Clojure"><y>#</y><d>2015-08-27</d><h>16:48</h><w>quoll</w>Most of the big sets are in RDF/XML (which I despise), and I really want to avoid Jena (I love those guys, but Jena is bloated), so I’ve started on an RDF/XML parser in Clojure</z><z id="t1440694141" t="quoll I have a decent Turtle parser though, and that seems OK"><y>#</y><d>2015-08-27</d><h>16:49</h><w>quoll</w>I have a decent Turtle parser though, and that seems OK</z><z id="t1440694146" t="rickmoynihan cool"><y>#</y><d>2015-08-27</d><h>16:49</h><w>rickmoynihan</w>cool</z><z id="t1440694147" t="quoll but I haven’t loaded anything really big through hit"><y>#</y><d>2015-08-27</d><h>16:49</h><w>quoll</w>but I haven’t loaded anything really big through hit</z><z id="t1440694152" t="rickmoynihan does it work with large files?"><y>#</y><d>2015-08-27</d><h>16:49</h><w>rickmoynihan</w>does it work with large files?</z><z id="t1440694189" t="quoll that’s another thing. Datomic recommends that you don’t try to do really big loads. They recommend chunking it up. That’s easy in Turtle, but not so much with RDF/XML"><y>#</y><d>2015-08-27</d><h>16:49</h><w>quoll</w>that’s another thing. Datomic recommends that you don’t try to do really big loads. They recommend chunking it up. That’s easy in Turtle, but not so much with RDF/XML</z><z id="t1440694219" t="rickmoynihan Jena do a good job - if you want a standards compliant, free store... but yeah the codebase is a mess... Sesame&apos;s code is so much better to work with"><y>#</y><d>2015-08-27</d><h>16:50</h><w>rickmoynihan</w>Jena do a good job - if you want a standards compliant, free store... but yeah the codebase is a mess...  Sesame&apos;s code is so much better to work with</z><z id="t1440694220" t="quoll besides that, I hate the idea of multiple transaction points at arbitrary locations in a load. But it’s pragmatic, so I guess I need to"><y>#</y><d>2015-08-27</d><h>16:50</h><w>quoll</w>besides that, I hate the idea of multiple transaction points at arbitrary locations in a load. But it’s pragmatic, so I guess I need to</z><z id="t1440694233" t="quoll yes, I’ve contributed to Jena"><y>#</y><d>2015-08-27</d><h>16:50</h><w>quoll</w>yes, I’ve contributed to Jena</z><z id="t1440694265" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] : yeah chunking sucks"><y>#</y><d>2015-08-27</d><h>16:51</h><w>rickmoynihan</w><a>@quoll</a>: yeah chunking sucks</z><z id="t1440694305" t="quoll Mulgara is actually faster if you don&apos;t"><y>#</y><d>2015-08-27</d><h>16:51</h><w>quoll</w>Mulgara is actually faster if you don&apos;t</z><z id="t1440694336" t="quoll annoyingly people would chunk their data, and then get annoyed at Mulgara for performing badly"><y>#</y><d>2015-08-27</d><h>16:52</h><w>quoll</w>annoyingly people would chunk their data, and then get annoyed at Mulgara for performing badly</z><z id="t1440694373" t="quoll but every chunk becomes a new transaction, which means that it requires a new root to the persistent tree"><y>#</y><d>2015-08-27</d><h>16:52</h><w>quoll</w>but every chunk becomes a new transaction, which means that it requires a new root to the persistent tree</z><z id="t1440694394" t="quoll if you load 1M triples, then you just have a simple tree"><y>#</y><d>2015-08-27</d><h>16:53</h><w>quoll</w>if you load 1M triples, then you just have a simple tree</z><z id="t1440694421" t="rickmoynihan so I&apos;m guessing you need to reindex if that happens"><y>#</y><d>2015-08-27</d><h>16:53</h><w>rickmoynihan</w>so I&apos;m guessing you need to reindex if that happens</z><z id="t1440694446" t="quoll if you load 100K triples 10 times, then you end up with most of the nodes in the first tree being duplicated while inserting the second 100K, and so on for each chunk"><y>#</y><d>2015-08-27</d><h>16:54</h><w>quoll</w>if you load 100K triples 10 times, then you end up with most of the nodes in the first tree being duplicated while inserting the second 100K, and so on for each chunk</z><z id="t1440694463" t="quoll actually, Mulgara does not do background indexing (which is something I started work on, but never finished)"><y>#</y><d>2015-08-27</d><h>16:54</h><w>quoll</w>actually, Mulgara does not do background indexing (which is something I started work on, but never finished)</z><z id="t1440694478" t="quoll so when it’s finished loading, it’s fully available"><y>#</y><d>2015-08-27</d><h>16:54</h><w>quoll</w>so when it’s finished loading, it’s fully available</z><z id="t1440694486" t="quoll but that makes loading slower"><y>#</y><d>2015-08-27</d><h>16:54</h><w>quoll</w>but that makes loading slower</z><z id="t1440694549" t="quoll Stardog, for instance, loads immediately into a linear file, and then moves those triples (or quads) into the indexes in the background. Querying looks in both the indexes (fast) and the linear file (slow)."><y>#</y><d>2015-08-27</d><h>16:55</h><w>quoll</w>Stardog, for instance, loads immediately into a linear file, and then moves those triples (or quads) into the indexes in the background. Querying looks in both the indexes (fast) and the linear file (slow).</z><z id="t1440694564" t="quoll So loads are lightning fast, but querying sucks for a while"><y>#</y><d>2015-08-27</d><h>16:56</h><w>quoll</w>So loads are lightning fast, but querying sucks for a while</z><z id="t1440694580" t="quoll the longer you wait, the faster the querying gets"><y>#</y><d>2015-08-27</d><h>16:56</h><w>quoll</w>the longer you wait, the faster the querying gets</z><z id="t1440694662" t="quoll anyway, Mulgara isn’t as complex, but it does not need reindexing"><y>#</y><d>2015-08-27</d><h>16:57</h><w>quoll</w>anyway, Mulgara isn’t as complex, but it does not need reindexing</z><z id="t1440694853" t="quoll Just got a response on twitter: yes, Stardog is symmetrically indexed (I thought it was)"><y>#</y><d>2015-08-27</d><h>17:00</h><w>quoll</w>Just got a response on twitter: yes, Stardog is symmetrically indexed (I thought it was)</z><z id="t1440694903" t="rickmoynihan thats interesting"><y>#</y><d>2015-08-27</d><h>17:01</h><w>rickmoynihan</w>thats interesting</z><z id="t1440695387" t="ricroberts hello."><y>#</y><d>2015-08-27</d><h>17:09</h><w>ricroberts</w>hello.</z><z id="t1440695390" t="rickmoynihan Welcome [:attrs {:href &quot;/_/_/users/U09NZV6R3&quot;}]"><y>#</y><d>2015-08-27</d><h>17:09</h><w>rickmoynihan</w>Welcome <a>@ricroberts</a></z><z id="t1440695633" t="joelkuiper Hey simple_smile"><y>#</y><d>2015-08-27</d><h>17:13</h><w>joelkuiper</w>Hey <b>simple_smile</b></z><z id="t1440697079" t="wagjo Hi"><y>#</y><d>2015-08-27</d><h>17:37</h><w>wagjo</w>Hi</z><z id="t1440697128" t="joelkuiper Hey! Thought you’d might also be interested in this channel, we’ve also been discussing some YeSPARQL related things simple_smile"><y>#</y><d>2015-08-27</d><h>17:38</h><w>joelkuiper</w>Hey! Thought you’d might also be interested in this channel, we’ve also been discussing some YeSPARQL related things <b>simple_smile</b></z><z id="t1440697162" t="wagjo Definitely! Thanks for inviting me."><y>#</y><d>2015-08-27</d><h>17:39</h><w>wagjo</w>Definitely! Thanks for inviting me.</z><z id="t1441538157" t="joelkuiper Pushed YeSPARQL 0.2.0-beta3 with bug fixes and annotated source code https://github.com/joelkuiper/yesparql"><y>#</y><d>2015-09-06</d><h>11:15</h><w>joelkuiper</w>Pushed YeSPARQL 0.2.0-beta3 with bug fixes and annotated source code <a href="https://github.com/joelkuiper/yesparql" target="_blank">https://github.com/joelkuiper/yesparql</a></z><z id="t1441538190" t="joelkuiper would it make sense to substitute the ParameterizedSparqlString with something based on this BNF? https://github.com/apache/jena/blob/jena-3.0.0-rc1/jena-arq/Grammar/Final/tokens_11.txt ?"><y>#</y><d>2015-09-06</d><h>11:16</h><w>joelkuiper</w>would it make sense to substitute the ParameterizedSparqlString with something based on this BNF? <a href="https://github.com/apache/jena/blob/jena-3.0.0-rc1/jena-arq/Grammar/Final/tokens_11.txt" target="_blank">https://github.com/apache/jena/blob/jena-3.0.0-rc1/jena-arq/Grammar/Final/tokens_11.txt</a>?</z><z id="t1441607593" t="wagjo does ParametrizedSparqlString validate input query? I would go with an approach that compiles into something that is fast where only bindings are checked at runtime."><y>#</y><d>2015-09-07</d><h>06:33</h><w>wagjo</w>does ParametrizedSparqlString validate input query? I would go with an approach that compiles into something that is fast where only bindings are checked at runtime.</z><z id="t1441621110" t="joelkuiper it does so at the textual level, https://jena.apache.org/documentation/javadoc/arq/org/apache/jena/query/ParameterizedSparqlString.html"><y>#</y><d>2015-09-07</d><h>10:18</h><w>joelkuiper</w>it does so at the textual level, <a href="https://jena.apache.org/documentation/javadoc/arq/org/apache/jena/query/ParameterizedSparqlString.html" target="_blank">https://jena.apache.org/documentation/javadoc/arq/org/apache/jena/query/ParameterizedSparqlString.html</a></z><z id="t1441621142" t="joelkuiper I’m not sure where the query parsing is actually done, I assume only when you call .asQuery on the ParameterizedSparqlString"><y>#</y><d>2015-09-07</d><h>10:19</h><w>joelkuiper</w>I’m not sure where the query parsing is actually done, I assume only when you call .asQuery on the ParameterizedSparqlString</z><z id="t1441621171" t="joelkuiper YeSQL applies another instaparse BNF to the SQL itself to do substitutions"><y>#</y><d>2015-09-07</d><h>10:19</h><w>joelkuiper</w>YeSQL applies another instaparse BNF to the SQL itself to do substitutions</z><z id="t1441621216" t="joelkuiper was thinking of abusing the BNF from Jena itself for a similar purpose, but it will add to the complexity"><y>#</y><d>2015-09-07</d><h>10:20</h><w>joelkuiper</w>was thinking of abusing the BNF from Jena itself for a similar purpose, but it will add to the complexity</z><z id="t1450451408" t="joelkuiper released a new version of YeSPARQL https://github.com/joelkuiper/yesparql"><y>#</y><d>2015-12-18</d><h>15:10</h><w>joelkuiper</w>released a new version of YeSPARQL <a href="https://github.com/joelkuiper/yesparql" target="_blank">https://github.com/joelkuiper/yesparql</a></z><z id="t1450451431" t="joelkuiper fixes include some de-complecting, now possible to pass a output stream to all serialization methods"><y>#</y><d>2015-12-18</d><h>15:10</h><w>joelkuiper</w>fixes include some de-complecting, now possible to pass a output stream to all serialization methods</z></g><g id="s1"><z id="t1450451442" t="joelkuiper and some other minor fixes/additions"><y>#</y><d>2015-12-18</d><h>15:10</h><w>joelkuiper</w>and some other minor fixes/additions</z><z id="t1453462780" t="rickmoynihan Grafter 0.6.0 released: http://grafter.org/"><y>#</y><d>2016-01-22</d><h>11:39</h><w>rickmoynihan</w>Grafter 0.6.0 released: <a href="http://grafter.org/" target="_blank">http://grafter.org/</a></z><z id="t1473334139" t="rickmoynihan A paper on a project using grafter just got accepted in the semantic web journal: http://www.semantic-web-journal.net/content/datagraft-one-stop-shop-open-data-management-0"><y>#</y><d>2016-09-08</d><h>11:28</h><w>rickmoynihan</w>A paper on a project using grafter just got accepted in the semantic web journal: <a href="http://www.semantic-web-journal.net/content/datagraft-one-stop-shop-open-data-management-0" target="_blank">http://www.semantic-web-journal.net/content/datagraft-one-stop-shop-open-data-management-0</a></z><z id="t1473334144" t="rickmoynihan might be of interest to some"><y>#</y><d>2016-09-08</d><h>11:29</h><w>rickmoynihan</w>might be of interest to some</z><z id="t1473336350" t="quoll cool! 🙂"><y>#</y><d>2016-09-08</d><h>12:05</h><w>quoll</w>cool! <b>🙂</b></z><z id="t1473337830" t="rickmoynihan quoll: who are you friends with?"><y>#</y><d>2016-09-08</d><h>12:30</h><w>rickmoynihan</w>quoll: who are you friends with?</z><z id="t1473337881" t="quoll Tom Heath"><y>#</y><d>2016-09-08</d><h>12:31</h><w>quoll</w>Tom Heath</z><z id="t1473337948" t="rickmoynihan ahh cool - I know Tom quite well 🙂"><y>#</y><d>2016-09-08</d><h>12:32</h><w>rickmoynihan</w>ahh cool - I know Tom quite well <b>🙂</b></z><z id="t1473337989" t="quoll we interviewed at Talis together"><y>#</y><d>2016-09-08</d><h>12:33</h><w>quoll</w>we interviewed at Talis together</z><z id="t1473338003" t="rickmoynihan ahh cool old school"><y>#</y><d>2016-09-08</d><h>12:33</h><w>rickmoynihan</w>ahh cool old school</z><z id="t1473338022" t="quoll I saw him at a couple of Semweb conferences, but I don’t get sent to those any more"><y>#</y><d>2016-09-08</d><h>12:33</h><w>quoll</w>I saw him at a couple of Semweb conferences, but I don’t get sent to those any more</z><z id="t1473338030" t="rickmoynihan He&apos;s now at Arup"><y>#</y><d>2016-09-08</d><h>12:33</h><w>rickmoynihan</w>He&apos;s now at Arup</z><z id="t1473338064" t="rickmoynihan where are you now quoll?"><y>#</y><d>2016-09-08</d><h>12:34</h><w>rickmoynihan</w>where are you now quoll?</z><z id="t1473338080" t="quoll Cisco"><y>#</y><d>2016-09-08</d><h>12:34</h><w>quoll</w>Cisco</z><z id="t1473338089" t="quoll trying to convince them to use RDF 🙂"><y>#</y><d>2016-09-08</d><h>12:34</h><w>quoll</w>trying to convince them to use RDF <b>🙂</b></z><z id="t1473338107" t="quoll but they’re letting me build a rules engine, so it’s all good"><y>#</y><d>2016-09-08</d><h>12:35</h><w>quoll</w>but they’re letting me build a rules engine, so it’s all good</z><z id="t1473338133" t="rickmoynihan nice 🙂 Rule engines are fun"><y>#</y><d>2016-09-08</d><h>12:35</h><w>rickmoynihan</w>nice <b>🙂</b> Rule engines are fun</z><z id="t1473338147" t="rickmoynihan using clojure?"><y>#</y><d>2016-09-08</d><h>12:35</h><w>rickmoynihan</w>using clojure?</z><z id="t1473338156" t="quoll of course!"><y>#</y><d>2016-09-08</d><h>12:35</h><w>quoll</w>of course!</z><z id="t1473338160" t="rickmoynihan yay!"><y>#</y><d>2016-09-08</d><h>12:36</h><w>rickmoynihan</w>yay!</z><z id="t1473338177" t="rickmoynihan core.logic?"><y>#</y><d>2016-09-08</d><h>12:36</h><w>rickmoynihan</w>core.logic?</z><z id="t1473338197" t="quoll My third rules engine. My first was in Java. My second was Clojure and closed source. This one is open source"><y>#</y><d>2016-09-08</d><h>12:36</h><w>quoll</w>My third rules engine. My first was in Java. My second was Clojure and closed source. This one is open source</z><z id="t1473338205" t="rickmoynihan or rete or something else?"><y>#</y><d>2016-09-08</d><h>12:36</h><w>rickmoynihan</w>or rete or something else?</z><z id="t1473338207" t="quoll no, it’s datalog not prolog"><y>#</y><d>2016-09-08</d><h>12:36</h><w>quoll</w>no, it’s datalog not prolog</z><z id="t1473338213" t="rickmoynihan ahh cool"><y>#</y><d>2016-09-08</d><h>12:36</h><w>rickmoynihan</w>ahh cool</z><z id="t1473338215" t="quoll rete based"><y>#</y><d>2016-09-08</d><h>12:36</h><w>quoll</w>rete based</z><z id="t1473338240" t="quoll https://github.com/threatgrid/naga"><y>#</y><d>2016-09-08</d><h>12:37</h><w>quoll</w><a href="https://github.com/threatgrid/naga" target="_blank">https://github.com/threatgrid/naga</a></z><z id="t1473338283" t="quoll I’m building it specifically to be backable by either Datomic or a SPARQL store"><y>#</y><d>2016-09-08</d><h>12:38</h><w>quoll</w>I’m building it specifically to be backable by either Datomic or a SPARQL store</z><z id="t1473338300" t="rickmoynihan I used to work for a startup that made a rule engine... As far as I know at the time - it was the first engine that supported defeasible inferencing with priorities"><y>#</y><d>2016-09-08</d><h>12:38</h><w>rickmoynihan</w>I used to work for a startup that made a rule engine...  As far as I know at the time - it was the first engine that supported defeasible inferencing with priorities</z><z id="t1473338313" t="quoll and the first thing I’m asked to? My own (i.e. not based on a third party library) database"><y>#</y><d>2016-09-08</d><h>12:38</h><w>quoll</w>and the first thing I’m asked to? My own (i.e. not based on a third party library) database</z><z id="t1473338333" t="quoll defeasible? Ooooh!"><y>#</y><d>2016-09-08</d><h>12:38</h><w>quoll</w>defeasible? Ooooh!</z><z id="t1473338352" t="rickmoynihan Yeah 🙂 it was fun"><y>#</y><d>2016-09-08</d><h>12:39</h><w>rickmoynihan</w>Yeah <b>🙂</b> it was fun</z><z id="t1473338364" t="quoll Naga has priorities, but I never considered defeasibility"><y>#</y><d>2016-09-08</d><h>12:39</h><w>quoll</w>Naga has priorities, but I never considered defeasibility</z><z id="t1473338373" t="rickmoynihan implemented as a meta-interpreter in prolog"><y>#</y><d>2016-09-08</d><h>12:39</h><w>rickmoynihan</w>implemented as a meta-interpreter in prolog</z><z id="t1473338421" t="quoll Naga is entirely its own thing. But the storage is abstracted, so it can use one of several"><y>#</y><d>2016-09-08</d><h>12:40</h><w>quoll</w>Naga is entirely its own thing. But the storage is abstracted, so it can use one of several</z><z id="t1473338464" t="quoll The thing about using Prolog (or any backtracking rule system, like Drools) is that it can’t handle databases efficiently."><y>#</y><d>2016-09-08</d><h>12:41</h><w>quoll</w>The thing about using Prolog (or any backtracking rule system, like Drools) is that it can’t handle databases efficiently.</z><z id="t1473338466" t="rickmoynihan It was academically on very sound foundations - built upon work in defeasible logics by Henry Prakken at Utrecht and Chris Reed at Dundee"><y>#</y><d>2016-09-08</d><h>12:41</h><w>rickmoynihan</w>It was academically on very sound foundations - built upon work in defeasible logics by Henry Prakken at Utrecht and Chris Reed at Dundee</z><z id="t1473338482" t="quoll Very nice"><y>#</y><d>2016-09-08</d><h>12:41</h><w>quoll</w>Very nice</z><z id="t1473338529" t="rickmoynihan yeah - sadly the company couldn&apos;t really find a market for it -- far too technology driven."><y>#</y><d>2016-09-08</d><h>12:42</h><w>rickmoynihan</w>yeah - sadly the company couldn&apos;t really find a market for it -- far too technology driven.</z><z id="t1473338542" t="quoll same with my previous company"><y>#</y><d>2016-09-08</d><h>12:42</h><w>quoll</w>same with my previous company</z><z id="t1473338560" t="quoll I always meant to build it again, since I’ve heard people crying out for it on Datomic"><y>#</y><d>2016-09-08</d><h>12:42</h><w>quoll</w>I always meant to build it again, since I’ve heard people crying out for it on Datomic</z><z id="t1473338598" t="quoll Jena has a capable rule system, but Jena never scaled well"><y>#</y><d>2016-09-08</d><h>12:43</h><w>quoll</w>Jena has a capable rule system, but Jena never scaled well</z><z id="t1473338599" t="rickmoynihan yeah something built on datalog/datomic would have nice practical properties"><y>#</y><d>2016-09-08</d><h>12:43</h><w>rickmoynihan</w>yeah something built on datalog/datomic would have nice practical properties</z><z id="t1473338627" t="rickmoynihan so I hear"><y>#</y><d>2016-09-08</d><h>12:43</h><w>rickmoynihan</w>so I hear</z><z id="t1473338671" t="quoll Coincidentally, Datomic is built along very similar lines to Mulgara (a project I run… though I’ve let it run down in recent years)."><y>#</y><d>2016-09-08</d><h>12:44</h><w>quoll</w>Coincidentally, Datomic is built along very similar lines to Mulgara (a project I run… though I’ve let it run down in recent years).</z><z id="t1473338679" t="rickmoynihan we used to use Fuseki as a triple store - but it couldn&apos;t cope with the data volumes... plus Jena&apos;s API&apos;s aren&apos;t that clean/consistent in my experience."><y>#</y><d>2016-09-08</d><h>12:44</h><w>rickmoynihan</w>we used to use Fuseki as a triple store - but it couldn&apos;t cope with the data volumes... plus Jena&apos;s API&apos;s aren&apos;t that clean/consistent in my experience.</z><z id="t1473338718" t="quoll I’ve been rebuilding the Mulgara indexes in Clojure. They have nearly identical properties to Datomic but load data faster, so I’m hoping to integrate it into Naga"><y>#</y><d>2016-09-08</d><h>12:45</h><w>quoll</w>I’ve been rebuilding the Mulgara indexes in Clojure. They have nearly identical properties to Datomic but load data faster, so I’m hoping to integrate it into Naga</z><z id="t1473338727" t="rickmoynihan cool... I&apos;ve stumbled across Mulgara before - but know nothing about it"><y>#</y><d>2016-09-08</d><h>12:45</h><w>rickmoynihan</w>cool... I&apos;ve stumbled across Mulgara before - but know nothing about it</z><z id="t1473338741" t="quoll I was one of the original developers on it (we started in about 2000)"><y>#</y><d>2016-09-08</d><h>12:45</h><w>quoll</w>I was one of the original developers on it (we started in about 2000)</z><z id="t1473338754" t="rickmoynihan sweet"><y>#</y><d>2016-09-08</d><h>12:45</h><w>rickmoynihan</w>sweet</z><z id="t1473338759" t="rickmoynihan where are you based?"><y>#</y><d>2016-09-08</d><h>12:45</h><w>rickmoynihan</w>where are you based?</z><z id="t1473338764" t="quoll central VA"><y>#</y><d>2016-09-08</d><h>12:46</h><w>quoll</w>central VA</z><z id="t1473338771" t="quoll I telecommute"><y>#</y><d>2016-09-08</d><h>12:46</h><w>quoll</w>I telecommute</z><z id="t1473338795" t="rickmoynihan forgive me VA?"><y>#</y><d>2016-09-08</d><h>12:46</h><w>rickmoynihan</w>forgive me VA?</z><z id="t1473338809" t="quoll sorry… it’s the common abbreviation for Virginia, USA"><y>#</y><d>2016-09-08</d><h>12:46</h><w>quoll</w>sorry… it’s the common abbreviation for Virginia, USA</z><z id="t1473338839" t="rickmoynihan ok I suspected that"><y>#</y><d>2016-09-08</d><h>12:47</h><w>rickmoynihan</w>ok I suspected that</z><z id="t1473338845" t="rickmoynihan but I&apos;m British so wasn&apos;t sure"><y>#</y><d>2016-09-08</d><h>12:47</h><w>rickmoynihan</w>but I&apos;m British so wasn&apos;t sure</z><z id="t1473338876" t="quoll I’m Australian, so I can appreciate not knowing"><y>#</y><d>2016-09-08</d><h>12:47</h><w>quoll</w>I’m Australian, so I can appreciate not knowing</z><z id="t1473338953" t="quoll are you based in the UK?"><y>#</y><d>2016-09-08</d><h>12:49</h><w>quoll</w>are you based in the UK?</z><z id="t1473339222" t="rickmoynihan yeah... I work in Manchester at swirrl http://www.swirrl.com/ live in Yorkshire..."><y>#</y><d>2016-09-08</d><h>12:53</h><w>rickmoynihan</w>yeah... I work in Manchester at swirrl  <a href="http://www.swirrl.com/" target="_blank">http://www.swirrl.com/</a> live in Yorkshire...</z><z id="t1473339413" t="quoll I’ve been a bit apart from SemWeb business recently. How is the market? That was the thing that everyone always struggled with. Great technology, but unable to get customers to invest in it"><y>#</y><d>2016-09-08</d><h>12:56</h><w>quoll</w>I’ve been a bit apart from SemWeb business recently. How is the market? That was the thing that everyone always struggled with. Great technology, but unable to get customers to invest in it</z><z id="t1473339684" t="rickmoynihan our market is good but small. We target government data publishers - I don&apos;t consider us to be a SemWeb business though... We tend to align more with Linked Data than SemWeb if you see the difference... And to be honest even though we&apos;re pretty much 100% Linked Data from a technology perspective, we tend to de-emphasise Linked Data (though I think we still emphasise it too much) and focus on the real problems and benefits."><y>#</y><d>2016-09-08</d><h>13:01</h><w>rickmoynihan</w>our market is good but small.

We target government data publishers - I don&apos;t consider us to be a SemWeb business though... We tend to align more with Linked Data than SemWeb if you see the difference... And to be honest even though we&apos;re pretty much 100% Linked Data from a technology perspective, we tend to de-emphasise Linked Data (though I think we still emphasise it too much) and focus on the real problems and benefits.</z><z id="t1473339869" t="rickmoynihan my feelings are that aside from the true believers - if you talk about URI&apos;s, dereferencing and triples (let alone SemWeb/Ontologies etc..) in the worst case you&apos;ve probably lost a sale... and in the best case you&apos;ve found yourself in the free education market - and that&apos;s not a business most people want to be in 🙂"><y>#</y><d>2016-09-08</d><h>13:04</h><w>rickmoynihan</w>my feelings are that aside from the true believers - if you talk about URI&apos;s, dereferencing and triples (let alone SemWeb/Ontologies etc..) in the worst case you&apos;ve probably lost a sale... and in the best case you&apos;ve found yourself in the free education market - and that&apos;s not a business most people want to be in <b>🙂</b></z><z id="t1473340983" t="quoll I agree that selling yourself as a SemanticWeb company isn’t a good idea. Nobody needs semantic web. They need to manage data. They need to connect data from different sources, in intelligent ways. They need to be able to analyze and find connections in that data. Any mentioned of how that gets done is a mistake IMO"><y>#</y><d>2016-09-08</d><h>13:23</h><w>quoll</w>I agree that selling yourself as a SemanticWeb company isn’t a good idea. Nobody needs semantic web. They need to manage data. They need to connect data from different sources, in intelligent ways. They need to be able to analyze and find connections in that data. Any mentioned of how that gets done is a mistake IMO</z><z id="t1473341211" t="quoll other companies don’t do that. Hardware companies don’t advertise that they build chips with advanced lithographic techniques for the implementation of even smaller features in CMOS. That info is usually available, but not part of the sales pitch"><y>#</y><d>2016-09-08</d><h>13:26</h><w>quoll</w>other companies don’t do that. Hardware companies don’t advertise that they build chips with advanced lithographic techniques for the implementation of even smaller features in CMOS. That info is usually available, but not part of the sales pitch</z><z id="t1473341353" t="quoll Too often, I saw SemWeb companies failing because they were trying to sell the technology to people who didn’t understand that they could implement [:attrs nil] task, or save money over [:attrs nil] , if they took it on and built something with it. But that was due to the maturity of the technology and community. I’ve been hoping that has been improving over time"><y>#</y><d>2016-09-08</d><h>13:29</h><w>quoll</w>Too often, I saw SemWeb companies failing because they were trying to sell the technology to people who didn’t understand that they could implement <b>this</b> task, or save money over <b>here</b>, if they took it on and built something with it. But that was due to the maturity of the technology and community. I’ve been hoping that has been improving over time</z><z id="t1473344020" t="rickmoynihan +1"><y>#</y><d>2016-09-08</d><h>14:13</h><w>rickmoynihan</w>+1</z><z id="t1474814433" t="malcolmsparks +1. Although it&apos;s never been about the data, it&apos;s always been about relationships between data, but the market doesn&apos;t differentiate so neither should we (when selling the idea)"><y>#</y><d>2016-09-25</d><h>14:40</h><w>malcolmsparks</w>+1. Although it&apos;s never been about the data, it&apos;s always been about relationships between data, but the market doesn&apos;t differentiate so neither should we (when selling the idea)</z><z id="t1474814639" t="malcolmsparks I take a long multi-decade view on technology, so I believe SemWeb is worth investing in. As an industry we have cronic myopia. Emacs, Lisp, the Web and SemWeb, these bets gradually pay off."><y>#</y><d>2016-09-25</d><h>14:43</h><w>malcolmsparks</w>I take a long multi-decade view on technology, so I believe SemWeb is worth investing in. As an industry we have cronic myopia. Emacs, Lisp, the Web and SemWeb, these bets gradually pay off.</z><z id="t1474814686" t="malcolmsparks If you don&apos;t know what your data means, it&apos;s virtually worthless."><y>#</y><d>2016-09-25</d><h>14:44</h><w>malcolmsparks</w>If you don&apos;t know what your data means, it&apos;s virtually worthless.</z><z id="t1474814778" t="malcolmsparks SemWeb will likely lose to deep learning in the short term, but deep learning is a trick, and limited in application. Ultimately maths wins."><y>#</y><d>2016-09-25</d><h>14:46</h><w>malcolmsparks</w>SemWeb will likely lose to deep learning in the short term, but deep learning is a trick, and limited in application. Ultimately maths wins.</z><z id="t1474862633" t="quoll Even with deep learning, a common use is generating metadata. So store it, link it, and analyze it. That&apos;s what SemWeb provides. If used correctly, they compliment each other"><y>#</y><d>2016-09-26</d><h>04:03</h><w>quoll</w>Even with deep learning, a common use is generating metadata. So store it, link it, and analyze it. That&apos;s what SemWeb provides. If used correctly, they compliment each other</z><z id="t1474891238" t="rickmoynihan Deep learning&apos;s cool - but it in no way competes with linked data / SemWeb, they&apos;re very different tools. Nobody seems to think Deep Learning is going to replace Postgresql, so it&apos;s definitely not going to replace linked data. Deep learning doesn&apos;t really provide you with a way to model knowledge or data, e.g. it might let you train a function to categorise photos into buckets, but it doesn&apos;t really have anything to say about the buckets and how they relate to each other. Linked Data is one way to represent that knowledge and data model."><y>#</y><d>2016-09-26</d><h>12:00</h><w>rickmoynihan</w>Deep learning&apos;s cool - but it in no way competes with linked data / SemWeb, they&apos;re very different tools.

Nobody seems to think Deep Learning is going to replace Postgresql, so it&apos;s definitely not going to replace linked data.

Deep learning doesn&apos;t really provide you with a way to model knowledge or data, e.g. it might let you train a function to categorise photos into buckets, but it doesn&apos;t really have anything to say about the buckets and how they relate to each other.  Linked Data is one way to represent that knowledge and data model.</z><z id="t1474891486" t="rickmoynihan I&apos;ve never seen anyone advocate deep learning as the one way to write a whole application... You can&apos;t deep learn a rails app into existence... at some point you need humans to write code - not everything can or should be a trained/learned function."><y>#</y><d>2016-09-26</d><h>12:04</h><w>rickmoynihan</w>I&apos;ve never seen anyone advocate deep learning as the one way to write a whole application... You can&apos;t deep learn a rails app into existence...  at some point you need humans to write code - not everything can or should be a trained/learned function.</z><z id="t1479427882" t="stain how are people doing RDF in Clojure today? I&apos;m one of the Apache Commons RDF developers, and I&apos;ve started work on https://github.com/stain/commons-rdf-clj to map it to Clojure using protocols - but I&apos;m struggling to come up with a better name for the project."><y>#</y><d>2016-11-18</d><h>00:11</h><w>stain</w>how are people doing RDF in Clojure today?  I&apos;m one of the Apache Commons RDF developers, and I&apos;ve started work on <a href="https://github.com/stain/commons-rdf-clj" target="_blank">https://github.com/stain/commons-rdf-clj</a> to map it to Clojure using protocols - but I&apos;m struggling to come up with a better name for the project.</z><z id="t1479506982" t="joelkuiper Hey! I made https://github.com/joelkuiper/yesparql and was actually looking into integrating it with commons-rdf"><y>#</y><d>2016-11-18</d><h>22:09</h><w>joelkuiper</w>Hey! I made <a href="https://github.com/joelkuiper/yesparql" target="_blank">https://github.com/joelkuiper/yesparql</a> and was actually looking into integrating it with commons-rdf</z><z id="t1479506994" t="joelkuiper but haven’t gotten around to it, your project might be a good starting point 🙂"><y>#</y><d>2016-11-18</d><h>22:09</h><w>joelkuiper</w>but haven’t gotten around to it, your project might be a good starting point <b>🙂</b></z><z id="t1479507063" t="joelkuiper ideally yesparql would yield commons-rdf data structures, rather than its own records, since it would be more interoperable that way"><y>#</y><d>2016-11-18</d><h>22:11</h><w>joelkuiper</w>ideally yesparql would yield commons-rdf data structures, rather than its own records, since it would be more interoperable that way</z><z id="t1479594308" t="quoll [:attrs {:href &quot;/_/_/users/U349GLV7X&quot;}] unless you want to go with a fancy name that has nothing to do with the project, then what you’ve chosen is fine. Anyone seeing it will instantly know what it’s doing, and it will be a popular choice when looking at a Google search page"><y>#</y><d>2016-11-19</d><h>22:25</h><w>quoll</w><a>@stain</a> unless you want to go with a fancy name that has nothing to do with the project, then what you’ve chosen is fine. Anyone seeing it will instantly know what it’s doing, and it will be a popular choice when looking at a Google search page</z><z id="t1479747927" t="stain [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] but it makes a bad namespace (ns commons-rdf-clj.core) .."><y>#</y><d>2016-11-21</d><h>17:05</h><w>stain</w><a>@quoll</a> but it makes a bad namespace <code>(ns commons-rdf-clj.core)</code> ..</z><z id="t1479747934" t="stain I guess I can drop the -clj"><y>#</y><d>2016-11-21</d><h>17:05</h><w>stain</w>I guess I can drop the <code>-clj</code></z><z id="t1479747944" t="stain someone suggested I just steal (ns rdf)"><y>#</y><d>2016-11-21</d><h>17:05</h><w>stain</w>someone suggested I just steal <code>(ns rdf)</code></z><z id="t1479747971" t="stain [:attrs {:href &quot;/_/_/users/U0508P8EK&quot;}] cool stuff! Yes, your dicts are pretty similar to mine - I wonder if it would be possible for the protocols to support both dict types"><y>#</y><d>2016-11-21</d><h>17:06</h><w>stain</w><a>@joelkuiper</a> cool stuff! Yes, your dicts are pretty similar to mine - I wonder if it would be possible for the protocols to support both dict types</z><z id="t1479748002" t="stain you have a JSON-LD-like data structure"><y>#</y><d>2016-11-21</d><h>17:06</h><w>stain</w>you have a JSON-LD-like data structure</z><z id="t1479748012" t="stain or more like RDF/JSON perhaps"><y>#</y><d>2016-11-21</d><h>17:06</h><w>stain</w>or more like RDF/JSON perhaps</z><z id="t1479748246" t="stain [:attrs {:href &quot;/_/_/users/U0508P8EK&quot;}] BTW, Commons RDF 0.3.0 has Jena bindings (and RDF4J)"><y>#</y><d>2016-11-21</d><h>17:10</h><w>stain</w><a>@joelkuiper</a>  BTW,  Commons RDF 0.3.0 has Jena bindings (and RDF4J)</z><z id="t1479751126" t="joelkuiper [:attrs {:href &quot;/_/_/users/U349GLV7X&quot;}] at some point I’d like to move away from my own records and use CommonsRDF -&gt; Clojure, so your solution seems to fit there nicely"><y>#</y><d>2016-11-21</d><h>17:58</h><w>joelkuiper</w><a>@stain</a> at some point I’d like to move away from my own records and use CommonsRDF -&gt; Clojure, so your solution seems to fit there nicely</z><z id="t1479766251" t="stain Great! I&apos;ll try to mature it a bit so it works again. 🙂"><y>#</y><d>2016-11-21</d><h>22:10</h><w>stain</w>Great! I&apos;ll try to mature it a bit so it works again. <b>🙂</b></z><z id="t1479852521" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U349GLV7X&quot;}] : Check out grafter - we have pretty comprehensive protocols to coerce between RDF/clojure types"><y>#</y><d>2016-11-22</d><h>22:08</h><w>rickmoynihan</w><a>@stain</a>: Check out grafter - we have pretty comprehensive protocols to coerce between RDF/clojure types</z><z id="t1479852702" t="rickmoynihan its been in production use for a few years - and its pretty robust… it should cover all of RDF 1.1 and support all RDF serialisations etc... https://github.com/Swirrl/grafter/blob/master/src/rdf-common/grafter/rdf/io.clj"><y>#</y><d>2016-11-22</d><h>22:11</h><w>rickmoynihan</w>its been in production use for a few years - and its pretty robust… it should cover all of RDF 1.1 and support all RDF serialisations etc...

<a href="https://github.com/Swirrl/grafter/blob/master/src/rdf-common/grafter/rdf/io.clj" target="_blank">https://github.com/Swirrl/grafter/blob/master/src/rdf-common/grafter/rdf/io.clj</a></z><z id="t1479852891" t="rickmoynihan I also have a side project to do what you’re starting… I’m basically extracting the protocols from grafter into a separate project, with plans to implement them on Commons RDF, Sesame and Jena.. https://github.com/Swirrl/core.rdf"><y>#</y><d>2016-11-22</d><h>22:14</h><w>rickmoynihan</w>I also have a side project to do what you’re starting…  I’m basically extracting the protocols from grafter into a separate project, with plans to implement them on Commons RDF, Sesame and Jena..

<a href="https://github.com/Swirrl/core.rdf" target="_blank">https://github.com/Swirrl/core.rdf</a></z><z id="t1479853469" t="rickmoynihan The grafter stuff isn’t perfect - it has a few minor aesthetic issues which are largely resolved on the 0.8.0-snapshot branch (which is basically stable and will be released soon) but we’ve been using it for almost all of our RDF processing for almost 3 years… It also has a super simple turtle like DSL which you can use for writing triple templates e.g. (def template (graph-fn [{:keys [dataset-title dataset-slug dataset-uri dataset-graph observation-graph structure-uri]}] (graph dataset-graph [dataset-uri [rdf:a qb:DataSet] [qb:structure structure-uri] [rdfs:label (lang dataset-title :en)] [dcterms:title (lang dataset-title :en)] [dcterms:modified (modified-date)]]))) "><y>#</y><d>2016-11-22</d><h>22:24</h><w>rickmoynihan</w>The grafter stuff isn’t perfect - it has a few minor aesthetic issues which are largely resolved on the 0.8.0-snapshot branch (which is basically stable and will be released soon) but we’ve been using it for almost all of our RDF processing for almost 3 years… 

It also has a super simple turtle like DSL which you can use for writing triple templates e.g.

<pre>(def template
  (graph-fn [{:keys [dataset-title dataset-slug dataset-uri dataset-graph
                     observation-graph
                     structure-uri]}]
            (graph dataset-graph
                   [dataset-uri
                    [rdf:a qb:DataSet]
                    [qb:structure structure-uri]
                    [rdfs:label (lang dataset-title :en)]
                    [dcterms:title (lang dataset-title :en)]
                    [dcterms:modified (modified-date)]])))
</pre></z><z id="t1479853655" t="rickmoynihan I do really need to write a tutorial for the RDF side of things though"><y>#</y><d>2016-11-22</d><h>22:27</h><w>rickmoynihan</w>I do really need to write a tutorial for the RDF side of things though</z><z id="t1479854186" t="rickmoynihan I see you’re based in Manchester! Small world!! I run the Manchester Lambda Lounge http://lambdalounge.org.uk/"><y>#</y><d>2016-11-22</d><h>22:36</h><w>rickmoynihan</w>I see you’re based in Manchester!  Small world!!  I run the Manchester Lambda Lounge

<a href="http://lambdalounge.org.uk/" target="_blank">http://lambdalounge.org.uk/</a></z><z id="t1481711176" t="stain I documented my Commons RDF library attempt in https://github.com/stain/rdf-clj/ -- note that it&apos;s still early days and the functionality and interoperability is not all there yet. (and so I&apos;ve not put it in clojars)"><y>#</y><d>2016-12-14</d><h>10:26</h><w>stain</w>I documented my Commons RDF library attempt in <a href="https://github.com/stain/rdf-clj/" target="_blank">https://github.com/stain/rdf-clj/</a>  -- note that it&apos;s still early days and the functionality and interoperability is not all there yet. (and so I&apos;ve not put it in clojars)</z><z id="t1481711207" t="stain How often do I need to check in with Slack to avoid &quot;Your team has more than 10,000 messages&quot; and see the backlog?"><y>#</y><d>2016-12-14</d><h>10:26</h><w>stain</w>How often do I need to check in with Slack to avoid &quot;Your team has more than 10,000 messages&quot; and see the backlog?</z><z id="t1481827040" t="quoll that question totally depends on how active your Slack is. It will be a little less active over the next 2 weeks, and you’ll only need to log in every few days. On other occasions, you need to log in more than once a day. 🙂"><y>#</y><d>2016-12-15</d><h>18:37</h><w>quoll</w>that question totally depends on how active your Slack is. It will be a little less active over the next 2 weeks, and you’ll only need to log in every few days. On other occasions, you need to log in more than once a day. <b>🙂</b></z><z id="t1482195640" t="stain I released https://github.com/stain/rdf-clj 0.1.0 -- welcome any suggestions and feedback! Preferably in Github issue tracker as Slack does not work well with high traffic.."><y>#</y><d>2016-12-20</d><h>01:00</h><w>stain</w>I released <a href="https://github.com/stain/rdf-clj" target="_blank">https://github.com/stain/rdf-clj</a> 0.1.0 -- welcome any suggestions and feedback!   Preferably in Github issue tracker as Slack does not work well with high traffic..</z><z id="t1482195701" t="stain obviously it also needs to expose more of the graph methods - in particular iteration!"><y>#</y><d>2016-12-20</d><h>01:01</h><w>stain</w>obviously it also needs to expose more of the graph methods - in particular iteration!</z><z id="t1486630853" t="nblumoe Hey, I am getting started with Ontologies and LinkedData in the IoT domain. I am researching tools and libs in the Clojure space and would be very interested what you consider essential pieces in that area."><y>#</y><d>2017-02-09</d><h>09:00</h><w>nblumoe</w>Hey, I am getting started with Ontologies and LinkedData in the IoT domain. I am researching tools and libs in the Clojure space and would be very interested what you consider essential pieces in that area.</z><z id="t1486630874" t="nblumoe I already found https://github.com/phillord/tawny-owl and am checking through stuff on http://notes.3kbo.com/clojure now"><y>#</y><d>2017-02-09</d><h>09:01</h><w>nblumoe</w>I already found <a href="https://github.com/phillord/tawny-owl" target="_blank">https://github.com/phillord/tawny-owl</a> and am checking through stuff on <a href="http://notes.3kbo.com/clojure" target="_blank">http://notes.3kbo.com/clojure</a> now</z><z id="t1486742863" t="nblumoe [:attrs {:href &quot;/_/_/users/U09NZV6R3&quot;}] maybe you and fellow swirrlers have some good tips? Checking your github for now 👍"><y>#</y><d>2017-02-10</d><h>16:07</h><w>nblumoe</w><a>@ricroberts</a> maybe you and fellow swirrlers have some good tips? Checking your github for now <b>👍</b></z><z id="t1486752049" t="ricroberts hello."><y>#</y><d>2017-02-10</d><h>18:40</h><w>ricroberts</w>hello.</z><z id="t1486752224" t="ricroberts We use Sesame and Jena (Java) libs a lot. Most of the rest we’ve built on top ourselves! Grafter is our open source ETL framework. [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] might be able to add more comments on Monday."><y>#</y><d>2017-02-10</d><h>18:43</h><w>ricroberts</w>We use Sesame and Jena (Java) libs a lot. Most of the rest we’ve built on top ourselves! Grafter is our open source ETL framework. <a>@rickmoynihan</a> might be able to add more comments on Monday.</z><z id="t1486752252" t="ricroberts (our other code is not open source at the mo)"><y>#</y><d>2017-02-10</d><h>18:44</h><w>ricroberts</w>(our other code is not open source at the mo)</z><z id="t1486822086" t="nblumoe Thanks, will check those out!"><y>#</y><d>2017-02-11</d><h>14:08</h><w>nblumoe</w>Thanks, will check those out!</z><z id="t1486998401" t="ricroberts Sesame is now called rdf4j btw"><y>#</y><d>2017-02-13</d><h>15:06</h><w>ricroberts</w>Sesame is now called rdf4j btw</z><z id="t1487027698" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U066HBF6J&quot;}] : Nice to hear from you again 🙂 as ricroberts said http://grafter.org/ is a clojure layer over sesame (will update at some point to RDF4j) — half of grafter is concerned with RDF… with a few bits and pieces in grafter.tabular - to integrate it with incanter - though this side is less well developed — and will soon likely be split out into a separate dependency. The sesame layer is pretty well developed, and mainly provides low level coercion between clojure/rdf datatypes, and access to sesame’s rdf serialisers/deserialisers… I keep meaning to find some time to write some better docs on how to use this side of it. Main focus isn’t on ontologies/semantics though… We also have a yes-sparql style sparql api in latest builds - which supports binding style query generation and coerces RDF types into clojure types. Happy to explain more if you’re interested."><y>#</y><d>2017-02-13</d><h>23:14</h><w>rickmoynihan</w><a>@nblumoe</a>: Nice to hear from you again <b>🙂</b> as ricroberts said <a href="http://grafter.org/" target="_blank">http://grafter.org/</a> is a clojure layer over sesame (will update at some point to RDF4j) — half of grafter is concerned with RDF… with a few bits and pieces in grafter.tabular - to integrate it with incanter - though this side is less well developed — and will soon likely be split out into a separate dependency.

The sesame layer is pretty well developed, and mainly provides low level coercion between clojure/rdf datatypes, and access to sesame’s rdf serialisers/deserialisers… I keep meaning to find some time to write some better docs on how to use this side of it.  

Main focus isn’t on ontologies/semantics though…  We also have a yes-sparql style sparql api in latest builds - which supports binding style query generation and coerces RDF types into clojure types.  Happy to explain more if you’re interested.</z><z id="t1487027940" t="rickmoynihan It started life as an ETL tool to get data from tabular data into RDF… and we still mainly use it for that but probably most of the code which is under grafter.rdf is about raw RDF processing."><y>#</y><d>2017-02-13</d><h>23:19</h><w>rickmoynihan</w>It started life as an ETL tool to get data from tabular data into RDF… and we still mainly use it for that but probably most of the code which is under <code>grafter.rdf</code> is about raw RDF processing.</z><z id="t1487028593" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U066HBF6J&quot;}] : I know you have been an incanter user/contributor too — basically in the early days of grafter we adopted incanter Datasets so we can retain both the order of columns and access them by name/id… however incanter has a habit of eagerly loading everything — which makes it problematic for large ETL — so the functions in grafter.tabular tend towards lazily loading datasets… which is not without its issues too. Anyway curious what you think the future of incanter is; 1.9/2.0 and the promised implementation in terms of core.matrix protocols seems increasingly less likely to happen.... Is that a fair assessment?"><y>#</y><d>2017-02-13</d><h>23:29</h><w>rickmoynihan</w><a>@nblumoe</a>: I know you have been an incanter user/contributor too — basically in the early days of grafter we adopted incanter Datasets so we can retain both the order of columns and access them by name/id… however incanter has a habit of eagerly loading everything — which makes it problematic for large ETL — so the functions in grafter.tabular tend towards lazily loading datasets… which is not without its issues too.

Anyway curious what you think the future of incanter is; 1.9/2.0 and the promised implementation in terms of core.matrix protocols seems increasingly less likely to happen....  Is that a fair assessment?</z><z id="t1487058676" t="nblumoe Thanks for the information [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] ! I will have a deeper look at grafter soon hopefully. Atm I have to deal too much with designing an ontology (mostly using Protege, but had a look at Tawny-OWL too). I am not so happy with the tooling for that I have to say. Especially getting a good, interactive visual representation seems to be lacking."><y>#</y><d>2017-02-14</d><h>07:51</h><w>nblumoe</w>Thanks for the information <a>@rickmoynihan</a> ! I will have a deeper look at grafter soon hopefully. Atm I have to deal too much with designing an ontology (mostly using Protege, but had a look at Tawny-OWL too). I am not so happy with the tooling for that I have to say. Especially getting a good, interactive visual representation seems to be lacking.</z><z id="t1487058724" t="nblumoe So grafter might become more important once we put more effort into actually processing data. The tabular -&gt; rdf aspect might not be needed but I am curious about the RDF processing you mentioned."><y>#</y><d>2017-02-14</d><h>07:52</h><w>nblumoe</w>So grafter might become more important once we put more effort into actually processing data. The tabular -&gt; rdf aspect might not be needed but I am curious about the RDF processing you mentioned.</z><z id="t1487058930" t="nblumoe About Incanter: I think you assessment is fair unfortunately. Every once in a while I consider taking some action on this and driving it forward. But then I am just using R for the stuff I would be using Incanter for (algorithm development and data exploration). For the things were I did some actual analysis with Clojure I am using core.matrix directly. So I do not have such a strong need for Incanter myself, the (former) contributors became quite inactive and IMHO Incanter could need some good amount of refactoring and splitting up. I always wondered about the usage of Incanter: How actively is this being used and what are people doing with it?"><y>#</y><d>2017-02-14</d><h>07:55</h><w>nblumoe</w>About Incanter: I think you assessment is fair unfortunately. Every once in a while I consider taking some action on this and driving it forward. But then I am just using R for the stuff I would be using Incanter for (algorithm development and data exploration). For the things were I did some actual analysis with Clojure I am using core.matrix directly. So I do not have such a strong need for Incanter myself, the (former) contributors became quite inactive and IMHO Incanter could need some good amount of refactoring and splitting up. I always wondered about the usage of Incanter: How actively is this being used and what are people doing with it?</z><z id="t1487065815" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U066HBF6J&quot;}] : seems like we’re entirely in agreement about incanter then. For a while I had thoughts about moving the tabular side of grafter onto core.matrix - and implementing some lazy versions of the Dataset protocols… However if we were to make a radical shift on the ETL side of things I’d be much more inclined to build on top of transducers - i.e. viewing an ETL job as a transduction; and I doubt that would be compatible with the core.matrix protocols."><y>#</y><d>2017-02-14</d><h>09:50</h><w>rickmoynihan</w><a>@nblumoe</a>: seems like we’re entirely in agreement about incanter then.  For a while I had thoughts about moving the tabular side of grafter onto core.matrix - and implementing some lazy versions of the Dataset protocols…  However if we were to make a radical shift on the ETL side of things I’d be much more inclined to build on top of transducers - i.e. viewing an ETL job as a transduction; and I doubt that would be compatible with the core.matrix protocols.</z><z id="t1487066690" t="rickmoynihan Regarding the grafter/rdf processing side of things… it’s quite cleanly separated from the rest of things (just not at the project.clj dependencies level)… so 0.9 or 0.10 will likely be broken into separate clojar artifacts. If you have any questions about it I can answer them… but we have pretty good support for all sesame repositories (native disk stored ones / memory repo&apos;s (great for testing) &amp; remote sparql repo’s too… also reading/writing RDF in basically any serialisation format, and as I said the coercions of data from sesame RDF type’s to native clojure/java ones (with a protocol layer that basically makes native types all behave like RDF types e.g. you can do (datatype-uri 10) ;; =&gt; and get the proper URI back etc… also coercions for #inst ‘s (java Date’s/DateTime&apos;s etc…)."><y>#</y><d>2017-02-14</d><h>10:04</h><w>rickmoynihan</w>Regarding the grafter/rdf processing side of things… it’s quite cleanly separated from the rest of things (just not at the project.clj dependencies level)…  so 0.9 or 0.10 will likely be broken into separate clojar artifacts.

If you have any questions about it I can answer them… but we have pretty good support for all sesame repositories (native disk stored ones / memory repo&apos;s (great for testing) &amp; remote sparql repo’s too… also reading/writing RDF in basically any serialisation format, and as I said the coercions of data from sesame RDF type’s to native clojure/java ones (with a protocol layer that basically makes native types all behave like RDF types e.g. you can do <code>(datatype-uri 10) ;; =&gt; </code> and get the proper URI back etc… also coercions for <code>#inst</code>‘s (java Date’s/DateTime&apos;s etc…).</z><z id="t1487066956" t="rickmoynihan longer term I have plans to improve the shape of the API (core.rdf — currently WIP - and not updated for 7 months) and implement it as a generic protocol layer that’s extended to both RDF4j and sesame… Since then [:attrs {:href &quot;/_/_/users/U349GLV7X&quot;}] has independently started a similar project to do a similar thing - and we’ve spoken about potentially collaborating on something - but so far no movement on that."><y>#</y><d>2017-02-14</d><h>10:09</h><w>rickmoynihan</w>longer term I have plans to improve the shape of the API (core.rdf — currently WIP - and not updated for 7 months) and implement it as a generic protocol layer that’s extended to both RDF4j and sesame…  Since then <a>@stain</a> has independently started a similar project to do a similar thing - and we’ve spoken about potentially collaborating on something - but so far no movement on that.</z><z id="t1487067013" t="rickmoynihan Anyway we have lots of plans to improve things… and have plans on improving things in future versions - but we’re making incremental improvements/updates all the time… but we have to move relatively slowly because we have a lot of code built ontop of it."><y>#</y><d>2017-02-14</d><h>10:10</h><w>rickmoynihan</w>Anyway we have lots of plans to improve things… and have plans on improving things in future versions - but we’re making incremental improvements/updates all the time… but we have to move relatively slowly because we have a lot of code built ontop of it.</z><z id="t1487067198" t="rickmoynihan 0.8.x-SNAPSHOT is the latest btw — it’ll be merged to master soon — maybe in the next month or so… and it’s basically stable for anything you’re likely to use it for."><y>#</y><d>2017-02-14</d><h>10:13</h><w>rickmoynihan</w><code>0.8.x-SNAPSHOT</code> is the latest btw — it’ll be merged to master soon — maybe in the next month or so… and it’s basically stable for anything you’re likely to use it for.</z><z id="t1487241336" t="stain thanks, [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] - let&apos;s try to meet up soon and talk about a common path forward. I&apos;m sorry I&apos;ve been a bit swamped this month with all these grant proposals."><y>#</y><d>2017-02-16</d><h>10:35</h><w>stain</w>thanks, <a>@rickmoynihan</a> - let&apos;s try to meet up soon and talk about a common path forward. I&apos;m sorry I&apos;ve been a bit swamped this month with all these grant proposals.</z><z id="t1487241453" t="stain let me know if you&apos;re around Oxford Rd one day (Mon/Thu/Fri) 🙂"><y>#</y><d>2017-02-16</d><h>10:37</h><w>stain</w>let me know if you&apos;re around Oxford Rd one day (Mon/Thu/Fri) <b>🙂</b></z><z id="t1487243019" t="rickmoynihan stain: Unlikely to be around that part of MCR… but will be at Lambda Lounge on Monday night if you’re about then"><y>#</y><d>2017-02-16</d><h>11:03</h><w>rickmoynihan</w>stain: Unlikely to be around that part of MCR… but will be at Lambda Lounge on Monday night if you’re about then</z><z id="t1536158133" t="rickmoynihan Experimental library for querying RDF models in clojure: https://github.com/Swirrl/matcha"><y>#</y><d>2018-09-05</d><h>14:35</h><w>rickmoynihan</w>Experimental library for querying RDF models in clojure: <a href="https://github.com/Swirrl/matcha" target="_blank">https://github.com/Swirrl/matcha</a></z><z id="t1584636477" t="Eric Scott Reposting here from #announcements :"><y>#</y><d>2020-03-19</d><h>16:47</h><w>Eric Scott</w>Reposting here from #announcements :</z><z id="t1584636480" t="Eric Scott https://github.com/ont-app/igraph"><y>#</y><d>2020-03-19</d><h>16:48</h><w>Eric Scott</w><a href="https://github.com/ont-app/igraph" target="_blank">https://github.com/ont-app/igraph</a></z><z id="t1584636541" t="rickmoynihan :thumbsup: [:attrs {:href &quot;/_/_/users/UB3R8UYA1&quot;}] I’m curious what your plans for the above and the other ont-app projects. It seems like you’re taking similar approaches, or at least trying to solve some of the same problems as myself"><y>#</y><d>2020-03-19</d><h>16:49</h><w>rickmoynihan</w><b>:thumbsup:</b>

<a>@eric.d.scott</a> I’m curious what your plans for the above and the other ont-app projects.

It seems like you’re taking similar approaches, or at least trying to solve some of the same problems as myself</z><z id="t1584636678" t="Eric Scott Well, the motivation for the project is that I think there could be a really good fit between clojure&apos;s functional programming paradigm and graph-based representations that use a ontology-driven approach."><y>#</y><d>2020-03-19</d><h>16:51</h><w>Eric Scott</w>Well, the motivation for the project is that I think there could be a really good fit between clojure&apos;s functional programming paradigm and graph-based representations that use a ontology-driven approach.</z><z id="t1584636757" t="Eric Scott I spent several years working in Python with semantic technologies, and when I came back to Clojure, I had this feeling that S-P-O-type graphs could make a really expressive sort of &apos;super-map&apos;"><y>#</y><d>2020-03-19</d><h>16:52</h><w>Eric Scott</w>I spent several years working in Python with semantic technologies, and when I came back to Clojure, I had this feeling that S-P-O-type graphs could make a really expressive sort of &apos;super-map&apos;</z><z id="t1584636792" t="rickmoynihan &gt; S-P-O-type graphs could make a really expressive sort of ‘super-map’ This is definitely my experience too. It’s why I wrote matcha"><y>#</y><d>2020-03-19</d><h>16:53</h><w>rickmoynihan</w>&gt; S-P-O-type graphs could make a really expressive sort of ‘super-map’
This is definitely my experience too.  It’s why I wrote matcha</z><z id="t1584636904" t="rickmoynihan Also I think other people have had similar ideas; cgrand spoke about this at clojurex last year — IIRC he called it “map fatigue”, and was suggesting datalog approaches. Obviously not a new idea, e.g. datomic/datascript particularly in the browser etc. Datalog is almost sparql though right?"><y>#</y><d>2020-03-19</d><h>16:55</h><w>rickmoynihan</w>Also I think other people have had similar ideas; cgrand spoke about this at clojurex last year — IIRC he called it “map fatigue”, and was suggesting datalog approaches.  Obviously not a new idea, e.g. datomic/datascript particularly in the browser etc.

Datalog is almost sparql though right?</z><z id="t1584636957" t="rickmoynihan need to go AFK for 10-15mins be back soon though"><y>#</y><d>2020-03-19</d><h>16:55</h><w>rickmoynihan</w>need to go AFK for  10-15mins be back soon though</z><z id="t1584637003" t="Eric Scott Yes I remember seeing Matcha and making a mental note to revisit it after I finished my thought with igraph."><y>#</y><d>2020-03-19</d><h>16:56</h><w>Eric Scott</w>Yes I remember seeing Matcha and making a mental note to revisit it after I finished my thought with igraph.</z><z id="t1584639138" t="rickmoynihan Main usecase matcha was made for is to provide BGP queries over in memory RDF graphs. Typically we use it to after CONSTRUCT ing a subset of graph data from the triplestore with sparql; then we throw it into a matcha graph; and query that model a bunch of times to build out a UI/tree."><y>#</y><d>2020-03-19</d><h>17:32</h><r>rickmoynihan</r>Main usecase matcha was made for is to provide BGP queries over in memory RDF graphs.

Typically we use it to after <code>CONSTRUCT</code> ing a subset of graph data from the triplestore with sparql; then we throw it into a matcha graph; and query that model a bunch of  times to build out a UI/tree.</z><z id="t1584637113" t="Eric Scott I&apos;m kind of looking for a common abstraction over a whole range of graph-based representations. Right now I&apos;m in the early stages of sussing out an approach to neo4j."><y>#</y><d>2020-03-19</d><h>16:58</h><w>Eric Scott</w>I&apos;m kind of looking for a common abstraction over a whole range of graph-based representations. Right now I&apos;m in the early stages of sussing out an approach to neo4j.</z><z id="t1584637423" t="Eric Scott At some point, I&apos;m hoping that some common approach can be developed for bringing all these disparate query languages under the same tent."><y>#</y><d>2020-03-19</d><h>17:03</h><w>Eric Scott</w>At some point, I&apos;m hoping that some common approach can be developed for bringing all these disparate query languages under the same tent.</z><z id="t1584637828" t="Eric Scott I guess the talk you&apos;re referring to is described here:"><y>#</y><d>2020-03-19</d><h>17:10</h><w>Eric Scott</w>I guess the talk you&apos;re referring to is described here:</z><z id="t1584637829" t="Eric Scott https://dev.solita.fi/2018/12/14/clojurex-2018.html"><y>#</y><d>2020-03-19</d><h>17:10</h><w>Eric Scott</w><a href="https://dev.solita.fi/2018/12/14/clojurex-2018.html" target="_blank">https://dev.solita.fi/2018/12/14/clojurex-2018.html</a></z><z id="t1584638944" t="rickmoynihan yes that’s the one"><y>#</y><d>2020-03-19</d><h>17:29</h><r>rickmoynihan</r>yes that’s the one</z><z id="t1584638882" t="rickmoynihan &gt; At some point, I’m hoping that some common approach can be developed for bringing all these disparate query languages under the same tent. This is what I don’t really understand; for what purpose?"><y>#</y><d>2020-03-19</d><h>17:28</h><w>rickmoynihan</w>&gt; At some point, I’m hoping that some common approach can be developed for bringing all these disparate query languages under the same tent.
This is what I don’t really understand; for what purpose?</z><z id="t1584639120" t="Eric Scott The idea of this protocol is to serve as an abstraction over a variety of graph implementations."><y>#</y><d>2020-03-19</d><h>17:32</h><w>Eric Scott</w>The idea of this protocol is to serve as an abstraction over a variety of graph implementations.</z><z id="t1584639215" t="rickmoynihan But what kind of libraries or applications want that portability?"><y>#</y><d>2020-03-19</d><h>17:33</h><w>rickmoynihan</w>But what kind of libraries or applications want that portability?</z><z id="t1585259348" t="quoll Naga was written specifically to be agnostic over databases like that. (The original goal was to switch between Datomic and SPARQL, and expand from there)"><y>#</y><d>2020-03-26</d><h>21:49</h><r>quoll</r>Naga was written specifically to be agnostic over databases like that. (The original goal was to switch between Datomic and SPARQL, and expand from there)</z><z id="t1585259350" t="quoll Sorry for the late response. I don’t look at the Clojurians slack very frequently."><y>#</y><d>2020-03-26</d><h>21:49</h><r>quoll</r>Sorry for the late response. I don’t look at the Clojurians slack very frequently.</z><z id="t1584639218" t="Eric Scott The various IFn forms allow some degree of generalization, but if you want to take a common view of datomic-based and say Wikidata-based content, you have to write datalog for one and SPARQL for the other."><y>#</y><d>2020-03-19</d><h>17:33</h><w>Eric Scott</w>The various IFn forms allow some degree of generalization, but if you want to take a common view of datomic-based and say Wikidata-based content, you have to write datalog for one and SPARQL for the other.</z><z id="t1584639457" t="Eric Scott The domain-driven- and behavior-driven-development folks talk a lot of about &quot;Ubiquitous Vocabulary&quot;. A lot of what&apos;s driving my thinking here is that there could be an ontology-driven approach to development where that vocabulary takes the form of an ontology."><y>#</y><d>2020-03-19</d><h>17:37</h><w>Eric Scott</w>The domain-driven- and behavior-driven-development folks talk a lot of about &quot;Ubiquitous Vocabulary&quot;.  A lot of what&apos;s driving my thinking here is that there could be an ontology-driven approach to development where that vocabulary takes the form of an ontology.</z><z id="t1584640220" t="rickmoynihan [:attrs {:href &quot;/_/_/users/UB3R8UYA1&quot;}] Sure. I don’t disagree with any of that, though I’m not sure I understand whats special about the approach. If you were doing it in an RDBMS, you’d just model the the vocabulary in tables and foreign keys. Isn’t it natural that if you were to do it in RDF you’d coin an ontology? Or do you see it more the other way round, bringing Ontologies to non-RDF systems?"><y>#</y><d>2020-03-19</d><h>17:50</h><r>rickmoynihan</r><a>@eric.d.scott</a> Sure.  I don’t disagree with any of that, though I’m not sure I understand whats special about the approach.

If you were doing it in an RDBMS, you’d just model the the vocabulary in tables and foreign keys.

Isn’t it natural that if you were to do it in RDF you’d coin an ontology?

Or do you see it more the other way round,  bringing Ontologies to non-RDF systems?</z><z id="t1584640278" t="Eric Scott Yes, bringing ontologies to non-RDF systems."><y>#</y><d>2020-03-19</d><h>17:51</h><r>Eric Scott</r>Yes, bringing ontologies to non-RDF systems.</z><z id="t1584640387" t="rickmoynihan ok cool… next question: What do you think ontologies bring to these other systems? They typically have some form of schema already — which is more or less equivalent."><y>#</y><d>2020-03-19</d><h>17:53</h><r>rickmoynihan</r>ok cool… next question:

What do you think ontologies bring to these other systems?  They typically have some form of schema already — which is more or less equivalent.</z><z id="t1584640432" t="rickmoynihan open world modelling assumption?"><y>#</y><d>2020-03-19</d><h>17:53</h><r>rickmoynihan</r>open world modelling assumption?</z><z id="t1584640464" t="rickmoynihan globally shared, namespaced property identifiers?"><y>#</y><d>2020-03-19</d><h>17:54</h><r>rickmoynihan</r>globally shared, namespaced property identifiers?</z><z id="t1584640487" t="rickmoynihan access to existing RDF vocabularies/ontologies?"><y>#</y><d>2020-03-19</d><h>17:54</h><r>rickmoynihan</r>access to existing RDF vocabularies/ontologies?</z><z id="t1584640534" t="Eric Scott Yes, the primary benefit here is I think the expressiveness of a common, public vocabulary."><y>#</y><d>2020-03-19</d><h>17:55</h><r>Eric Scott</r>Yes, the primary benefit here is I think the expressiveness of a common, public vocabulary.</z><z id="t1584640637" t="Eric Scott Speaking of the OWA, I wonder if a better alternative might be in many cases to just publish your close-world data as an immutable graph, then leave it to me to decide whether and how to use that data as a resource."><y>#</y><d>2020-03-19</d><h>17:57</h><r>Eric Scott</r>Speaking of the OWA, I wonder if a better alternative might be in many cases to just publish your close-world data as an immutable graph, then leave it to me to decide whether and how to use that data as a resource.</z><z id="t1584640696" t="Eric Scott The hoops that OWL makes you jump through in order to support the OWA add a lot of friction to the whole process."><y>#</y><d>2020-03-19</d><h>17:58</h><r>Eric Scott</r>The hoops that OWL makes you jump through in order to support the OWA add a lot of friction to the whole process.</z><z id="t1584640742" t="Eric Scott I remember going to a conference a few years back, and Peter Norvig was the invited speaker. The subject of RDF came up in the context of Knowledge Graphs."><y>#</y><d>2020-03-19</d><h>17:59</h><r>Eric Scott</r>I remember going to a conference a few years back, and Peter Norvig was the invited speaker. The subject of RDF came up in the context of Knowledge Graphs.</z><z id="t1584640759" t="rickmoynihan haha I’ve heard this story I think…"><y>#</y><d>2020-03-19</d><h>17:59</h><r>rickmoynihan</r>haha I’ve heard this story I think…</z><z id="t1584640773" t="rickmoynihan semantic web is always X years away?"><y>#</y><d>2020-03-19</d><h>17:59</h><r>rickmoynihan</r>semantic web is always X years away?</z><z id="t1584640781" t="Eric Scott He said something to the effect that he wouldn&apos;t feel right making his engineers wrap their heads around OWL."><y>#</y><d>2020-03-19</d><h>17:59</h><r>Eric Scott</r>He said something to the effect that he wouldn&apos;t feel right  making his engineers wrap their heads around OWL.</z><z id="t1584640834" t="Eric Scott When the head of research for Google doesn&apos;t want to subject his engineers to your technology, I think that&apos;s significant."><y>#</y><d>2020-03-19</d><h>18:00</h><r>Eric Scott</r>When the head of research for Google doesn&apos;t want to subject his engineers to your technology, I think that&apos;s significant.</z><z id="t1584640913" t="rickmoynihan &gt; Speaking of the OWA, I wonder if a better alternative might be in many cases to just publish your close-world data as an immutable graph, then leave it to me to decide whether and how to use that data as a resource. &gt; The hoops that OWL makes you jump through in order to support the OWA add a lot of friction to the whole process. Indeed. I think in practice that’s what most people do already though. Almost all real applications want to make some closed world assumptions about the data."><y>#</y><d>2020-03-19</d><h>18:01</h><r>rickmoynihan</r>&gt; Speaking of the OWA, I wonder if a better alternative might be in many cases to just publish your close-world data as an immutable graph, then leave it to me to decide whether and how to use that data as a resource.
&gt; The hoops that OWL makes you jump through in order to support the OWA add a lot of friction to the whole process.
Indeed.

I think in practice that’s what most people do already though.

Almost all real applications want to make some closed world assumptions about the data.</z><z id="t1584640981" t="rickmoynihan however I think OWA is useful in a modelling context. I just think you often want additional closed world constraints on the data you load."><y>#</y><d>2020-03-19</d><h>18:03</h><r>rickmoynihan</r>however I think OWA is useful in a modelling context.  I just think you often want additional closed world constraints on the data you load.</z><z id="t1584641072" t="rickmoynihan backing up a bit… There’s a difference between OWL and RDF though. When you speak about bringing ontologies to other systems, do you mean OWL-like reasoning?"><y>#</y><d>2020-03-19</d><h>18:04</h><r>rickmoynihan</r>backing up a bit… There’s a difference between OWL and RDF though.

When you speak about bringing ontologies to other systems, do you mean OWL-like reasoning?</z><z id="t1584641158" t="rickmoynihan Or is that the bit you want to leave out?"><y>#</y><d>2020-03-19</d><h>18:05</h><r>rickmoynihan</r>Or is that the bit you want to leave out?</z><z id="t1584641241" t="Eric Scott That&apos;s a bit I&apos;d like to make optional 🙂"><y>#</y><d>2020-03-19</d><h>18:07</h><r>Eric Scott</r>That&apos;s a bit I&apos;d like to make optional <b>🙂</b></z><z id="t1584641265" t="Eric Scott As I understand it, default logic is inconsistent with the OWA."><y>#</y><d>2020-03-19</d><h>18:07</h><r>Eric Scott</r>As I understand it, default logic is inconsistent with the OWA.</z><z id="t1584641275" t="rickmoynihan Well it already is optional — even in RDF. 🙂"><y>#</y><d>2020-03-19</d><h>18:07</h><r>rickmoynihan</r>Well it already is optional — even in RDF. <b>🙂</b></z><z id="t1584641421" t="Eric Scott I definitely want to leave the door open to putting whatever data you have into a triple/quad store, and firing up the reasoner, if you can tame your data to the point where everything fits together just so."><y>#</y><d>2020-03-19</d><h>18:10</h><r>Eric Scott</r>I definitely want to leave the door open to putting whatever data you have into a triple/quad store, and firing up the reasoner, if you can tame your data to the point where everything fits together just so.</z><z id="t1584641482" t="Eric Scott My experience with RDF was almost entirely ingesting crowd-sourced Linked Data, and my data was seldom that tame. I mostly used property paths and did my reasoning lazily."><y>#</y><d>2020-03-19</d><h>18:11</h><r>Eric Scott</r>My experience with RDF was almost entirely ingesting crowd-sourced Linked Data, and my data was seldom that tame. I mostly used property paths and did my reasoning lazily.</z><z id="t1584641503" t="Eric Scott The traversal functions in IGraph are directly inspired by that."><y>#</y><d>2020-03-19</d><h>18:11</h><r>Eric Scott</r>The traversal functions in IGraph are directly inspired by that.</z><z id="t1584641611" t="rickmoynihan Funny you should mention defeasible logics; I used to work for a company that made what was I think at the time one of the first real implementations of a defeasible reasoner — based on the logic of Henry Prakken."><y>#</y><d>2020-03-19</d><h>18:13</h><r>rickmoynihan</r>Funny you should mention defeasible logics; I used to work for a company that made what was I think at the time one of the first real implementations of a defeasible reasoner — based on the logic of Henry Prakken.</z><z id="t1584641782" t="rickmoynihan Either way it’s an interesting objection to raise against OWL. At the time we certainly had those objections to it too… IIRC our CTO felt OWL was pretty conservative. Are you actually applying or wanting to apply defeasible reasoning against your data? If so what for?"><y>#</y><d>2020-03-19</d><h>18:16</h><r>rickmoynihan</r>Either way it’s an interesting objection to raise against OWL.  At the time we certainly had those objections to it too… IIRC our CTO felt OWL was pretty conservative.

Are you actually applying or wanting to apply defeasible reasoning against your data?  If so what for?</z><z id="t1584642087" t="Eric Scott Well I&apos;m far from the point where I&apos;m ready to announce this yet, but I&apos;ve started a project here: https://github.com/ont-app/prototypes"><y>#</y><d>2020-03-19</d><h>18:21</h><r>Eric Scott</r>Well I&apos;m far from the point where I&apos;m ready to announce this yet, but I&apos;ve started a project here: <a href="https://github.com/ont-app/prototypes" target="_blank">https://github.com/ont-app/prototypes</a></z><z id="t1584642135" t="Eric Scott It&apos;s quite raw, and even the README is crappy."><y>#</y><d>2020-03-19</d><h>18:22</h><r>Eric Scott</r>It&apos;s quite raw, and even the README is crappy.</z><z id="t1584642151" t="rickmoynihan we will forgive you having one crap readme 🙂"><y>#</y><d>2020-03-19</d><h>18:22</h><r>rickmoynihan</r>we will forgive you having one crap readme <b>🙂</b></z><z id="t1584642159" t="rickmoynihan the others were great"><y>#</y><d>2020-03-19</d><h>18:22</h><r>rickmoynihan</r>the others were great</z><z id="t1584642182" t="Eric Scott Oh thanks!"><y>#</y><d>2020-03-19</d><h>18:23</h><r>Eric Scott</r>Oh thanks!</z><z id="t1584642209" t="Eric Scott If you&apos;re interested, I guess you might get some idea from the test file:"><y>#</y><d>2020-03-19</d><h>18:23</h><r>Eric Scott</r>If you&apos;re interested, I guess you might get some idea from the test file:</z><z id="t1584642210" t="Eric Scott https://github.com/ont-app/prototypes/blob/master/test/ont_app/prototypes/core_test.cljc"><y>#</y><d>2020-03-19</d><h>18:23</h><r>Eric Scott</r><a href="https://github.com/ont-app/prototypes/blob/master/test/ont_app/prototypes/core_test.cljc" target="_blank">https://github.com/ont-app/prototypes/blob/master/test/ont_app/prototypes/core_test.cljc</a></z><z id="t1584642338" t="rickmoynihan hold on let me find something you’ll find interesting"><y>#</y><d>2020-03-19</d><h>18:25</h><r>rickmoynihan</r>hold on let me find something you’ll find interesting</z><z id="t1584639694" t="Eric Scott So the idea is that you sit down with your domain expert and systematically name the pertinent set of collections and relationships, but instead of parlaying that into a set of Java classes, you use this vocabulary to describe your application state as a graph or perhaps a set of graphs."><y>#</y><d>2020-03-19</d><h>17:41</h><w>Eric Scott</w>So the idea is that you sit down with your domain expert and systematically name the pertinent set of collections and relationships, but instead of parlaying that into a set of Java classes, you use this vocabulary to describe your application state as a graph or perhaps a set of graphs.</z><z id="t1584639745" t="Eric Scott RDF graphs make sense for many cases, but say, Datomic and Neo4j make sense for others."><y>#</y><d>2020-03-19</d><h>17:42</h><w>Eric Scott</w>RDF graphs make sense for many cases, but say, Datomic and Neo4j make sense for others.</z><z id="t1584639831" t="Eric Scott There may even be advantages to viewing table-based data or web APIs using the same basic abstraction."><y>#</y><d>2020-03-19</d><h>17:43</h><w>Eric Scott</w>There may even be advantages to viewing table-based data or web APIs using the same basic abstraction.</z><z id="t1584639970" t="Ivan the way I view things, if you have such an encoding in RDF, you can then model -on top of it- graph relations or other types of relations. How e-a-v is ordered and indexed makes this possible."><y>#</y><d>2020-03-19</d><h>17:46</h><w>Ivan</w>the way I view things, if you have such an encoding in RDF, you can then model -on top of it- graph relations or other types of relations. How e-a-v is ordered and indexed makes this possible.</z><z id="t1584640031" t="Eric Scott Would this involve translating your e-a-v data into a triple store?"><y>#</y><d>2020-03-19</d><h>17:47</h><w>Eric Scott</w>Would this involve translating your e-a-v data into a triple store?</z><z id="t1584641250" t="Ivan https://www.youtube.com/watch?v=dQWcD2_FzAU#t=28m6s on 28:06"><y>#</y><d>2020-03-19</d><h>18:07</h><r>Ivan</r><a href="https://www.youtube.com/watch?v=dQWcD2_FzAU#t=28m6s" target="_blank">https://www.youtube.com/watch?v=dQWcD2_FzAU#t=28m6s</a>

on 28:06</z><z id="t1584640745" t="Ivan What I understand that you&apos;re talking about is a generic interface that gives you the ability to plug into different systems to manipulate the data. Which, in other words, means that the data are &quot;viewed&quot; in different formats by the (db) systems, but can be manipulated in other formats, too - because this generic interface allows for this. Which in turn reminds me of the &quot;Polyglot Data&quot; talk by Greg Young - ie, the database system is a projection of the data; data can have multiple projections but only a single source."><y>#</y><d>2020-03-19</d><h>17:59</h><w>Ivan</w>What I understand that you&apos;re talking about is a generic interface that gives you the ability to plug into different systems to manipulate the data. Which, in other words, means that the data are &quot;viewed&quot; in different formats by the (db) systems, but can be manipulated in other formats, too - because this generic interface allows for this.
Which in turn reminds me of the &quot;Polyglot Data&quot; talk by Greg Young - ie, the database system is a projection of the data; data can have multiple projections but only a single source.</z><z id="t1584707885" t="Eric Scott Thanks [:attrs {:href &quot;/_/_/users/U8ZE1VBSS&quot;}] , that was a good talk."><y>#</y><d>2020-03-20</d><h>12:38</h><r>Eric Scott</r>Thanks <a>@U8ZE1VBSS</a>, that was a good talk.</z><z id="t1584708317" t="Ivan it is mixed with the whole event-sourcing paradigm (although there is good reason for that: reducing complexity through immutability), but the concept, I think, is similar. From what I understand, you have very close goals. - Greg says that there is single source of data, and adapters expose the data in different formats through the different DBMS systems. - You say that there is a common interface; an API that allows you to talk to the different systems, as if the data bellow them had the same format. He centralizes on the data (data-first); you centralize on the interface (function-first). There are pros and cons to those choices that can make up a nice discussion. Who is lifting the weight in each case, what way provides more flexibility in case something is found to be &quot;incompatible&quot;, etc"><y>#</y><d>2020-03-20</d><h>12:45</h><r>Ivan</r>it is mixed with the whole event-sourcing paradigm (although there is good reason for that: reducing complexity through immutability), but the concept, I think, is similar.

From what I understand, you have very close goals.
- Greg says that there is single source of data, and adapters expose the data in different formats through the different DBMS systems.
- You say that there is a common interface; an API that allows you to talk to the different systems, as if the data bellow them had the same format.
He centralizes on the data (data-first); you centralize on the interface (function-first).

There are pros and cons to those choices that can make up a nice discussion. Who is lifting the weight in each case, what way provides more flexibility in case something is found to be &quot;incompatible&quot;, etc</z><z id="t1584640963" t="Eric Scott This one, I guess?"><y>#</y><d>2020-03-19</d><h>18:02</h><w>Eric Scott</w>This one, I guess?</z><z id="t1584640965" t="Eric Scott https://www.youtube.com/watch?v=hv2dKtPq0ME"><y>#</y><d>2020-03-19</d><h>18:02</h><w>Eric Scott</w><a href="https://www.youtube.com/watch?v=hv2dKtPq0ME" target="_blank">https://www.youtube.com/watch?v=hv2dKtPq0ME</a></z><z id="t1584641017" t="Eric Scott Thanks, I&apos;ll have to watch this."><y>#</y><d>2020-03-19</d><h>18:03</h><w>Eric Scott</w>Thanks, I&apos;ll have to watch this.</z><z id="t1584708317" t="Ivan it is mixed with the whole event-sourcing paradigm (although there is good reason for that: reducing complexity through immutability), but the concept, I think, is similar. From what I understand, you have very close goals. - Greg says that there is single source of data, and adapters expose the data in different formats through the different DBMS systems. - You say that there is a common interface; an API that allows you to talk to the different systems, as if the data bellow them had the same format. He centralizes on the data (data-first); you centralize on the interface (function-first). There are pros and cons to those choices that can make up a nice discussion. Who is lifting the weight in each case, what way provides more flexibility in case something is found to be &quot;incompatible&quot;, etc"><y>#</y><d>2020-03-20</d><h>12:45</h><w>Ivan</w>it is mixed with the whole event-sourcing paradigm (although there is good reason for that: reducing complexity through immutability), but the concept, I think, is similar.

From what I understand, you have very close goals.
- Greg says that there is single source of data, and adapters expose the data in different formats through the different DBMS systems.
- You say that there is a common interface; an API that allows you to talk to the different systems, as if the data bellow them had the same format.
He centralizes on the data (data-first); you centralize on the interface (function-first).

There are pros and cons to those choices that can make up a nice discussion. Who is lifting the weight in each case, what way provides more flexibility in case something is found to be &quot;incompatible&quot;, etc</z><z id="t1584704961" t="rickmoynihan &gt; joined #rdf along with 13 others. Wow, it’s like RDF became popular all of a sudden!"><y>#</y><d>2020-03-20</d><h>11:49</h><w>rickmoynihan</w>&gt; joined #rdf along with 13 others.
Wow, it’s like RDF became popular all of a sudden!</z><z id="t1584705191" t="akond it is popular (among 65 people)."><y>#</y><d>2020-03-20</d><h>11:53</h><w>akond</w>it is popular (among 65 people).</z><z id="t1584705218" t="rickmoynihan There were a lot less people yesterday tho! 🙂"><y>#</y><d>2020-03-20</d><h>11:53</h><w>rickmoynihan</w>There were a lot less people yesterday tho! <b>🙂</b></z><z id="t1584706045" t="Eric Scott Out of curiosity, How many people here besides [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] are using RDF in production, or something that supports something in production?"><y>#</y><d>2020-03-20</d><h>12:07</h><w>Eric Scott</w>Out of curiosity, How many people here besides <a>@rickmoynihan</a> are using RDF in production, or something that supports something in production?</z><z id="t1584708611" t="refset We use a couple of RDF datasets to benchmark #crux against other DBs: https://dsg.uwaterloo.ca/watdiv/ http://swat.cse.lehigh.edu/projects/lubm/ (the bench runs nightly)"><y>#</y><d>2020-03-20</d><h>12:50</h><r>refset</r>We use a couple of RDF datasets to benchmark #crux against other DBs: <a href="https://dsg.uwaterloo.ca/watdiv/" target="_blank">https://dsg.uwaterloo.ca/watdiv/</a> <a href="http://swat.cse.lehigh.edu/projects/lubm/" target="_blank">http://swat.cse.lehigh.edu/projects/lubm/</a> (the bench runs nightly)</z><z id="t1585077911" t="joelkuiper We do for https://covid-search.doctorevidence.com/"><y>#</y><d>2020-03-24</d><h>19:25</h><r>joelkuiper</r>We do for <a href="https://covid-search.doctorevidence.com/" target="_blank">https://covid-search.doctorevidence.com/</a></z><z id="t1585077926" t="joelkuiper backing ontology is RDF via Jena/Fuseki and YeSPARQL"><y>#</y><d>2020-03-24</d><h>19:25</h><r>joelkuiper</r>backing ontology is RDF via Jena/Fuseki and YeSPARQL</z><z id="t1584708794" t="Eric Scott Yeah, you kind of need both of these at the same time, so putting one or the other &apos;first&apos; seems a little like saying &apos;right foot first&apos; vs. &apos;left-foot-first&apos;. 🙂"><y>#</y><d>2020-03-20</d><h>12:53</h><w>Eric Scott</w>Yeah, you kind of need both of these at  the same time, so putting one or the other &apos;first&apos; seems a little like saying &apos;right foot first&apos; vs. &apos;left-foot-first&apos;. <b>🙂</b></z><z id="t1584708861" t="Eric Scott I really love Clojure&apos;s map abstraction, and a lot of what motivated the IGraph project was a desire to extend a similar abstraction to the expression of relationships."><y>#</y><d>2020-03-20</d><h>12:54</h><w>Eric Scott</w>I really love Clojure&apos;s map abstraction, and a lot of what motivated the IGraph project was a desire to extend a similar abstraction to the expression of relationships.</z><z id="t1584710245" t="rickmoynihan Maps are great but have their limits; analagous to structured databases vs relational. At some point you realise you’re building a database with your datastructures, e.g. when you start computing lookup tables, and having lists of denormalised/sorted keys etc; which you then need bespoke functions to handle. Rich Hickey: Every class is an island. Rick Moynihan: Your datastructure’s become islands too 🙂 At that point you want a query language; and graphs are super flexible In all fairness Rich has known this since pre clojure 1.0, and he spoke a lot about it back in the early days of clojure… A lot of that thinking I think manifested itself into datomic."><y>#</y><d>2020-03-20</d><h>13:17</h><w>rickmoynihan</w>Maps are great but have their limits; analagous to structured databases vs relational.

At some point you realise you’re building a database with your datastructures, e.g. when you start computing lookup tables, and having lists of denormalised/sorted keys etc; which you then need bespoke functions to handle.

Rich Hickey: Every class is an island.
Rick Moynihan: Your datastructure’s become islands too <b>🙂</b>

At that point you want a query language; and graphs are super flexible

In all fairness Rich has known this since pre clojure 1.0, and he spoke a lot about it back in the early days of clojure…  A lot of that thinking I think manifested itself into datomic.</z><z id="t1587273170" t="EmmanuelOga wow I did not realize there was an RDF channel here and with more than 70 ppl. Nice 🙂"><y>#</y><d>2020-04-19</d><h>05:12</h><w>EmmanuelOga</w>wow I did not realize there was an RDF channel here and with more than 70 ppl. Nice <b>🙂</b></z><z id="t1587273180" t="EmmanuelOga is anybody working in any interesting rdf projects these days?"><y>#</y><d>2020-04-19</d><h>05:13</h><w>EmmanuelOga</w>is anybody working in any interesting rdf projects these days?</z><z id="t1587273305" t="EmmanuelOga https://clojurians-log.clojureverse.org/rdf"><y>#</y><d>2020-04-19</d><h>05:15</h><w>EmmanuelOga</w><a href="https://clojurians-log.clojureverse.org/rdf" target="_blank">https://clojurians-log.clojureverse.org/rdf</a></z><z id="t1587412095" t="Al Baker we&apos;re working on a bunch of stuff at Stardog"><y>#</y><d>2020-04-20</d><h>19:48</h><w>Al Baker</w>we&apos;re working on a bunch of stuff at Stardog</z><z id="t1587412109" t="Al Baker and, where we can, love to use clojure to query the knowledge graph"><y>#</y><d>2020-04-20</d><h>19:48</h><w>Al Baker</w>and, where we can, love to use clojure to query the knowledge graph</z><z id="t1587412558" t="EmmanuelOga cool. I&apos;m learning SHACL these days: https://twitter.com/EmmanuelOga/status/1252157230826156035"><y>#</y><d>2020-04-20</d><h>19:55</h><w>EmmanuelOga</w>cool. I&apos;m learning SHACL these days: <a href="https://twitter.com/EmmanuelOga/status/1252157230826156035" target="_blank">https://twitter.com/EmmanuelOga/status/1252157230826156035</a></z><z id="t1587412590" t="EmmanuelOga nothing too... &quot;clojuresque&quot; so far, but it is just so convenient to explore Java libraries through clojure&apos;s REPL"><y>#</y><d>2020-04-20</d><h>19:56</h><w>EmmanuelOga</w>nothing too... &quot;clojuresque&quot; so far, but it is just so convenient to explore Java libraries through clojure&apos;s REPL</z><z id="t1587846713" t="EmmanuelOga managed to run ShEx validation from clojure yaay 🙂"><y>#</y><d>2020-04-25</d><h>20:31</h><w>EmmanuelOga</w>managed to run ShEx validation from clojure yaay <b>🙂</b></z><z id="t1587846715" t="EmmanuelOga https://gist.github.com/EmmanuelOga/2fcb3a76ad58c0a73122544306d4b3fb#file-src-rdfex-shex-clj"><y>#</y><d>2020-04-25</d><h>20:31</h><w>EmmanuelOga</w><a href="https://gist.github.com/EmmanuelOga/2fcb3a76ad58c0a73122544306d4b3fb#file-src-rdfex-shex-clj" target="_blank">https://gist.github.com/EmmanuelOga/2fcb3a76ad58c0a73122544306d4b3fb#file-src-rdfex-shex-clj</a></z><z id="t1587962153" t="EmmanuelOga ShEx is such an expressive validation system. The main problem I&apos;m finding so far is that the implementations are not quite there yet."><y>#</y><d>2020-04-27</d><h>04:35</h><w>EmmanuelOga</w>ShEx is such an expressive validation system. The main problem I&apos;m finding so far is that the implementations are not quite there yet.</z><z id="t1587962164" t="EmmanuelOga Sample issue: impossible to understand validation errors: https://github.com/weso/shaclex/issues/272#issuecomment-619707908"><y>#</y><d>2020-04-27</d><h>04:36</h><w>EmmanuelOga</w>Sample issue: impossible to understand validation errors: <a href="https://github.com/weso/shaclex/issues/272#issuecomment-619707908" target="_blank">https://github.com/weso/shaclex/issues/272#issuecomment-619707908</a></z><z id="t1587962194" t="EmmanuelOga The javascript implementation, from the few tests I ran, generated a bit more readable error messages."><y>#</y><d>2020-04-27</d><h>04:36</h><w>EmmanuelOga</w>The javascript implementation, from the few tests I ran, generated a bit more readable error messages.</z><z id="t1587962206" t="EmmanuelOga As soon as I finish exploring the ShEx chapter I plan to move to SHACL"><y>#</y><d>2020-04-27</d><h>04:36</h><w>EmmanuelOga</w>As soon as I finish exploring the ShEx chapter I plan to move to SHACL</z><z id="t1587993171" t="rickmoynihan [:attrs {:href &quot;/_/_/users/UCFTL4UQP&quot;}] : We currently use SHACL for some of our validations. I’ve never used ShEx, though we chose SHACL; mainly because I felt there were more credible implementation efforts, along with more mature implementations. However it’s certainly also true that the implementations in SHACL also seem to vary in their coverage of the standard and quality."><y>#</y><d>2020-04-27</d><h>13:12</h><w>rickmoynihan</w><a>@emmanueloga</a>: We currently use SHACL for some of our validations.  I’ve never used ShEx, though we chose SHACL; mainly because I felt there were more credible implementation efforts, along with more mature implementations.  However it’s certainly also true that the implementations in SHACL also seem to vary in their coverage of the standard and quality.</z><z id="t1588031712" t="EmmanuelOga they [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] what are you guys using RDF for?"><y>#</y><d>2020-04-27</d><h>23:55</h><w>EmmanuelOga</w>they <a>@rickmoynihan</a> what are you guys using RDF for?</z><z id="t1588031758" t="EmmanuelOga Found you 😛 &quot;I&apos;m making tools to connect https://twitter.com/search?q=%23government&amp;amp;src=hashtag_click https://twitter.com/search?q=%23data&amp;amp;src=hashtag_click &quot;"><y>#</y><d>2020-04-27</d><h>23:55</h><w>EmmanuelOga</w>Found you <b>😛</b> &quot;I&apos;m making tools to connect <a href="https://twitter.com/search?q=%23government&amp;amp;src=hashtag_click" target="_blank">https://twitter.com/search?q=%23government&amp;amp;src=hashtag_click</a> <a href="https://twitter.com/search?q=%23data&amp;amp;src=hashtag_click" target="_blank">https://twitter.com/search?q=%23data&amp;amp;src=hashtag_click</a>&quot;</z><z id="t1588054857" t="rickmoynihan Yeah, we provide a data publishing platform for government data; mainly stats but also environmental data and other stuff"><y>#</y><d>2020-04-28</d><h>06:20</h><w>rickmoynihan</w>Yeah, we provide a data publishing platform for government data; mainly stats but also environmental data and other stuff</z><z id="t1588081866" t="cjsauer Hello all 👋 My coworker recently handed me “Semantic Web for the Working Ontologist” which I’ve about finished with little trouble understanding the theory, but the book intentionally side steps practical tech choices. What are the “must have” tools for building an application leveraging these standards? Is Jena a practical choice for getting one’s feet wet with semantic web tech? Thanks. "><y>#</y><d>2020-04-28</d><h>13:51</h><w>cjsauer</w>Hello all <b>👋</b>
My coworker recently handed me “Semantic Web for the Working Ontologist” which I’ve about finished with little trouble understanding the theory, but the book intentionally side steps practical tech choices. 
What are the “must have” tools for building an application leveraging these standards?
Is Jena a practical choice for getting one’s feet wet with semantic web tech? Thanks. </z><z id="t1588133247" t="EmmanuelOga both Jena and RDF4J are great and obvious choices for a Clojure users"><y>#</y><d>2020-04-29</d><h>04:07</h><r>EmmanuelOga</r>both Jena and RDF4J are great and obvious choices for a Clojure users</z><z id="t1588133275" t="EmmanuelOga RDF4J feels a bit more modern but in the end it doesn&apos;t matter if you are using it from clojure"><y>#</y><d>2020-04-29</d><h>04:07</h><r>EmmanuelOga</r>RDF4J feels a bit more modern but in the end it doesn&apos;t matter if you are using it from clojure</z><z id="t1588133304" t="EmmanuelOga I ended up switching to jena mostly because I wanted to use https://github.com/TopQuadrant/shacl"><y>#</y><d>2020-04-29</d><h>04:08</h><r>EmmanuelOga</r>I ended up switching to jena mostly because I wanted to use <a href="https://github.com/TopQuadrant/shacl" target="_blank">https://github.com/TopQuadrant/shacl</a></z><z id="t1588133338" t="EmmanuelOga btw, SHACL seems like a must for a traditional web service if you plan to replace the traditional relational database design with RDF"><y>#</y><d>2020-04-29</d><h>04:08</h><r>EmmanuelOga</r>btw, SHACL seems like a must for a traditional web service if you plan to replace the traditional relational database design with RDF</z><z id="t1588133475" t="EmmanuelOga but I&apos;m just learning about this stuff myself... working through :"><y>#</y><d>2020-04-29</d><h>04:11</h><r>EmmanuelOga</r>but I&apos;m just learning about this stuff myself... working through :</z><z id="t1588133475" t="EmmanuelOga http://www.learningsparql.com/"><y>#</y><d>2020-04-29</d><h>04:11</h><r>EmmanuelOga</r><a href="http://www.learningsparql.com/" target="_blank">http://www.learningsparql.com/</a></z><z id="t1588133480" t="EmmanuelOga and"><y>#</y><d>2020-04-29</d><h>04:11</h><r>EmmanuelOga</r>and</z><z id="t1588133481" t="EmmanuelOga https://book.validatingrdf.com/"><y>#</y><d>2020-04-29</d><h>04:11</h><r>EmmanuelOga</r><a href="https://book.validatingrdf.com/" target="_blank">https://book.validatingrdf.com/</a></z><z id="t1588133485" t="EmmanuelOga both great resources"><y>#</y><d>2020-04-29</d><h>04:11</h><r>EmmanuelOga</r>both great resources</z><z id="t1588164023" t="Eric Scott I can&apos;t remember whether that book mentions protege:"><y>#</y><d>2020-04-29</d><h>12:40</h><r>Eric Scott</r>I can&apos;t remember whether that book mentions protege:</z><z id="t1588164023" t="Eric Scott https://protege.stanford.edu/"><y>#</y><d>2020-04-29</d><h>12:40</h><r>Eric Scott</r><a href="https://protege.stanford.edu/" target="_blank">https://protege.stanford.edu/</a></z><z id="t1588341778" t="cjsauer Thanks for all the great replies 🙏 There certainly is a lot for one to wrap their head around when starting off. Protege looks really great. Can I ask, is it primarily a tool for experimenting/modeling, or can I actually use it to then “export” a model into something like Jena? Said another way, how would one use this as a companion to a production server (if at all)?"><y>#</y><d>2020-05-01</d><h>14:02</h><r>cjsauer</r>Thanks for all the great replies <b>🙏</b>
There certainly is a lot for one to wrap their head around when starting off. Protege looks really great. Can I ask, is it primarily a tool for experimenting/modeling, or can I actually use it to then “export” a model into something like Jena?
Said another way, how would one use this as a companion to a production server (if at all)?</z><z id="t1588343487" t="rickmoynihan My understanding is Protege is used for ontology development, it doubtless has other uses too as there’s a plugin ecosystem too. But yes, you can write an ontology in protege and export it in various formats (including RDF, e.g. as .ttl ) which you should then be able to load into a Jena OntModel or whatever… Though I’m really no expert on this; I’ve only ever used it for very basic ontology work — using the reasoner to essentially flush out basic implications and check the domains/ranges all align etc."><y>#</y><d>2020-05-01</d><h>14:31</h><r>rickmoynihan</r>My understanding is Protege is used for ontology development, it doubtless has other uses too as there’s a plugin ecosystem too.  But yes, you can write an ontology in protege and export it in various formats (including RDF, e.g. as <code>.ttl</code>) which you should then be able to load into a Jena OntModel or whatever…

Though I’m really no expert on this; I’ve only ever used it for very basic ontology work — using the reasoner to essentially flush out basic implications and check the domains/ranges all align etc.</z><z id="t1588343984" t="rickmoynihan I’d personally recommend going easy on the reasoning/OWL stuff… as there are a lot of sharp edges for the uninitiated to cut themselves on. I’d include myself amongst the uninitiated, and I’ve probably got more experience of this stuff than most. I’m not saying this stuff isn’t valuable or useful, it is; it’s just very easy to sink a lot of time into it for very little reward. So just be careful, because it might not be able to infer what you hope… and the inferences may not support useful business cases. Stick to the simple practical benefits, like making certain queries easier to write, and just using simple stuff like domains/ranges, subclasses etc… essentially rdfs+; and you’ll be happier… and even that isn’t normally necessary."><y>#</y><d>2020-05-01</d><h>14:39</h><r>rickmoynihan</r>I’d personally recommend going easy on the reasoning/OWL stuff… as there are a lot of sharp edges for the uninitiated to cut themselves on.

I’d include myself amongst the uninitiated, and I’ve probably got more experience of this stuff than most.

I’m not saying this stuff isn’t valuable or useful, it is; it’s just very easy to sink a lot of time into it for very little reward.

So just be careful, because it might not be able to infer what you hope… and the inferences may not support useful business cases.

Stick to the simple practical benefits, like making certain queries easier to write, and just using simple stuff like domains/ranges, subclasses etc… essentially rdfs+; and you’ll be happier… and even that isn’t normally necessary.</z><z id="t1588341778" t="cjsauer Thanks for all the great replies 🙏 There certainly is a lot for one to wrap their head around when starting off. Protege looks really great. Can I ask, is it primarily a tool for experimenting/modeling, or can I actually use it to then “export” a model into something like Jena? Said another way, how would one use this as a companion to a production server (if at all)?"><y>#</y><d>2020-05-01</d><h>14:02</h><w>cjsauer</w>Thanks for all the great replies <b>🙏</b>
There certainly is a lot for one to wrap their head around when starting off. Protege looks really great. Can I ask, is it primarily a tool for experimenting/modeling, or can I actually use it to then “export” a model into something like Jena?
Said another way, how would one use this as a companion to a production server (if at all)?</z><z id="t1588158431" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U6GFE9HS7&quot;}] it depends what you’re interested in applying. That book is very much about reasoning with OWL and various subsets of it, e.g. RDFS+ in which case Jena is probably your best starting point: https://jena.apache.org/documentation/ontology/ RDF4j doesn’t have such good reasoning support out of the box, though it is in my opinion superior to jena in the cleanliness of its API etc… Jena and the open source triplestores do have some limitations; mainly in performance and size of database - so if you need to go bigger/faster you’ll want a commercial triplestore. We use stardog, which is in our experience the best, though we’ve not re-assessed the market for maybe 5 years; we compared bigdata/blazegraph with ontotext’s GraphDB (formerly owlim — which should also have decent OWL support). Stardog has decent reasoning support in the database via SPARQL too if you need it."><y>#</y><d>2020-04-29</d><h>11:07</h><w>rickmoynihan</w><a>@cjsauer</a> it depends what you’re interested in applying.  That book is very much about reasoning with OWL and various subsets of it, e.g. RDFS+ in which case Jena is probably your best starting point: <a href="https://jena.apache.org/documentation/ontology/" target="_blank">https://jena.apache.org/documentation/ontology/</a>

RDF4j doesn’t have such good reasoning support out of the box, though it is in my opinion superior to jena in the cleanliness of its API etc…

Jena and the open source triplestores do have some limitations; mainly in performance and size of database - so if you need to go bigger/faster you’ll want a commercial triplestore.  We use stardog, which is in our experience the best, though we’ve not re-assessed the market for maybe 5 years; we compared bigdata/blazegraph with ontotext’s GraphDB (formerly owlim — which should also have decent OWL support).

Stardog has decent reasoning support in the database via SPARQL too if you need it.</z><z id="t1588158825" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U6GFE9HS7&quot;}] It’s also worth saying there are broadly speaking two distinct communities in the RDF world, that overlap somewhat in the middle. These are essentially the Linked Data community and the Semantic Web community. The former are less interested in formal open world reasoning, and are more interested in shipping data on the web that people can work with. OWL in linked data is just another vocabulary, and the modelling is usually restricted to a subset; which may or may not be consistent. The latter care more about logical consistency, ontological modelling in OWL etc. So the answer also partly depends on what camp you’re in. e.g. for applying that book you might want to look at Protege."><y>#</y><d>2020-04-29</d><h>11:13</h><w>rickmoynihan</w><a>@cjsauer</a> It’s also worth saying there are broadly speaking two distinct communities in the RDF world, that overlap somewhat in the middle.  These are essentially the Linked Data community and the Semantic Web community.

The former are less interested in formal open world reasoning, and are more interested in shipping data on the web that people can work with.  OWL in linked data is just another vocabulary, and the modelling is usually restricted to a subset; which may or may not be consistent.

The latter care more about logical consistency, ontological modelling in OWL etc.

So the answer also partly depends on what camp you’re in.

e.g. for applying that book you might want to look at Protege.</z><z id="t1588158923" t="rickmoynihan Also the above is of course an over-generalisation; but broadly speaking it’s true… and it’s even a division that go backs to the roots of good old fashion AI… e.g. neats and scruffies etc…"><y>#</y><d>2020-04-29</d><h>11:15</h><w>rickmoynihan</w>Also the above is of course an over-generalisation; but broadly speaking it’s true… and it’s even a division that go backs to the roots of good old fashion AI… e.g. neats and scruffies etc…</z><z id="t1588164349" t="Eric Scott https://protege.stanford.edu/"><y>#</y><d>2020-04-29</d><h>12:45</h><w>Eric Scott</w><a href="https://protege.stanford.edu/" target="_blank">https://protege.stanford.edu/</a></z><z id="t1588164420" t="Eric Scott [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] is too modest to mention this, but he and his outfit have authored Grafter ( https://github.com/Swirrl/grafter ), if you&apos;re looking to work in Clojure."><y>#</y><d>2020-04-29</d><h>12:47</h><w>Eric Scott</w><a>@rickmoynihan</a> is too modest to mention this, but he and his outfit have authored Grafter (<a href="https://github.com/Swirrl/grafter" target="_blank">https://github.com/Swirrl/grafter</a>), if you&apos;re looking to work in Clojure.</z><z id="t1588169446" t="rickmoynihan haha thanks… I did consider mentioning it, but figured the question was more about reasoning/ontologies which isn’t RDF4j/grafter’s strength. At least not without adding the Jena backend I’ve been planning to add forever."><y>#</y><d>2020-04-29</d><h>14:10</h><r>rickmoynihan</r>haha thanks… I did consider mentioning it, but figured the question was more about reasoning/ontologies which isn’t RDF4j/grafter’s strength.

At least not without adding the Jena backend I’ve been planning to add forever.</z><z id="t1588174963" t="Eric Scott This week I plan to write an IGraph wrapper around grafter. This&apos;ll be my first time dealing with rdf4j. Looking forward to it."><y>#</y><d>2020-04-29</d><h>15:42</h><r>Eric Scott</r>This week I plan to write an IGraph wrapper around grafter.  This&apos;ll be my first time dealing with rdf4j. Looking forward to it.</z><z id="t1588263919" t="Cliff Wulfman I&apos;m very interested in learning how to use Grafter and Stardog. Any pointers to getting started with Grafter 2?"><y>#</y><d>2020-04-30</d><h>16:25</h><r>Cliff Wulfman</r>I&apos;m very interested in learning how to use Grafter and Stardog. Any pointers to getting started with Grafter 2?</z><z id="t1588266614" t="rickmoynihan I really need to provide clearer docs around usage etc… It has been on the list of jobs for a long time… What is it that you want to do? Grafter is mainly focussed on RDF I/O; essentially treating immutable triples as lazy-sequences — with some light tooling around SPARQL queries etc. There’s also https://github.com/Swirrl/matcha for querying RDF graphs in memory (which is roughly equivalent to datascript but for RDF). Other tools include: https://github.com/Swirrl/csv2rdf which is more or less a complete implementation of the W3C standards for CSVW (CSV on the web) — which can be used for transforming CSV files into RDF, via a jsonld metadata file."><y>#</y><d>2020-04-30</d><h>17:10</h><r>rickmoynihan</r>I really need to provide clearer docs around usage etc…  It has been on the list of jobs for a long time…

What is it that you want to do?

Grafter is mainly focussed on RDF I/O; essentially treating immutable triples as lazy-sequences  — with some light tooling around SPARQL queries etc.

There’s also <a href="https://github.com/Swirrl/matcha" target="_blank">https://github.com/Swirrl/matcha</a> for querying RDF graphs in memory (which is roughly equivalent to datascript but for RDF).

Other tools include: <a href="https://github.com/Swirrl/csv2rdf" target="_blank">https://github.com/Swirrl/csv2rdf</a> which is more or less a complete implementation of the W3C standards for CSVW (CSV on the web) — which can be used for transforming CSV files into RDF, via a jsonld metadata file.</z><z id="t1588267285" t="Cliff Wulfman This is very helpful, [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] ; thanks! I have a couple of projects that involve various sorts of prosopographical and bibliographic knowledge, and I&apos;ve been using CIDOC-CRM &amp; PRESSoo to capture it; now I want to start building actual knowledge-based systems. I&apos;m also beginning to investigate ways to implement Web Annotations with IIIF. I find myself falling down a tooling rabbit-hole everytime I look at this! I&apos;d be happy to help out with documentation, if you&apos;re looking for volunteers..."><y>#</y><d>2020-04-30</d><h>17:21</h><r>Cliff Wulfman</r>This is very helpful, <a>@rickmoynihan</a>; thanks! I have a couple of projects that involve various sorts of prosopographical and bibliographic knowledge, and I&apos;ve been using CIDOC-CRM &amp; PRESSoo to capture it; now I want to start building actual knowledge-based systems. I&apos;m also beginning to investigate ways to implement Web Annotations with IIIF.  I find myself falling down a tooling rabbit-hole everytime I look at this!  I&apos;d be happy to help out with documentation, if you&apos;re looking for volunteers...</z><z id="t1588164883" t="Eric Scott One factor to consider if you&apos;re using a reasoner (esp. an owl reasoner) is that that your data has to be absolutely pristine, and in quantities low enough that they match the resources you&apos;re ready to allocate. I&apos;ve heard stories of people adding a single axiom to an OWL KR, and waiting days for the system to work out all of the implications. Probably an exaggeration, but it does shine a light on a real issue."><y>#</y><d>2020-04-29</d><h>12:54</h><w>Eric Scott</w>One factor to consider if you&apos;re using a reasoner (esp. an owl reasoner) is that that your data has to be absolutely pristine, and in quantities low enough that they match the resources you&apos;re ready to allocate. I&apos;ve heard stories of people adding a single axiom to an OWL KR, and waiting days for the system to work out all of the implications. Probably an exaggeration, but it does shine a light on a real issue.</z><z id="t1588164936" t="Eric Scott Reasoners are eager, a lazy alternative to inferencing is to use property paths."><y>#</y><d>2020-04-29</d><h>12:55</h><w>Eric Scott</w>Reasoners are eager, a lazy alternative to inferencing is to use property paths.</z><z id="t1588164937" t="Eric Scott https://www.w3.org/TR/sparql11-property-paths/"><y>#</y><d>2020-04-29</d><h>12:55</h><w>Eric Scott</w><a href="https://www.w3.org/TR/sparql11-property-paths/" target="_blank">https://www.w3.org/TR/sparql11-property-paths/</a></z><z id="t1588169618" t="rickmoynihan Yeah you can definitely use some SPARQL features like property paths, VALUES clauses etc to simulate or work around not having reasoning available… What do you mean by “reasoners are eager”?"><y>#</y><d>2020-04-29</d><h>14:13</h><w>rickmoynihan</w>Yeah you can definitely use some SPARQL features like property paths, VALUES clauses etc to simulate or work around not having reasoning available…

 What do you mean by “reasoners are eager”?</z><z id="t1588173648" t="Eric Scott Well it&apos;s been a while since I&apos;ve actually used a reasoner in anger, and maybe you can set me straight on this, but when I&apos;ve used a graph configured with a reasoner, adding a new assertion would automatically result in a materialization of everything the reasoner can infer about that assertion."><y>#</y><d>2020-04-29</d><h>15:20</h><r>Eric Scott</r>Well it&apos;s been a while since I&apos;ve actually used a reasoner in anger, and maybe you can set me straight on this, but when I&apos;ve used a graph configured with a reasoner, adding a new assertion would automatically result in a materialization of everything the reasoner can infer about that assertion.</z><z id="t1588173673" t="Eric Scott Am I incorrect in this impression?"><y>#</y><d>2020-04-29</d><h>15:21</h><r>Eric Scott</r>Am I incorrect in this impression?</z><z id="t1588173799" t="rickmoynihan I think that depends on the implementation"><y>#</y><d>2020-04-29</d><h>15:23</h><r>rickmoynihan</r>I think that depends on the implementation</z><z id="t1588173856" t="rickmoynihan You can do it on update; but you can also do it on query… In which case you only need to compute the closure over what you’re querying"><y>#</y><d>2020-04-29</d><h>15:24</h><r>rickmoynihan</r>You can do it on update; but you can also do it on query… In which case you only need to compute the closure over what you’re querying</z><z id="t1588174700" t="Eric Scott Ah! Good to know. Thx."><y>#</y><d>2020-04-29</d><h>15:38</h><r>Eric Scott</r>Ah! Good to know. Thx.</z><z id="t1588174888" t="Eric Scott I abandoned the use of reasoners early on, working with large amounts of LD that was full of inconsistencies. Fun puzzles with long chains of inference leading to contradictions."><y>#</y><d>2020-04-29</d><h>15:41</h><r>Eric Scott</r>I abandoned the use of reasoners early on, working with large amounts of LD that was full of inconsistencies. Fun puzzles with long chains of inference leading to contradictions.</z><z id="t1588238421" t="rickmoynihan Yeah, this is the problem with the broad semantic web vision. It’s hard enough to create consistent highly curated knowledge bases anyway; let alone when disparate groups of disconnected people try to do so. This was one of things we were eventually hoping to make a meaningful (academic) contribution towards solving in my early days developing multi-agent systems with defeasible logic. Essentially we had an agent communication language, based around two primary primitives inform and request. The semantics of inform were then based around a concept of mutual belief… so if you informed me of something (assuming a level trust) I would then believe that you believed it, and may then choose to believe it myself via other defeasible rules. In terms of security you could also then introduce notions of trust, credulity, lying etc; and have further defeasible rules to reason about such things… Those rules may include not believing you if you said inconsistent things, or if others said you’d said things that you then contradicted to me etc etc… It was an interesting way to think about protocol design etc."><y>#</y><d>2020-04-30</d><h>09:20</h><r>rickmoynihan</r>Yeah, this is the problem with the broad semantic web vision.  It’s hard enough to create consistent highly curated knowledge bases anyway; let alone when disparate groups of disconnected people try to do so.

This was one of things we were eventually hoping to make a meaningful (academic) contribution towards solving in my early days developing multi-agent systems with defeasible logic.

Essentially we had an agent communication language, based around two primary primitives inform and request.

The semantics of inform were then based around a concept of mutual belief… so if you informed me of something (assuming a level trust) I would then believe that you believed it, and may then choose to believe it myself via other defeasible rules.

In terms of security you could also then introduce notions of trust, credulity, lying etc; and have further defeasible rules to reason about such things…  Those rules may include not believing you if you said inconsistent things, or if others said you’d said things that you then contradicted to me etc etc…

It was an interesting way to think about protocol design etc.</z><z id="t1588254040" t="Eric Scott Interesting! I gather it didn&apos;t pay the bills?"><y>#</y><d>2020-04-30</d><h>13:40</h><r>Eric Scott</r>Interesting! I gather it didn&apos;t pay the bills?</z><z id="t1588260493" t="rickmoynihan The joys of overly academic startups 😁"><y>#</y><d>2020-04-30</d><h>15:28</h><r>rickmoynihan</r>The joys of overly academic startups <b>😁</b></z><z id="t1588260920" t="Eric Scott I feel your pain."><y>#</y><d>2020-04-30</d><h>15:35</h><r>Eric Scott</r>I feel your pain.</z><z id="t1588261458" t="Eric Scott My first interesting job was doing cognitive modeling in Common Lisp. US gov&apos;t reasearch grant. That was a great few years, then it was all venture-capital-driven from there, and things get significantly less interesting."><y>#</y><d>2020-04-30</d><h>15:44</h><r>Eric Scott</r>My first interesting job was doing cognitive modeling in Common Lisp.  US gov&apos;t reasearch grant.  That was a great few years, then it was all venture-capital-driven from there, and things get significantly less interesting.</z><z id="t1588169666" t="dcj FYI for anyone interested in using OWL: https://github.com/phillord/tawny-owl Phil Lord has several associated example projects.... And this: https://github.com/phillord/protege-nrepl"><y>#</y><d>2020-04-29</d><h>14:14</h><w>dcj</w>FYI for anyone interested in using OWL: <a href="https://github.com/phillord/tawny-owl" target="_blank">https://github.com/phillord/tawny-owl</a>
Phil Lord has several associated example projects....
And this:  <a href="https://github.com/phillord/protege-nrepl" target="_blank">https://github.com/phillord/protege-nrepl</a></z><z id="t1588172766" t="teodorlu Anyone here tried Roam research? I&apos;m finding that having an RDF perspective is a huge asset when designing your Roam knowledge graph."><y>#</y><d>2020-04-29</d><h>15:06</h><w>teodorlu</w>Anyone here tried Roam research? I&apos;m finding that having an RDF perspective is a huge asset when designing your Roam knowledge graph.</z><z id="t1588217263" t="EmmanuelOga how so?"><y>#</y><d>2020-04-30</d><h>03:27</h><w>EmmanuelOga</w>how so?</z><z id="t1588217841" t="EmmanuelOga last I checked Roam did not have any way of exporting content that I could find"><y>#</y><d>2020-04-30</d><h>03:37</h><w>EmmanuelOga</w>last I checked Roam did not have any way of exporting content that I could find</z><z id="t1588217870" t="EmmanuelOga for that kind of tool, that&apos;s a feature that should exist from day one"><y>#</y><d>2020-04-30</d><h>03:37</h><w>EmmanuelOga</w>for that kind of tool, that&apos;s a feature that should exist from day one</z><z id="t1588231805" t="teodorlu There&apos;s working export to both JSON and markdown. Perhaps it&apos;s been added since you checked?"><y>#</y><d>2020-04-30</d><h>07:30</h><w>teodorlu</w>There&apos;s working export to both JSON and markdown. Perhaps it&apos;s been added since you checked?</z><z id="t1588231917" t="teodorlu I&apos;m asking because I feel like creating a good information structure within roam seems to require skills in technology like RDF. It&apos;s also the first time I&apos;ve felt that creating and consuming content as a user is as efficient as I&apos;d like, without throwing possibility for automation overboard."><y>#</y><d>2020-04-30</d><h>07:31</h><w>teodorlu</w>I&apos;m asking because I feel like creating a good information structure within roam seems to require skills in technology like RDF. It&apos;s also the first time I&apos;ve felt that creating and consuming content as a user is as efficient as I&apos;d like, without throwing possibility for automation overboard.</z><z id="t1588233960" t="EmmanuelOga cool, that means I need to give it a second look"><y>#</y><d>2020-04-30</d><h>08:06</h><w>EmmanuelOga</w>cool, that means I need to give it a second look</z><z id="t1588341925" t="cjsauer I’m leaning towards Jena at this point given that it’s really simple to bring in to a vanilla Clojure project as a jar using tools.deps."><y>#</y><d>2020-05-01</d><h>14:05</h><w>cjsauer</w>I’m leaning towards Jena at this point given that it’s really simple to bring in to a vanilla Clojure project as a jar using tools.deps.</z><z id="t1588342032" t="cjsauer &gt; there are broadly speaking two distinct communities in the RDF world This is really helpful information, thank you [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] . It even prompted me to look up “neats vs scruffies” which was a wonderful rabbit hole 🙂 Approaching tools with this distinction in mind has been helpful."><y>#</y><d>2020-05-01</d><h>14:07</h><w>cjsauer</w>&gt; there are broadly speaking two distinct communities in the RDF world
This is really helpful information, thank you <a>@rickmoynihan</a>. It even prompted me to look up “neats vs scruffies” which was a wonderful rabbit hole <b>🙂</b>
Approaching tools with this distinction in mind has been helpful.</z><z id="t1588378687" t="EmmanuelOga an article that helped me compare between RDF and property graphs (the model of databases like Neo4J and others): http://www.snee.com/bobdc.blog/2018/04/reification-is-a-red-herring.html"><y>#</y><d>2020-05-02</d><h>00:18</h><w>EmmanuelOga</w>an article that helped me compare between RDF and property graphs (the model of databases like Neo4J and others): <a href="http://www.snee.com/bobdc.blog/2018/04/reification-is-a-red-herring.html" target="_blank">http://www.snee.com/bobdc.blog/2018/04/reification-is-a-red-herring.html</a></z><z id="t1588378712" t="EmmanuelOga since you are talking abou t&quot;servers&quot; I&apos;m guessing it could be an interested read to you since it talks about practical/real world data modeling"><y>#</y><d>2020-05-02</d><h>00:18</h><w>EmmanuelOga</w>since you are talking abou t&quot;servers&quot; I&apos;m guessing it could be an interested read to you since it talks about practical/real world data modeling</z><z id="t1588412199" t="rickmoynihan There’s been movement on reification in RDF since then in that there’s a proposed set of standards RDF*/SPARQL*/Turtle* etc that support edge properties. Whats more at least 4 RDF databases already claim support for it (including stardog), and RDF4j has experimental support available in their native store and memory store implementations too. https://rdf4j.org/documentation/programming/rdfstar/ Reification would definitely be nice in some instances, but you can get by easily enough without it. Also RDF* is not RDF; so I’d be wary of it introducing interop problems, and a more complex data model."><y>#</y><d>2020-05-02</d><h>09:36</h><w>rickmoynihan</w>There’s been movement on reification in RDF since then in that there’s a proposed set of standards RDF*/SPARQL*/Turtle* etc that support edge properties.  Whats more at least 4 RDF databases already claim support for it (including stardog), and RDF4j has experimental support available in their native store and memory store implementations too.

<a href="https://rdf4j.org/documentation/programming/rdfstar/" target="_blank">https://rdf4j.org/documentation/programming/rdfstar/</a>

Reification would definitely be nice in some instances, but you can get by easily enough without it.  Also RDF* is not RDF; so I’d be wary of it introducing interop problems, and a more complex data model.</z><z id="t1588735842" t="EmmanuelOga found this thread which answers some questions about addressability and compares RDF* with named graphs https://lists.w3.org/Archives/Public/public-rdf-star/2020Feb/thread.html"><y>#</y><d>2020-05-06</d><h>03:30</h><r>EmmanuelOga</r>found this thread which answers some questions about addressability and compares RDF* with named graphs <a href="https://lists.w3.org/Archives/Public/public-rdf-star/2020Feb/thread.html" target="_blank">https://lists.w3.org/Archives/Public/public-rdf-star/2020Feb/thread.html</a></z><z id="t1588731235" t="EmmanuelOga I think the author of RDF* did initial prototype works with Jena"><y>#</y><d>2020-05-06</d><h>02:13</h><w>EmmanuelOga</w>I think the author of RDF* did initial prototype works with Jena</z><z id="t1588731283" t="EmmanuelOga https://github.com/RDFstar/RDFstarTools"><y>#</y><d>2020-05-06</d><h>02:14</h><w>EmmanuelOga</w><a href="https://github.com/RDFstar/RDFstarTools" target="_blank">https://github.com/RDFstar/RDFstarTools</a></z><z id="t1588731394" t="EmmanuelOga so I&apos;ve been thinking... would be really nice to have a custom slack for RDF and related technologies"><y>#</y><d>2020-05-06</d><h>02:16</h><w>EmmanuelOga</w>so I&apos;ve been thinking... would be really nice to have a custom slack for RDF and related technologies</z><z id="t1588731400" t="EmmanuelOga I don&apos;t think there&apos;s such thing right now"><y>#</y><d>2020-05-06</d><h>02:16</h><w>EmmanuelOga</w>I don&apos;t think there&apos;s such thing right now</z><z id="t1588731409" t="EmmanuelOga I found a mattermost for Rdf4J but it seems dead"><y>#</y><d>2020-05-06</d><h>02:16</h><w>EmmanuelOga</w>I found a mattermost for Rdf4J but it seems dead</z><z id="t1588731428" t="EmmanuelOga ( https://mattermost.eclipse.org/eclipse/channels/eclipse-rdf4j )"><y>#</y><d>2020-05-06</d><h>02:17</h><w>EmmanuelOga</w>(<a href="https://mattermost.eclipse.org/eclipse/channels/eclipse-rdf4j" target="_blank">https://mattermost.eclipse.org/eclipse/channels/eclipse-rdf4j</a>)</z><z id="t1588799731" t="Al Baker we&apos;ve implemented rdf* and sparql* in Stardog"><y>#</y><d>2020-05-06</d><h>21:15</h><w>Al Baker</w>we&apos;ve implemented rdf* and sparql* in Stardog</z><z id="t1589251045" t="EmmanuelOga reading https://patterns.dataincubator.org/book/data-management-patterns.html"><y>#</y><d>2020-05-12</d><h>02:37</h><w>EmmanuelOga</w>reading <a href="https://patterns.dataincubator.org/book/data-management-patterns.html" target="_blank">https://patterns.dataincubator.org/book/data-management-patterns.html</a></z><z id="t1589251121" t="EmmanuelOga the ideas of how to use named graphs seem almost obvious but I&apos;m glad someone wrote about it since I&apos;ve seen comments of the tenor that named graphs is something one would almost never need on real life RDF"><y>#</y><d>2020-05-12</d><h>02:38</h><w>EmmanuelOga</w>the ideas of how to use named graphs seem almost obvious but I&apos;m glad someone wrote about it since I&apos;ve seen comments of the tenor that named graphs is something one would almost never need on real life RDF</z><z id="t1589251132" t="EmmanuelOga it seems at least someone is using them 🙂"><y>#</y><d>2020-05-12</d><h>02:38</h><w>EmmanuelOga</w>it seems at least someone is using them <b>🙂</b></z><z id="t1589265372" t="rickmoynihan As far as I know almost everyone doing anything substantial with RDF will use named graphs; it’d be hard not to."><y>#</y><d>2020-05-12</d><h>06:36</h><w>rickmoynihan</w>As far as I know almost everyone doing anything substantial with RDF will use named graphs; it’d be hard not to.</z><z id="t1589270105" t="EmmanuelOga &gt; I use these rarely enough that I re-read the “Named Graphs” section of my book’s “Updating Data with SPARQL” chapter as a review before I assembled the steps below."><y>#</y><d>2020-05-12</d><h>07:55</h><r>EmmanuelOga</r>&gt; I use these rarely enough that I re-read the “Named Graphs” section of my book’s “Updating Data with SPARQL” chapter as a review before I assembled the steps below.</z><z id="t1589270114" t="EmmanuelOga http://www.bobdc.com/blog/materializing/"><y>#</y><d>2020-05-12</d><h>07:55</h><r>EmmanuelOga</r><a href="http://www.bobdc.com/blog/materializing/" target="_blank">http://www.bobdc.com/blog/materializing/</a></z><z id="t1589270138" t="EmmanuelOga I guess I&apos;d been &quot;consuming&quot; quite a bit of Bob&apos;s content lately hahah"><y>#</y><d>2020-05-12</d><h>07:55</h><r>EmmanuelOga</r>I guess I&apos;d been &quot;consuming&quot; quite a bit of Bob&apos;s content lately hahah</z><z id="t1589279466" t="rickmoynihan Yeah I should clarify; if you’re publishing, changing or creating RDF; graphs are important. Much less so for querying as the semantics should ideally just be in the data (i.e. the ?s ?p ?o part of the quads)."><y>#</y><d>2020-05-12</d><h>10:31</h><r>rickmoynihan</r>Yeah I should clarify; if you’re publishing, changing or creating RDF; graphs are important.  Much less so for querying as the semantics should ideally just be in the data (i.e. the <code>?s ?p ?o</code> part of the quads).</z><z id="t1589265640" t="rickmoynihan We certainly use named graphs extensively"><y>#</y><d>2020-05-12</d><h>06:40</h><w>rickmoynihan</w>We certainly use named graphs extensively</z><z id="t1589266196" t="rickmoynihan You should however be careful with them; named graphs are a bit of an afterthought (introduced in) SPARQL 1.1. It’s a real shame for example that CONSTRUCT statements can’t construct graphs - there’s an issue discussing adding this to SPARQL 1.2 though. In particular you shouldn’t give graphs any real semantic meaning. Typically you usually only want to use them for data management; or establishing known/trusted subset(s) of information for processing."><y>#</y><d>2020-05-12</d><h>06:49</h><w>rickmoynihan</w>You should however be careful with them; named graphs are a bit of an afterthought (introduced in) SPARQL 1.1.  It’s a real shame for example that <code>CONSTRUCT</code> statements can’t construct graphs - there’s an issue discussing adding this to SPARQL 1.2 though.

In particular you shouldn’t give graphs any real semantic meaning.  Typically you usually only want to use them for data management; or establishing known/trusted subset(s) of information for processing.</z><z id="t1589269754" t="EmmanuelOga I&apos;m trying to create a lil blog generator"><y>#</y><d>2020-05-12</d><h>07:49</h><w>EmmanuelOga</w>I&apos;m trying to create a lil blog generator</z><z id="t1589269778" t="EmmanuelOga I was thinking of giving every &quot;document&quot; on my blog its own named graph"><y>#</y><d>2020-05-12</d><h>07:49</h><w>EmmanuelOga</w>I was thinking of giving every &quot;document&quot; on my blog its own named graph</z><z id="t1589269883" t="EmmanuelOga say, the backing data of &quot;/blog/2020-05-01-title&quot; would live on the named graph &quot;/blog/2020-05-01-title/data&quot;"><y>#</y><d>2020-05-12</d><h>07:51</h><w>EmmanuelOga</w>say, the backing data of &quot;/blog/2020-05-01-title&quot; would live on the named graph &quot;/blog/2020-05-01-title/data&quot;</z><z id="t1589269892" t="EmmanuelOga (not sure about the urls yet)"><y>#</y><d>2020-05-12</d><h>07:51</h><w>EmmanuelOga</w>(not sure about the urls yet)</z><z id="t1589269939" t="EmmanuelOga i think that would correspond to this pattern: https://patterns.dataincubator.org/book/graph-per-resource.html"><y>#</y><d>2020-05-12</d><h>07:52</h><w>EmmanuelOga</w>i think that would correspond to this pattern: <a href="https://patterns.dataincubator.org/book/graph-per-resource.html" target="_blank">https://patterns.dataincubator.org/book/graph-per-resource.html</a></z><z id="t1589270390" t="rickmoynihan Yes that makes sense and is pretty common; as it enables you to update the resource through a simple drop and replace of the graph"><y>#</y><d>2020-05-12</d><h>07:59</h><w>rickmoynihan</w>Yes that makes sense and is pretty common; as it enables you to update the resource through a simple drop and replace of the graph</z><z id="t1589292353" t="Eric Scott Am I correct in the recollection that data is only expected to be consistent within a given named graph?"><y>#</y><d>2020-05-12</d><h>14:05</h><w>Eric Scott</w>Am I correct in the recollection that data is only expected to be consistent within a given named graph?</z><z id="t1589341351" t="EmmanuelOga not sure what you mean Eric"><y>#</y><d>2020-05-13</d><h>03:42</h><w>EmmanuelOga</w>not sure what you mean Eric</z><z id="t1589375224" t="rickmoynihan [:attrs {:href &quot;/_/_/users/UB3R8UYA1&quot;}] : Data can be consistent across any arbitrary set of graphs… but yes it’s common for applications to make assumptions that some graph sets have certain properties that might not hold in a less controlled context"><y>#</y><d>2020-05-13</d><h>13:07</h><w>rickmoynihan</w><a>@eric.d.scott</a>: Data can be consistent across any arbitrary set of graphs… but yes it’s common for applications to make assumptions that some graph sets have certain properties that might not hold in a less controlled context</z><z id="t1589375849" t="Eric Scott I&apos;m thinking of the case where you might want to represent the contents of different natural language discourses, say speeches by various politicians. One might say for example that the birthplace of Barack Obama was Kenya..."><y>#</y><d>2020-05-13</d><h>13:17</h><w>Eric Scott</w>I&apos;m thinking of the case where you might want to represent the contents of different natural language discourses, say speeches by various politicians. One might say for example that the birthplace of Barack Obama was Kenya...</z></g><g id="s2"><z id="t1589376171" t="Eric Scott I personally try to avoid using the Default Graph as much as possible, and put all my data in meaningfully named graphs with suitable metadata about the source and purpose of the data."><y>#</y><d>2020-05-13</d><h>13:22</h><w>Eric Scott</w>I personally try to avoid using the Default Graph as much as possible, and put all my data in meaningfully named graphs with suitable metadata about the source and purpose of the data.</z><z id="t1589439890" t="EmmanuelOga so you mean one graph may say my name is emmanuel and the other may say my name is gustavo, so which is correct?"><y>#</y><d>2020-05-14</d><h>07:04</h><w>EmmanuelOga</w>so you mean one graph may say my name is emmanuel and the other may say my name is gustavo, so which is correct?</z><z id="t1589455907" t="rickmoynihan [:attrs {:href &quot;/_/_/users/UB3R8UYA1&quot;}] : it really depends on the application and what you’re doing though. If you’re pulling in data from untrusted sources yes you need to care about provenance, and track it. A lot of applications can make a closed world assumption about the data; because they control or manage it, and not worry about it."><y>#</y><d>2020-05-14</d><h>11:31</h><w>rickmoynihan</w><a>@eric.d.scott</a>: it really depends on the application and what you’re doing though.  If you’re pulling in data from untrusted sources yes you need to care about provenance, and track it.  A lot of applications can make a closed world assumption about the data; because they control or manage it, and not worry about it.</z><z id="t1589456020" t="rickmoynihan There’s a new release of grafter by the way… It now has a small clojure DSL for generating sparql property paths: https://github.com/Swirrl/grafter/blob/master/test/grafter_2/rdf4j/sparql/path_test.clj#L176-L220"><y>#</y><d>2020-05-14</d><h>11:33</h><w>rickmoynihan</w>There’s a new release of grafter by the way… It now has a small clojure DSL for generating sparql property paths:

<a href="https://github.com/Swirrl/grafter/blob/master/test/grafter_2/rdf4j/sparql/path_test.clj#L176-L220" target="_blank">https://github.com/Swirrl/grafter/blob/master/test/grafter_2/rdf4j/sparql/path_test.clj#L176-L220</a></z><z id="t1589456052" t="rickmoynihan https://github.com/Swirrl/grafter/releases/tag/2.1.8"><y>#</y><d>2020-05-14</d><h>11:34</h><w>rickmoynihan</w><a href="https://github.com/Swirrl/grafter/releases/tag/2.1.8" target="_blank">https://github.com/Swirrl/grafter/releases/tag/2.1.8</a></z><z id="t1591122340" t="isak I remember someone talking about a macro for generating sparql in Clojure recently (last few months). Anyone remember what it was?"><y>#</y><d>2020-06-02</d><h>18:25</h><w>isak</w>I remember someone talking about a macro for generating sparql in Clojure recently (last few months). Anyone remember what it was?</z><z id="t1591124459" t="isak found it https://clojurians-log.clojureverse.org/clojure/2020-05-14/1589491059.102500"><y>#</y><d>2020-06-02</d><h>19:00</h><w>isak</w>found it <a href="https://clojurians-log.clojureverse.org/clojure/2020-05-14/1589491059.102500" target="_blank">https://clojurians-log.clojureverse.org/clojure/2020-05-14/1589491059.102500</a></z><z id="t1600428100" t="apbleonard Is there an RDF vocabulary out there that matches (or is a superset of) the datomic data model? https://docs.datomic.com/cloud/schema/schema-reference.html"><y>#</y><d>2020-09-18</d><h>11:21</h><w>apbleonard</w>Is there an RDF vocabulary out there that matches (or is a superset of) the datomic data model? <a href="https://docs.datomic.com/cloud/schema/schema-reference.html" target="_blank">https://docs.datomic.com/cloud/schema/schema-reference.html</a></z><z id="t1600428218" t="apbleonard I looked for an equivalent of :db/cardinality in RDFS and OWL and now I have a sore head."><y>#</y><d>2020-09-18</d><h>11:23</h><w>apbleonard</w>I looked for an equivalent of :db/cardinality in RDFS and OWL and now I have a sore head.</z><z id="t1600439142" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U3TSNPRT9&quot;}] : I doubt it. They have very different backgrounds. There is of course owl:maxCardinality but OWL is a logic language for inferencing; not constraints… So owl:maxCardinality 1 doesn’t so much enforce that a predicate has only one value, it just draws an additional conclusion from your data via the non-unique names assumption that all the values for that predicate are actually the same instance."><y>#</y><d>2020-09-18</d><h>14:25</h><w>rickmoynihan</w><a>@apbleonard</a>: I doubt it.  They have very different backgrounds.

There is of course <code>owl:maxCardinality</code> but OWL is a logic language for inferencing; not constraints…  So <code>owl:maxCardinality 1</code> doesn’t so much enforce that a predicate has only one value, it just draws an additional conclusion from your data via the non-unique names assumption that all the values for that predicate are actually the same instance.</z><z id="t1600439177" t="rickmoynihan If you want closed world constraints, you probably want to enforce them via something like SHACL."><y>#</y><d>2020-09-18</d><h>14:26</h><w>rickmoynihan</w>If you want closed world constraints, you probably want to enforce them via something like SHACL.</z><z id="t1600444292" t="apbleonard [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] Thanks. Interesting day reading up on these topics 🙂 Realising that ontologies need not focus on constraints (like cardinality) and still provide value...."><y>#</y><d>2020-09-18</d><h>15:51</h><w>apbleonard</w><a>@rickmoynihan</a> Thanks. Interesting day reading up on these topics <b>🙂</b> Realising that ontologies need not focus on constraints (like cardinality) and still provide value....</z><z id="t1600444900" t="rickmoynihan It really depends what you’re doing, and what camp you’re in. If you’re in the linked data camp; then you mainly just use ontologies as a set of global identifiers for things, for basic modelling. If you’re in the semweb camp; then you use ontologies as a logic to conclude more things about the world via reasoning. You can have a foot in both camps… but the distinction is pretty much real. Many vocabularies purposefully avoid having to lean on owl too much e.g. skos… as many people find OWL to be somewhat arcane and cumbersome and it comes with a load of caveats/nuance etc."><y>#</y><d>2020-09-18</d><h>16:01</h><w>rickmoynihan</w>It really depends what you’re doing, and what camp you’re in.

If you’re in the linked data camp; then you mainly just use ontologies as a set of global identifiers for things, for basic modelling.

If you’re in the semweb camp; then you use ontologies as a logic to conclude more things about the world via reasoning.

You can have a foot in both camps… but the distinction is pretty much real.

Many vocabularies purposefully avoid having to lean on owl too much e.g. skos… as many people find OWL to be somewhat arcane and cumbersome and it comes with a load of caveats/nuance etc.</z><z id="t1600545370" t="apbleonard Then I am definitely in the linked (enterprise/government) data camp :) I don&apos;t need to infer anything new, just get folks to understand their own data when publishing, and others&apos; when consuming... Remembering now that cardinality is fundamental to Datomic as a store of changing data, where a new value for a cardinality one attribute is interpreted as replacing the previous statement from that time forward - whereas cardinality has less need to be clearly defined in point-in-time model of the truth."><y>#</y><d>2020-09-19</d><h>19:56</h><w>apbleonard</w>Then I am definitely in the linked (enterprise/government) data camp :) I don&apos;t need to infer anything new, just get folks to understand their own data when publishing, and others&apos; when consuming...

Remembering now that cardinality is fundamental to Datomic as a store of changing data, where a new value for a cardinality one attribute is interpreted as replacing the previous statement from that time forward - whereas cardinality has less need to be clearly defined in point-in-time model of the truth.</z><z id="t1600849633" t="simongray Hey guys. I’m evaluating Apache Jena (through the Aristotle library) and Neo4j (using the neosemantics RDF plugin) as triplestores for an RDF dataset (wordnets). Ideally, I would have used something even more Datomic-like, but I think that requires investing a significant amount of time writing some RDF boilerplate. I think Jena/aristotle is interesting, but I’m enticed by neo4j offering a bunch of more graph-based visualisations. However, Neo4j locks you out of some random features if you don’t pay for the enterprise edition, removing features from the community edition over time, which is making me worried that I am backing the wrong horse. Another gripe I have with Neo4j is that it seems to run out of memory and crash for certain queries. Do you any experience with either?"><y>#</y><d>2020-09-23</d><h>08:27</h><w>simongray</w>Hey guys. I’m evaluating Apache Jena (through the Aristotle library) and Neo4j (using the neosemantics RDF plugin) as triplestores for an RDF dataset (wordnets). Ideally, I would have used something even more Datomic-like, but I think that requires investing a significant amount of time writing some RDF boilerplate.

I think Jena/aristotle is interesting, but I’m enticed by neo4j offering a bunch of more graph-based visualisations. However, Neo4j locks you out of some random features if you don’t pay for the enterprise edition, removing features from the community edition over time, which is making me worried that I am backing the wrong horse. Another gripe I have with Neo4j is that it seems to run out of memory and crash for certain queries.

Do you any experience with either?</z><z id="t1600984835" t="rickmoynihan Jena or RDF4j are both safe bets, in that they’re active and will be around for a long time to come. All graph databases in my experience are pretty memory hungry, and some queries will inevitably risk that. I can’t say much about visualisation features for graph data, other than that I’ve rarely found them useful in practice. graphviz layout algorithms become too big to big to be useful, and force directed graph visualisations rarely seem useful to me."><y>#</y><d>2020-09-24</d><h>22:00</h><r>rickmoynihan</r>Jena or RDF4j are both safe bets, in that they’re active and will be around for a long time to come.

All graph databases in my experience are pretty memory hungry, and some queries will inevitably risk that.

I can’t say much about  visualisation features for graph data, other than that I’ve rarely found them useful in practice.

graphviz layout algorithms become too big to big to be  useful, and force directed graph visualisations rarely seem useful to me.</z><z id="t1600984895" t="rickmoynihan I think genuinely useful visualisations tend to be heavily curated."><y>#</y><d>2020-09-24</d><h>22:01</h><r>rickmoynihan</r>I think genuinely useful visualisations tend to be heavily curated.</z><z id="t1600984940" t="rickmoynihan But I guess it depends on what you’re doing."><y>#</y><d>2020-09-24</d><h>22:02</h><r>rickmoynihan</r>But I guess it depends on what you’re doing.</z><z id="t1601032580" t="simongray It’s really all about delivering features in very little development time and keeping the system relatively open for potential future development efforts. The datasets are a couple of wordnets (that will eventually be linked) and the users of the system are academics with soft technical skills. Since this needs to be done purely within the limited budget of a single academic project, any kind of generic user interface will be appreciated — in addition to whatever specialised interface I actually have time to develop. Unfortunately, I have bunch of different requirements to live up to - including periodic synchronisation with an ancient SQL database - and only a few months to do it."><y>#</y><d>2020-09-25</d><h>11:16</h><r>simongray</r>It’s really all about delivering features in very little development time and keeping the system relatively open for potential future development efforts. The datasets are a couple of wordnets (that will eventually be linked) and the users of the system are academics with soft technical skills. Since this needs to be done purely within the limited budget of a single academic project, any kind of generic user interface will be appreciated — in addition to whatever specialised interface I actually have time to develop. Unfortunately, I have bunch of different requirements to live up to - including periodic synchronisation with an ancient SQL database - and only a few months to do it.</z><z id="t1601687380" t="EmmanuelOga for visualization there&apos;s a Gephi SPARQL plugin: https://github.com/gephi/gephi/wiki/SemanticWebImport"><y>#</y><d>2020-10-03</d><h>01:09</h><r>EmmanuelOga</r>for visualization there&apos;s a Gephi SPARQL plugin: <a href="https://github.com/gephi/gephi/wiki/SemanticWebImport" target="_blank">https://github.com/gephi/gephi/wiki/SemanticWebImport</a></z><z id="t1601687423" t="EmmanuelOga if you already have RDF as source, you would just have to write some SPARQL queries"><y>#</y><d>2020-10-03</d><h>01:10</h><r>EmmanuelOga</r>if you already have RDF as source, you would just have to write some SPARQL queries</z><z id="t1601687702" t="EmmanuelOga I&apos;ve been playing with RDF4J quite a bit, it has a pretty nice API. Here&apos;s a breadth first search I implemented recently: fun bfs(start: Resource, maxDepth: Int = 5, statementLimit: Int = 1024) = LinkedHashModel().also { sparqlDb!!.connection.use { conn -&gt; val queue = mutableListOf&lt;Pair&lt;Resource, Int&gt;&gt;(Pair(start, 0)) val seen = mutableMapOf&lt;Resource, Boolean&gt;() while (queue.isNotEmpty() &amp;&amp; it.size &lt; statementLimit) { val (next, depth) = queue.removeFirst() if (!seen.containsKey(next)) { for (st in conn.getStatements(next, null, null)) { st.`object`.let { r -&gt; if (r is Resource &amp;&amp; depth &lt; maxDepth) queue.add(Pair(r, depth + 1)) } it.add(st) } } seen[next] = true } } } "><y>#</y><d>2020-10-03</d><h>01:15</h><r>EmmanuelOga</r>I&apos;ve been playing with RDF4J quite a bit, it has a pretty nice API. Here&apos;s a breadth first search I implemented recently:
<pre>fun bfs(start: Resource, maxDepth: Int = 5, statementLimit: Int = 1024) =
        LinkedHashModel().also {
            sparqlDb!!.connection.use { conn -&gt;
                val queue = mutableListOf&lt;Pair&lt;Resource, Int&gt;&gt;(Pair(start, 0))
                val seen = mutableMapOf&lt;Resource, Boolean&gt;()

                while (queue.isNotEmpty() &amp;&amp; it.size &lt; statementLimit) {
                    val (next, depth) = queue.removeFirst()

                    if (!seen.containsKey(next)) {
                        for (st in conn.getStatements(next, null, null)) {
                            st.`object`.let { r -&gt;
                                if (r is Resource &amp;&amp; depth &lt; maxDepth) queue.add(Pair(r, depth + 1))
                            }
                            it.add(st)
                        }
                    }
                    seen[next] = true
                }
            }
        } </pre></z><z id="t1601687718" t="EmmanuelOga Kotlin, but you get the idea"><y>#</y><d>2020-10-03</d><h>01:15</h><r>EmmanuelOga</r>Kotlin, but you get the idea</z><z id="t1601687786" t="EmmanuelOga I also been eyeing https://github.com/jgrapht/jgrapht . It has a collection of graph algorithms, visualizations, serialization to graphviz and other formats that you could load into gephi, etc."><y>#</y><d>2020-10-03</d><h>01:16</h><r>EmmanuelOga</r>I also been eyeing <a href="https://github.com/jgrapht/jgrapht" target="_blank">https://github.com/jgrapht/jgrapht</a>. It has a collection of graph algorithms, visualizations, serialization to graphviz and other formats that you could load into gephi, etc.</z><z id="t1601686538" t="EmmanuelOga https://twitter.com/EmmanuelOga/status/1312194254949609475"><y>#</y><d>2020-10-03</d><h>00:55</h><w>EmmanuelOga</w><a href="https://twitter.com/EmmanuelOga/status/1312194254949609475" target="_blank">https://twitter.com/EmmanuelOga/status/1312194254949609475</a></z><z id="t1602492187" t="simongray Can someone with more knowledge of RDF explain to me the rationale of langStrings like “word”@en? They seem like the completely wrong abstraction. Strings of letters are not encoded in a language, but rather it’s the other way around: languages use strings of letters to represent words and these string literals can of course be claimed by multiple languages and their interpretation may be different, but they are still the same string of letters. With the way langStrings are implemented in SPARQL queries (an enforced filter), basic string queries for words in multiple languages either suffer a combinatorial explosion of language-encoded strings or an unintentionally smaller result set (if not using an exhaustive set of languages). It seems to me like languages are better implemented simply as an entity that can be linked to any number of strings or simply just a separate property. This can then be used to selectively filter in different languages. I cannot see any benefit to hardcoding languages into string literals. I wonder why the current implementation is even a thing."><y>#</y><d>2020-10-12</d><h>08:43</h><w>simongray</w>Can someone with more knowledge of RDF explain to me the rationale of langStrings like “word”@en? They seem like the completely wrong abstraction. Strings of letters are not encoded in a language, but rather it’s the other way around: languages use strings of letters to represent words and these string literals can of course be claimed by multiple languages and their interpretation may be different, but they are still the same string of letters. With the way langStrings are implemented in SPARQL queries (an enforced filter), basic string queries for words in multiple languages either suffer a combinatorial explosion of language-encoded strings or an unintentionally smaller result set (if not using an exhaustive set of languages).

It seems to me like languages are better implemented simply as an entity that can be linked to any number of strings or simply just a separate property. This can then be used to selectively filter in different languages. I cannot see any benefit to hardcoding languages into string literals. I wonder why the current implementation is even a thing.</z><z id="t1602494435" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U4P4NREBY&quot;}] I think it’s partly pragmatic, but it runs deeper than lang strings. For better or worse no RDF primitives can be used in ?s position. Whatever design was taken in this area would make similar trade offs elsewhere, so the implemented solution is I think a fair compromise. Philosophically in RDF you have a kind of platonic concept which is assumed to exist outside of human languages, and any human language can in principle know it; but it’s the same universal concept regardless of human language. Obviously there are competing schools in philosophy of language that say meaning is relative to other terms, and consequently relative to the language they’re expressed in (e.g. the Sapir–Whorf hypothesis), but that’s not how things are modelled in RDF. In RDF labels etc are just annotation properties; i.e. they don’t carry any formal semantics… they’re just an aid to humans, the real meaning is intended to be in the identifiers and their relationships. Regarding SPARQL’s deficiencies (real or perceived), SPARQL was developed independently of RDF. RDF came first. Of course you could choose to implement language specific predicates en:name , fr:nom if you had a good reason to — but it’s not how modelling in conventionally done."><y>#</y><d>2020-10-12</d><h>09:20</h><w>rickmoynihan</w><a>@simongray</a> I think it’s partly pragmatic, but it runs deeper than lang strings.  For better or worse no RDF primitives can be used in <code>?s</code> position.  Whatever design was taken in this area would make similar trade offs elsewhere, so the implemented solution is I think a fair compromise.

Philosophically in RDF you have a kind of platonic concept which is assumed to exist outside of human languages, and any human language can in principle know it; but it’s the same universal concept regardless of human language.  Obviously there are competing schools in philosophy of language that say meaning is relative to other terms, and consequently relative to the language they’re expressed in (e.g. the Sapir–Whorf hypothesis), but that’s not how things are modelled in RDF.

In RDF labels etc are just annotation properties; i.e. they don’t carry any formal semantics… they’re just an aid to humans, the real meaning is intended to be in the identifiers and their relationships.

Regarding SPARQL’s deficiencies (real or perceived), SPARQL was developed independently of RDF.  RDF came first.

Of course you could choose to implement language specific predicates <code>en:name</code>, <code>fr:nom</code> if you had a good reason to — but it’s not how modelling in conventionally done.</z><z id="t1602494665" t="simongray Well, looking past the fact that basic multi-language string queries cannot be guaranteed to be correct when using unknown datasets since the full range of possible language-encodings must be specified in the query, It just seems to defy logic to me as well. Are names language-encoded? Do I have 6000+ different names then to account for every possible official language?"><y>#</y><d>2020-10-12</d><h>09:24</h><w>simongray</w>Well, looking past the fact that basic multi-language string queries cannot be guaranteed to be correct when using unknown datasets since the full range of possible language-encodings must be specified in the query, It just seems to defy logic to me as well. Are names language-encoded? Do I have 6000+ different names then to account for every possible official language?</z><z id="t1602494778" t="simongray It’s also seems completely idiosyncratic to RDF. What other languages have implemented languages as equality-distorting aspects of strings literals?"><y>#</y><d>2020-10-12</d><h>09:26</h><w>simongray</w>It’s also seems completely idiosyncratic to RDF. What other languages have implemented languages as equality-distorting aspects of strings literals?</z><z id="t1602494804" t="rickmoynihan Not sure how to interpret this: &gt; Are names language-encoded?"><y>#</y><d>2020-10-12</d><h>09:26</h><w>rickmoynihan</w>Not sure how to interpret this:

&gt; Are names language-encoded?</z><z id="t1602494855" t="simongray If I am to query for my own name in an RSF resource how should I refer to it? “Simon”@en, “Simon”@da, and 6000 other entries?"><y>#</y><d>2020-10-12</d><h>09:27</h><w>simongray</w>If I am to query for my own name in an RSF resource how should I refer to it? “Simon”@en, “Simon”@da, and 6000 other entries?</z><z id="t1602494886" t="simongray My name has nothing to do with my mother tongue"><y>#</y><d>2020-10-12</d><h>09:28</h><w>simongray</w>My name has nothing to do with my mother tongue</z><z id="t1602494935" t="rickmoynihan In that case just use &quot;simon&quot; … an xsd:string language is optional."><y>#</y><d>2020-10-12</d><h>09:28</h><w>rickmoynihan</w>In that case just use <code>&quot;simon&quot;</code> … an <code>xsd:string</code> language is optional.</z><z id="t1602495035" t="simongray That has not been my experience."><y>#</y><d>2020-10-12</d><h>09:30</h><w>simongray</w>That has not been my experience.</z><z id="t1602495116" t="samir [:attrs {:href &quot;/_/_/users/U4P4NREBY&quot;}] It is useful to see the lang string as belonging to the presentation layer. Whenever you deal with data that will be queried, with most SPARQL implementations, basic strings are the main choice. (opiniated)"><y>#</y><d>2020-10-12</d><h>09:31</h><w>samir</w><a>@simongray</a> It is useful to see the lang string as belonging to the presentation layer. Whenever you deal with data that will be queried, with most SPARQL implementations, basic strings are the main choice. (opiniated)</z><z id="t1602495192" t="simongray If I search for a basic string, won’t this simple not not include any language-encoded strings?"><y>#</y><d>2020-10-12</d><h>09:33</h><w>simongray</w>If I search for a basic string, won’t this simple not not include any language-encoded strings?</z><z id="t1602495211" t="simongray e.g. https://stackoverflow.com/questions/40246175/sparql-matching-literals-with-any-language-tags-without-run-into-timeout"><y>#</y><d>2020-10-12</d><h>09:33</h><w>simongray</w>e.g. <a href="https://stackoverflow.com/questions/40246175/sparql-matching-literals-with-any-language-tags-without-run-into-timeout" target="_blank">https://stackoverflow.com/questions/40246175/sparql-matching-literals-with-any-language-tags-without-run-into-timeout</a></z><z id="t1602495249" t="simongray so in order to actually return a full result set I will have to construct a query for the string in every possible human language in addition to the basic string"><y>#</y><d>2020-10-12</d><h>09:34</h><w>simongray</w>so in order to actually return a full result set I will have to construct a query for the string in every possible human language in addition to the basic string</z><z id="t1602495322" t="simongray I ran into this issue already using Apache Jena (aristotle)"><y>#</y><d>2020-10-12</d><h>09:35</h><w>simongray</w>I ran into this issue already using Apache Jena (aristotle)</z><z id="t1602495374" t="rickmoynihan String searching isn’t really what RDF is optimised for. I’d say the use case for lang strings is mainly to provide labels for display. And yes this can be awkward. multi-lingual stuff is always awkward in any system. I’m not defending RDF here btw, I’m just explaining how to think of it."><y>#</y><d>2020-10-12</d><h>09:36</h><w>rickmoynihan</w>String searching isn’t really what RDF is optimised for.  I’d say the use case for lang strings is mainly to provide labels for display.  And yes this can be awkward.  multi-lingual stuff is always awkward in any system.

I’m not defending RDF here btw, I’m just explaining how to think of it.</z><z id="t1602495445" t="simongray When the dream is interconnectivity of resources, the fact that strings are represented in this way with no reliable enforcement mechanism seems to completely destroy any hope of integrating multilingual datasets"><y>#</y><d>2020-10-12</d><h>09:37</h><w>simongray</w>When the dream is interconnectivity of resources, the fact that strings are represented in this way with no reliable enforcement mechanism seems to completely destroy any hope of integrating multilingual datasets</z><z id="t1602495460" t="samir This is what I meant with presentation layer. You fetch the lang strings with the proper language before display of the entity in some viewer. Or some of them if you have precedence rules between languages"><y>#</y><d>2020-10-12</d><h>09:37</h><w>samir</w>This is what I meant with presentation layer. You fetch the lang strings with the proper language before display of the entity in some viewer. Or some of them if you have precedence rules between languages</z><z id="t1602495497" t="rickmoynihan Typically most real world systems won’t work in 6000+ languages. You’ll probably have a just a handful…. So filtering to the users locale is tractable. Normally I just slightly overselect, e.g. :s rdfs:label ?label and then pick the most appropriate based on a precedence etc."><y>#</y><d>2020-10-12</d><h>09:38</h><w>rickmoynihan</w>Typically most real world systems won’t work in 6000+ languages.  You’ll probably have a just a handful….  So filtering to the users locale is tractable.  Normally I just slightly overselect, e.g. <code>:s rdfs:label ?label</code> and then pick the most appropriate based on a precedence etc.</z><z id="t1602495511" t="rickmoynihan Jinx!"><y>#</y><d>2020-10-12</d><h>09:38</h><w>rickmoynihan</w>Jinx!</z><z id="t1602495630" t="simongray Ok, so is it the case that different RDF implementations treat strings with no language-encoding in different ways? Cause when I have tried searching a for basic string it will not return any results unless I specify the language (the enforced filter). This was in Apache Jena"><y>#</y><d>2020-10-12</d><h>09:40</h><w>simongray</w>Ok, so is it the case that different RDF implementations treat strings with no language-encoding in different ways? Cause when I have tried searching a for basic string it will not return any results unless I specify the language (the enforced filter). This was in Apache Jena</z><z id="t1602495667" t="samir The main point of RDF is actually to take resource identity very seriously. The labels are seen as helper data. I agree that this is suboptimal. On the other hand, systems integrating over multiple language can analyse all labels to infer the likelihood that two entity are identical and then add an appropriate statement articulating this fact"><y>#</y><d>2020-10-12</d><h>09:41</h><w>samir</w>The main point of RDF is actually to take resource identity very seriously. The labels are seen as helper data. I agree that this is suboptimal. On the other hand, systems integrating over multiple language can analyse all labels to infer the likelihood that two entity are identical and then add an appropriate statement articulating this fact</z><z id="t1602495716" t="simongray But langStrings are not just used for labels"><y>#</y><d>2020-10-12</d><h>09:41</h><w>simongray</w>But langStrings are not just used for labels</z><z id="t1602495738" t="simongray The object of a triplet can either be a resource or a literal, right?"><y>#</y><d>2020-10-12</d><h>09:42</h><w>simongray</w>The object of a triplet can either be a resource or a literal, right?</z><z id="t1602495801" t="samir That is right, I used the term labels to simplify the discussion"><y>#</y><d>2020-10-12</d><h>09:43</h><w>samir</w>That is right, I used the term labels to simplify the discussion</z><z id="t1602495825" t="rickmoynihan I’m pretty sure most systems treat (no)-language encoding the same. :foo rdfs:label &quot;foo&quot; and :foo rdfs:label &quot;foo&quot;@en are different triples. I’m pretty sure this is part of the standard. Obviously you can choose to handle this stuff at an ETL layer if you need too… e.g. by normalising labels into xsd strings or whatever to suit your application."><y>#</y><d>2020-10-12</d><h>09:43</h><w>rickmoynihan</w>I’m pretty sure most systems treat (no)-language encoding the same.  <code>:foo rdfs:label &quot;foo&quot;</code> and <code>:foo rdfs:label &quot;foo&quot;@en</code> are different triples.  I’m pretty sure this is part of the standard.

Obviously you can choose to handle this stuff at an ETL layer if you need too… e.g. by normalising labels into xsd strings or whatever to suit your application.</z><z id="t1602495961" t="samir Actually text searching is not really part of SPARQL, often you have a parallel text indexing service (and the clauses for text search can be integrated in the SPARQL request). From the point of view of RDF “foo” and “foo”@en are just different litterals."><y>#</y><d>2020-10-12</d><h>09:46</h><w>samir</w>Actually text searching is not really part of SPARQL, often you have a parallel text indexing service (and the clauses for text search can be integrated in the SPARQL request). From the point of view of RDF “foo” and “foo”@en are just different litterals.</z><z id="t1602496001" t="rickmoynihan yeah, though some triple stores have non standard extensions to handle it better"><y>#</y><d>2020-10-12</d><h>09:46</h><w>rickmoynihan</w>yeah, though some triple stores have non standard extensions to handle it better</z><z id="t1602496006" t="simongray ok… thanks for responding both of you. I still think this aspect of RDF is completely idiosyncratic and to me simply introduces complexity that will need to be handled elsewhere."><y>#</y><d>2020-10-12</d><h>09:46</h><w>simongray</w>ok… thanks for responding both of you. I still think this aspect of RDF is completely idiosyncratic and to me simply introduces complexity that will need to be handled elsewhere.</z><z id="t1602496088" t="simongray I can’t imagine dealing with a programming langauge where every string is potentially language-encoded… shudder"><y>#</y><d>2020-10-12</d><h>09:48</h><w>simongray</w>I can’t imagine dealing with a programming langauge where every string is potentially language-encoded… shudder</z><z id="t1602496194" t="rickmoynihan You’re right it is idiosyncratic, and it does pass the complexity buck, and I have experienced this frustration myself. So I’m not disagreeing with you. Though I think any solution that wasn’t tailor made for your application would be idiosyncratic here too. You really just need to learn to work with it rather than against it. If you do that your life will be easier."><y>#</y><d>2020-10-12</d><h>09:49</h><w>rickmoynihan</w>You’re right it is idiosyncratic, and it does pass the complexity buck, and I have experienced this frustration myself.  So I’m not disagreeing with you.  Though I think any solution that wasn’t tailor made for your application would be idiosyncratic here too.

You really just need to learn to work with it rather than against it.  If you do that your life will be easier.</z><z id="t1602496440" t="simongray Yeah. I guess I am somewhat getting around it by representing the data as a labeled property graph instead, both in Neo4j and using ubergraph in Clojure. I just can’t believe that this passed through multiple committee (re)designs and the - to me - obvious and much more flexible way to represent languages already used in HTML/XML was not simply reused here."><y>#</y><d>2020-10-12</d><h>09:54</h><w>simongray</w>Yeah. I guess I am somewhat getting around it by representing the data as a labeled property graph instead, both in Neo4j and using ubergraph in Clojure. I just can’t believe that this passed through multiple committee (re)designs and the - to me - obvious and much more flexible way to represent languages already used in HTML/XML was not simply reused here.</z><z id="t1602496856" t="rickmoynihan how do xml/html solve this? It sounds to me like your issue is more that you’re doing an exact string match, rather using FTS."><y>#</y><d>2020-10-12</d><h>10:00</h><r>rickmoynihan</r>how do xml/html solve this?

It sounds to me like your issue is more that you’re doing an exact string match, rather using FTS.</z><z id="t1602496899" t="simongray lang=“en” and xml:lang=“en”"><y>#</y><d>2020-10-12</d><h>10:01</h><r>simongray</r>lang=“en” and xml:lang=“en”</z><z id="t1602496906" t="simongray what’s FTS?"><y>#</y><d>2020-10-12</d><h>10:01</h><r>simongray</r>what’s FTS?</z><z id="t1602496914" t="rickmoynihan full text search"><y>#</y><d>2020-10-12</d><h>10:01</h><r>rickmoynihan</r>full text search</z><z id="t1602497003" t="simongray well, I am a SPARQL n00b. Is full-text search built-in and if that is the case, how is it accessed?"><y>#</y><d>2020-10-12</d><h>10:03</h><r>simongray</r>well, I am a SPARQL n00b. Is full-text search built-in and if that is the case, how is it accessed?</z><z id="t1602497041" t="rickmoynihan It’s not part of the standard, but lots of backends support it"><y>#</y><d>2020-10-12</d><h>10:04</h><r>rickmoynihan</r>It’s not part of the standard, but lots of backends support it</z><z id="t1602497043" t="rickmoynihan https://jena.apache.org/documentation/query/text-query.html"><y>#</y><d>2020-10-12</d><h>10:04</h><r>rickmoynihan</r><a href="https://jena.apache.org/documentation/query/text-query.html" target="_blank">https://jena.apache.org/documentation/query/text-query.html</a></z><z id="t1602497196" t="simongray I see. Thanks for pointing that out. I tried getting around the language-encodings by using regex but that was just unbearably slow. I guess FTS is close to the performance of matching string directly?"><y>#</y><d>2020-10-12</d><h>10:06</h><r>simongray</r>I see. Thanks for pointing that out. I tried getting around the language-encodings by using regex but that was just unbearably slow. I guess FTS is close to the performance of matching string directly?</z><z id="t1602497454" t="rickmoynihan Yes it should be. It’ll use lucene indexing etc underneath, and you can probably even tweak the indexing there too should you need to. Enabling FTS will usually make indexing slower of course."><y>#</y><d>2020-10-12</d><h>10:10</h><r>rickmoynihan</r>Yes it should be.  It’ll use lucene indexing etc underneath, and you can probably even tweak the indexing there too should you need to.

Enabling FTS will usually make indexing slower of course.</z><z id="t1602497564" t="simongray Right. Thank you very much for educating me. I just noticed that I had already starred one of your libraries on github as part of my initial research of dealing with RDF in Clojure. 🙂"><y>#</y><d>2020-10-12</d><h>10:12</h><r>simongray</r>Right. Thank you very much for educating me. I just noticed that I had already starred one of your libraries on github as part of my initial research of dealing with RDF in Clojure. <b>🙂</b></z><z id="t1602496506" t="simongray In general, RDF seems a bit over-engineered"><y>#</y><d>2020-10-12</d><h>09:55</h><w>simongray</w>In general, RDF seems a bit over-engineered</z><z id="t1602497842" t="rickmoynihan I can see why you might think that, but I think RDF itself is actually very well engineered. RDF is actually pretty minimal. The complication comes from the fact that there are lots of interwoven standards; so the ecosystem is complicated; and so are some of the other standards, e.g. OWL. You should only use what you really need though."><y>#</y><d>2020-10-12</d><h>10:17</h><w>rickmoynihan</w>I can see why you might think that, but I think RDF itself is actually very well engineered.  RDF is actually pretty minimal.

The complication comes from the fact that there are lots of interwoven standards; so the ecosystem is complicated; and so are some of the other standards, e.g. OWL.  You should only use what you really need though.</z><z id="t1602502420" t="Steven Deobald Out of curiosity, what are the domains/projects you folks are working on with RDF and Clojure? I&apos;m currently working on an implementation of a digital library for http://pariyatti.org , which requires quite a bit of relationship management between entities: ancient Pali literature with many variations and translations over the past ~2000 years, authors, topics, etc. I started with Neo4j but I&apos;m currently spiking a move to Crux, for a variety of reasons. Because librarians at http://pariyatti.org will forever consist of volunteers with limited time, I&apos;ve leaned away from semantic web tech in favour of writing something (potentially?) simpler by hand... but if the project ever begins to concern itself with the contents of the documents within the library, it might be foolish to continue avoiding things like RDF. My go-to example at the current granularity is Ledi Sayadaw, a monk who authored a long list of books in contemporary Pali about ~100 years ago. He&apos;s now a topic for other, modern literature in other languages. Those sorts of relationships would be a nightmare in Postgres but they&apos;ve been manageable in Neo4j and Crux so far. &quot;Contents&quot; might be something as fine-grained as the knowledge that kukkara in Pali means dog in English (and a dozen other translations)... obviously I have no intention of encoding that knowledge at that granularity in a database layer I&apos;ve hand-rolled. 😉 Have other folks in here walked a similar road?"><y>#</y><d>2020-10-12</d><h>11:33</h><w>Steven Deobald</w>Out of curiosity, what are the domains/projects you folks are working on with RDF and Clojure? I&apos;m currently working on an implementation of a digital library for <a href="http://pariyatti.org" target="_blank">http://pariyatti.org</a>, which requires quite a bit of relationship management between entities: ancient Pali literature with many variations and translations over the past ~2000 years, authors, topics, etc. I started with Neo4j but I&apos;m currently spiking a move to Crux, for a variety of reasons. Because librarians at <a href="http://pariyatti.org" target="_blank">http://pariyatti.org</a> will forever consist of volunteers with limited time, I&apos;ve leaned away from semantic web tech in favour of writing something (potentially?) simpler by hand... but if the project ever begins to concern itself with the contents of the documents within the library, it might be foolish to continue avoiding things like RDF.

My go-to example at the current granularity is Ledi Sayadaw, a monk who authored a long list of books in contemporary Pali about ~100 years ago. He&apos;s now a topic for other, modern literature in other languages. Those sorts of relationships would be a nightmare in Postgres but they&apos;ve been manageable in Neo4j and Crux so far. &quot;Contents&quot; might be something as fine-grained as the knowledge that <code>kukkara</code> in Pali means <code>dog</code> in English (and a dozen other translations)... obviously I have no intention of encoding that knowledge at that granularity in a database layer I&apos;ve hand-rolled. <b>😉</b> Have other folks in here walked a similar road?</z><z id="t1602504624" t="simongray I think having to figure everything out yourself is both freeing and requires more extensive research. Sounds like you don&apos;t need to integrate with any other sources or distribute your data? In that case, I don&apos;t think RDF is a requirement."><y>#</y><d>2020-10-12</d><h>12:10</h><r>simongray</r>I think having to figure everything out yourself is both freeing and requires more extensive research. Sounds like you don&apos;t need to integrate with any other sources or distribute your data? In that case, I don&apos;t think RDF is a requirement.</z><z id="t1602505007" t="simongray I&apos;ve inherited the official Danish wordnet which was created as part of a big research project more than a decade ago. The primary data lives in a SQL db and only exists as RDF in a limited exported version using the original draft version of RDF/XML. I need to support linking with the Princeton WordNet while supporting a bunch of future functionality, so my mission has been normalising the usage of RDF and graphs for data modelling, including at the db level."><y>#</y><d>2020-10-12</d><h>12:16</h><r>simongray</r>I&apos;ve inherited the official Danish wordnet which was created as part of a big research project more than a decade ago. The primary data lives in a SQL db and only exists as RDF in a limited exported version using the original draft version of RDF/XML. I need to support linking with the Princeton WordNet while supporting a bunch of future functionality, so my mission has been normalising the usage of RDF and graphs for data modelling, including at the db level.</z><z id="t1602509597" t="rickmoynihan Actually the cultural/arts/museum space has historically been a large adopter of rdf and linked data. Lots of big museums, art collections and libraries etc use RDF for their metadata catalogs. There is definitely a tonne of vocabularies and work using RDF in this space… in particular probably: https://iiif.io/ which is adopted by dozens of national musuems/galleries etc worldwide. but also cidoc: http://www.cidoc-crm.org/ and probably a bunch more. Not sure what the latest stuff is, but I could probably find out. SKOS was designed for representing thesaurus etc. https://www.w3.org/2004/02/skos/ [:attrs {:href &quot;/_/_/users/U01AVNG2XNF&quot;}] I’d say there’s a strong argument to use RDF here, given it’s wide adoption. Also there’s a good chance RDF will be around long after trendier stuff like crux."><y>#</y><d>2020-10-12</d><h>13:33</h><r>rickmoynihan</r>Actually the cultural/arts/museum space has historically been a large adopter of rdf and linked data.

Lots of big museums, art collections and libraries etc use RDF for their metadata catalogs.

There is definitely a tonne of vocabularies and work using RDF in this space… in particular probably:

<a href="https://iiif.io/" target="_blank">https://iiif.io/</a>

which is adopted by dozens of national musuems/galleries etc worldwide.

but also cidoc:

<a href="http://www.cidoc-crm.org/" target="_blank">http://www.cidoc-crm.org/</a>

and probably a bunch more.

Not sure what the latest stuff is, but I could probably find out.

SKOS was designed for representing thesaurus etc.

<a href="https://www.w3.org/2004/02/skos/" target="_blank">https://www.w3.org/2004/02/skos/</a>

<a>@U01AVNG2XNF</a> I’d say there’s a strong argument to use RDF here, given it’s wide adoption.  Also there’s a good chance RDF will be around long after trendier stuff like crux.</z><z id="t1602511493" t="Steven Deobald [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] Interesting! If you had a line on more recent developments, I&apos;d be very curious to know what they are. My entire career was spent in finance / e-commerce type things so I&apos;m really a fish out of water in what seems to be an almost entirely government / academic dominated space."><y>#</y><d>2020-10-12</d><h>14:04</h><r>Steven Deobald</r><a>@U06HHF230</a> Interesting! If you had a line on more recent developments, I&apos;d be very curious to know what they are. My entire career was spent in finance / e-commerce type things so I&apos;m really a fish out of water in what seems to be an almost entirely government / academic dominated space.</z><z id="t1602511635" t="Steven Deobald &gt; Also there’s a good chance RDF will be around long after trendier stuff like crux. [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] I suppose I hadn&apos;t considered these two things at odds with each other. Is there a particular backing store(s) people tend to rely on in the world of RDF?"><y>#</y><d>2020-10-12</d><h>14:07</h><r>Steven Deobald</r>&gt;  Also there’s a good chance RDF will be around long after trendier stuff like crux.
<a>@U06HHF230</a> I suppose I hadn&apos;t considered these two things at odds with each other. Is there a particular backing store(s) people tend to rely on in the world of RDF?</z><z id="t1602511838" t="Steven Deobald [:attrs {:href &quot;/_/_/users/U4P4NREBY&quot;}] You&apos;re right, for the foreseeable future this system won&apos;t integrate with any other or require any sort of data distribution. Pariyatti will be internally curated and won&apos;t resemble anything like Wikimedia&apos;s work. That said, it&apos;s a fine line between a curated library and a system for researching ancient linguistics. The latter no doubt has a lot to learn from the work already done on the semantic web, whether the system is open or not."><y>#</y><d>2020-10-12</d><h>14:10</h><r>Steven Deobald</r><a>@simongray</a> You&apos;re right, for the foreseeable future this system won&apos;t integrate with any other or require any sort of data distribution. Pariyatti will be internally curated and won&apos;t resemble anything like Wikimedia&apos;s work. That said, it&apos;s a fine line between a curated library and a system for researching ancient linguistics. The latter no doubt has a lot to learn from the work already done on the semantic web, whether the system is open or not.</z><z id="t1602512303" t="Steven Deobald [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] Do you know of any specific organizations or projects using cidoc? I&apos;m surprised it didn&apos;t come up when I was researching off-the-shelf tools."><y>#</y><d>2020-10-12</d><h>14:18</h><r>Steven Deobald</r><a>@U06HHF230</a> Do you know of any specific organizations or projects using cidoc? I&apos;m surprised it didn&apos;t come up when I was researching off-the-shelf tools.</z><z id="t1602516307" t="rickmoynihan http://www.cidoc-crm.org/stackeholders http://www.cidoc-crm.org/sig-members-list I guess the above lists would be a good place look Also the Smithsonian… https://americanart.si.edu/about/lod There’s been lots of other linked data projects in this area; but I can’t recall many off the top of my head… I can ask some colleagues."><y>#</y><d>2020-10-12</d><h>15:25</h><r>rickmoynihan</r><a href="http://www.cidoc-crm.org/stackeholders" target="_blank">http://www.cidoc-crm.org/stackeholders</a>
<a href="http://www.cidoc-crm.org/sig-members-list" target="_blank">http://www.cidoc-crm.org/sig-members-list</a>

I guess the above lists would be a good place look

Also the Smithsonian…

<a href="https://americanart.si.edu/about/lod" target="_blank">https://americanart.si.edu/about/lod</a>

There’s been lots of other linked data projects in this area; but I can’t recall many off the top of my head… I can ask some colleagues.</z><z id="t1602516504" t="rickmoynihan &gt; Is there a particular backing store(s) people tend to rely on in the world of RDF? There are many… probably half a dozen serious commercial options, plus the two big opensource ones Jena and RDF4j; and then maybe twenty or more opensource ones targeting various niches or in various stages of development."><y>#</y><d>2020-10-12</d><h>15:28</h><r>rickmoynihan</r>&gt;  Is there a particular backing store(s) people tend to rely on in the world of RDF?
There are many… probably half a dozen serious commercial options, plus the two big opensource ones Jena and RDF4j; and then maybe twenty or more opensource ones targeting various niches or in various stages of development.</z><z id="t1602564363" t="Steven Deobald Not sure why I wasn&apos;t expecting to find the Stakeholders list under Community . :woman-facepalming: Thanks! I&apos;d love to know about more LOD projects... understanding the surface area will help make more informed decisions as the project plods forward."><y>#</y><d>2020-10-13</d><h>04:46</h><r>Steven Deobald</r>Not sure why I wasn&apos;t expecting to find the Stakeholders list under <code>Community</code>. <b>:woman-facepalming:</b> Thanks! I&apos;d love to know about more LOD projects... understanding the surface area will help make more informed decisions as the project plods forward.</z><z id="t1602564724" t="Steven Deobald rdf4j .... offers an easy-to-use API that can be connected to all leading RDF database solutions. -- I&apos;m still finding this whole space a bit confusing. It seems like Jena actually does all the database work itself (maybe?) but RDF4j relies exclusively on a third-party triplestore? https://en.wikipedia.org/wiki/Comparison_of_triplestores sheds a little light on the situation but I&apos;m still a bit unclear who/what is writing data to disk in these different setups. 😉"><y>#</y><d>2020-10-13</d><h>04:52</h><r>Steven Deobald</r><code>rdf4j .... offers an easy-to-use API that can be connected to all leading RDF database solutions.</code> -- I&apos;m still finding this whole space a bit confusing. It seems like Jena actually does all the database work itself (maybe?) but RDF4j relies exclusively on a third-party triplestore? <a href="https://en.wikipedia.org/wiki/Comparison_of_triplestores" target="_blank">https://en.wikipedia.org/wiki/Comparison_of_triplestores</a> sheds a little light on the situation but I&apos;m still a bit unclear who/what is writing data to disk in these different setups. <b>😉</b></z><z id="t1602575830" t="rickmoynihan RDF4j has several triple store backends just like Jena. In particular a native store (which is persisted to disk) and a memory store, plus a few more… It also comes with a workbench (database server) that you can run, like Jena (Jena’s is called Fuseki). RDF4j has a much cleaner API in my mind, but Jena has more features in some areas. In particular WRT inferencing. (Disclosure I’m actually supposed to be a core contributor to RDF4j; but it’s only because I submitted a bunch of extensive bug reports a few years back; with a few small patches.) I actually use Jena in a few places too."><y>#</y><d>2020-10-13</d><h>07:57</h><r>rickmoynihan</r>RDF4j has several triple store backends just like Jena.  In particular a native store (which is persisted to disk) and a memory store, plus a few more…  It also comes with a workbench (database server) that you can run, like Jena (Jena’s is called Fuseki).

RDF4j has a much cleaner API in my mind, but Jena has more features in some areas.  In particular WRT inferencing.

(Disclosure I’m actually supposed to be a core contributor to RDF4j; but it’s only because I submitted a bunch of extensive bug reports a few years back; with a few small patches.)  I actually use Jena in a few places too.</z><z id="t1602508951" t="rickmoynihan publishing government data (mostly statistical data)"><y>#</y><d>2020-10-12</d><h>13:22</h><w>rickmoynihan</w>publishing government data (mostly statistical data)</z><z id="t1602511932" t="Steven Deobald Very cool. My partner has been working with http://CivicDataLab.in for the past half year or so, in a similar space. I&apos;m not sure they&apos;ve ever even contemplated RDF for their statistical data, though."><y>#</y><d>2020-10-12</d><h>14:12</h><w>Steven Deobald</w>Very cool. My partner has been working with <a href="http://CivicDataLab.in" target="_blank">http://CivicDataLab.in</a> for the past half year or so, in a similar space. I&apos;m not sure they&apos;ve ever even contemplated RDF for their statistical data, though.</z><z id="t1602576960" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U01AVNG2XNF&quot;}] Ok I asked some friends and colleagues about some of the bigger projects in the heritage/museums/library/arts/humanities space here’s some more: The british museum’s catalog (this is the one I remembered but couldn’t quite find): https://www.britishmuseum.org/collection/object/W_1892-0516-351-a (looks like they’ve hidden or removed their public sparql endpoint, but the structure of the collections is clearly SKOS - I have it on good authority they’re also using cidoc that I mentioned in the thread) Another big one The library of congress: https://id.loc.gov/ Europeana — which is a huge cross europe collaboration to connect the european heritage sector, through various projects based around linked data etc… https://pro.europeana.eu/ e.g. see here for one of their many big projects: https://www.europeana.eu/en try searching for e.g. “van gogh” or historiana here: https://historiana.eu/ plus many others… Also the British Library: https://bnb.data.bl.uk/ Also all UK legislation is represented/managed as a repository of linked data, giving URI identifiers for everything on the official site here: https://www.legislation.gov.uk/developer/uris"><y>#</y><d>2020-10-13</d><h>08:16</h><w>rickmoynihan</w><a>@steven427</a> Ok I asked some friends and colleagues about some of the bigger projects in the heritage/museums/library/arts/humanities space here’s some more:

The british museum’s catalog (this is the one I remembered but couldn’t quite find):

<a href="https://www.britishmuseum.org/collection/object/W_1892-0516-351-a" target="_blank">https://www.britishmuseum.org/collection/object/W_1892-0516-351-a</a> (looks like they’ve hidden or removed their public sparql endpoint, but the structure of the collections is clearly SKOS - I have it on good authority they’re also using cidoc that I mentioned in the thread)

Another big one The library of congress:

<a href="https://id.loc.gov/" target="_blank">https://id.loc.gov/</a>

Europeana — which is a huge cross europe collaboration to connect the european heritage sector, through various projects based around linked data etc…

<a href="https://pro.europeana.eu/" target="_blank">https://pro.europeana.eu/</a>

e.g. see here for one of their many big projects: <a href="https://www.europeana.eu/en" target="_blank">https://www.europeana.eu/en</a> try searching for e.g. “van gogh” or historiana here: <a href="https://historiana.eu/" target="_blank">https://historiana.eu/</a> plus many others…

Also the British Library:

<a href="https://bnb.data.bl.uk/" target="_blank">https://bnb.data.bl.uk/</a>

Also all UK legislation is represented/managed as a repository of linked data, giving URI identifiers for everything on the official site here:

<a href="https://www.legislation.gov.uk/developer/uris" target="_blank">https://www.legislation.gov.uk/developer/uris</a></z><z id="t1602579752" t="rickmoynihan Ok asked another friend who works for a client of ours who used to work at the BBC on their linked data platforms &gt; 4 years ago… This is what he said about heritage orgs that he knows of who ha(d|ve) linked data projects in the UK at least: &gt; BBC themselves, The National Archives, British Museum, National Library Wales, National Library Scotland, Rijksmuseum, Getty (thesauri for artists, geography and others), Wellcome, Archaeology Data Service, People’s Collection Wales, Science Museum, University of Manchester Image Collection, Tate Gallery, BFI Archive Collections, Nature…"><y>#</y><d>2020-10-13</d><h>09:02</h><w>rickmoynihan</w>Ok asked another friend who works for a client of ours who used to work at the BBC on their linked data platforms &gt; 4 years ago…

This is what he said about heritage orgs that he knows of who ha(d|ve) linked data projects in the UK at least:

&gt; BBC themselves, The National Archives, British Museum, National Library Wales, National Library Scotland, Rijksmuseum, Getty (thesauri for artists, geography and others), Wellcome, Archaeology Data Service, People’s Collection Wales, Science Museum, University of Manchester Image Collection, Tate Gallery, BFI Archive Collections, Nature…</z><z id="t1602579892" t="rickmoynihan BBC was a big one obviously… their news publishing and editorial processes uses linked data so journalists can cross reference articles and topics/articles when writing them, and also IIRC the olympics, and I think football coverage was/is done on linked data… though I could be wrong about the footy."><y>#</y><d>2020-10-13</d><h>09:04</h><w>rickmoynihan</w>BBC was a big one obviously… their news publishing and editorial processes uses linked data so journalists can cross reference articles and topics/articles when writing them, and also IIRC the olympics, and I think football coverage was/is done on linked data… though I could be wrong about the footy.</z><z id="t1602585657" t="Steven Deobald [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] This is a great list! Thanks so much for taking the time to compile this. Before it&apos;s lost to the sands of Slack-is-the-worst-service-possible-for-something-like-Clojurians ( :face_with_rolling_eyes: ) do you know if this channel is logged somewhere?"><y>#</y><d>2020-10-13</d><h>10:40</h><w>Steven Deobald</w><a>@rickmoynihan</a> This is a great list! Thanks so much for taking the time to compile this. Before it&apos;s lost to the sands of Slack-is-the-worst-service-possible-for-something-like-Clojurians (<b>:face_with_rolling_eyes:</b>) do you know if this channel is logged somewhere?</z><z id="t1602585826" t="rickmoynihan It’s logged here: https://clojurians-log.clojureverse.org/rdf"><y>#</y><d>2020-10-13</d><h>10:43</h><w>rickmoynihan</w>It’s logged here: <a href="https://clojurians-log.clojureverse.org/rdf" target="_blank">https://clojurians-log.clojureverse.org/rdf</a></z><z id="t1602614091" t="quoll [:attrs {:href &quot;/_/_/users/U4P4NREBY&quot;}] Sorry I was not online yesterday. I’ve only just seen your comments now. In general, I like [:attrs {:href &quot;/_/_/users/U067TE75L&quot;}] ’s responses. &quot;foo&quot; and &quot;foo&quot;@en are different literals. In fact, for RDF 1.0, there were 3 distinct types of string: • &quot;foo&quot; was a Simple Literal • &quot;foo&quot;^^&lt;xml:string&gt; was a Typed Literal • &quot;foo&quot;@en was a simple literal with a Language Tag All 3 were distinct, and I can’t tell you the grief this caused. It was a relief when RDF 1.1 was introduced and gave all simple literals (that didn’t have a language tag) a datatype of xml:string . Those with a tag are now rdf:langString"><y>#</y><d>2020-10-13</d><h>18:34</h><w>quoll</w><a>@simongray</a> Sorry I was not online yesterday. I’ve only just seen your comments now.
In general, I like <a>@samir</a>’s responses.
<code>&quot;foo&quot;</code> and <code>&quot;foo&quot;@en</code> are different literals. In fact, for RDF 1.0, there were 3 distinct types of string:
• <code>&quot;foo&quot;</code> was a Simple Literal
• <code>&quot;foo&quot;^^&lt;xml:string&gt;</code> was a Typed Literal
• <code>&quot;foo&quot;@en</code> was a simple literal with a Language Tag
All 3 were distinct, and I can’t tell you the grief this caused. It was a relief when RDF 1.1 was introduced and gave all simple literals (that didn’t have a language tag) a datatype of <code>xml:string</code>. Those with a tag are now <code>rdf:langString</code></z><z id="t1602614186" t="quoll In terms of SPARQL stores, there are requirements on correctness, but performance may be terrible. In general, Jena worked hard for correctness, but typically did so with naïve code. Over time, this was reimplemented for better performance."><y>#</y><d>2020-10-13</d><h>18:36</h><w>quoll</w>In terms of SPARQL stores, there are requirements on correctness, but performance may be terrible. In general, Jena worked hard for correctness, but typically did so with naïve code. Over time, this was reimplemented for better performance.</z><z id="t1602614374" t="quoll Generally, most stores get indexed around triples, and not strings. The store I worked on (Originally called Tucana, then Kowari, and finally Mulgara) had the option to use a Lucene index on strings, and extended the query language to allow for Lucene lookups. But SPARQL was intentionally technology agnostic, so how you might implement string indexing is not considered."><y>#</y><d>2020-10-13</d><h>18:39</h><w>quoll</w>Generally, most stores get indexed around triples, and not strings. The store I worked on (Originally called Tucana, then Kowari, and finally Mulgara) had the option to use a Lucene index on strings, and extended the query language to allow for Lucene lookups. But SPARQL was intentionally technology agnostic, so how you might implement string indexing is not considered.</z><z id="t1602614510" t="quoll For instance, a Patricia index may be used for all strings, and then any queries that include a regex could convert that operation into an index lookup. However, I’m not aware of anyone who did that (we started on Tucana, but lost funding). Consequently, I think that most regex queries are managed exclusively as filters… and that will never scale."><y>#</y><d>2020-10-13</d><h>18:41</h><w>quoll</w>For instance, a Patricia index may be used for all strings, and then any queries that include a <code>regex</code> could convert that operation into an index lookup. However, I’m not aware of anyone who did that (we started on Tucana, but lost funding). Consequently, I think that most regex queries are managed exclusively as filters… and that will never scale.</z><z id="t1602614774" t="quoll As for languages… the idea of tagging is to provide semantics for a group of letters. The simple literal &quot;chat&quot; is just a sequence of 4 unicode characters. However, &quot;chat&quot;@en has a semantic that means a conversation, and &quot;chat&quot;@fr has a semantic that means a male cat. These semantics were considered important to capture"><y>#</y><d>2020-10-13</d><h>18:46</h><w>quoll</w>As for languages… the idea of tagging is to provide semantics for a group of letters. The simple literal <code>&quot;chat&quot;</code> is just a sequence of 4 unicode characters. However, <code>&quot;chat&quot;@en</code> has a semantic that means a conversation, and <code>&quot;chat&quot;@fr</code> has a semantic that means a male cat. These semantics were considered important to capture</z><z id="t1602624701" t="rickmoynihan this is a great example"><y>#</y><d>2020-10-13</d><h>21:31</h><r>rickmoynihan</r>this is a great example</z><z id="t1602682738" t="simongray I still find it weird and not every ergonomic that in a system where knowledge is otherwise defined using named relations, for some reason this particular information has to be hardcoded into strings. 😛 but thank you for the in-depth history lesson."><y>#</y><d>2020-10-14</d><h>13:38</h><r>simongray</r>I still find it weird and not every ergonomic that in a system where knowledge is otherwise defined using named relations, for some reason this particular information has to be hardcoded into strings. <b>😛</b> but thank you for the in-depth history lesson.</z><z id="t1602683200" t="simongray [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] btw Paula, if I may ask, what is the end goal of Asami? the readme says it is inspired by RDF, but it doesn’t really mention RDF otehrwise. If I wanted to use it as a triplestore for an existing dataset I guess I would have to develop code for importing RDF files and other necessary functionality?"><y>#</y><d>2020-10-14</d><h>13:46</h><r>simongray</r><a>@U051N6TTC</a> btw Paula, if I may ask, what is the end goal of Asami? the readme says it is inspired by RDF, but it doesn’t really mention RDF otehrwise. If I wanted to use it as a triplestore for an existing dataset I guess I would have to develop code for importing RDF files and other necessary functionality?</z><z id="t1602683248" t="quoll That’s right, you would. Though I have an old project that would get you some of the way there"><y>#</y><d>2020-10-14</d><h>13:47</h><r>quoll</r>That’s right, you would. Though I have an old project that would get you some of the way there</z><z id="t1602683335" t="quoll Ummm… the end goal. I only have vague notions right now. I can tell you why I started and where it’s going 🙂"><y>#</y><d>2020-10-14</d><h>13:48</h><r>quoll</r>Ummm… the end goal. I only have vague notions right now. I can tell you why I started and where it’s going <b>🙂</b></z><z id="t1602683346" t="simongray Please do 🙂"><y>#</y><d>2020-10-14</d><h>13:49</h><r>simongray</r>Please do <b>🙂</b></z><z id="t1602683414" t="quoll It was written for Naga. Naga was designed to be an agnostic rule engine for graph databases. Implement a protocol for a graph database, and Naga could execute rules for it"><y>#</y><d>2020-10-14</d><h>13:50</h><r>quoll</r>It was written for Naga.
Naga was designed to be an agnostic rule engine for graph databases. Implement a protocol for a graph database, and Naga could execute rules for it</z><z id="t1602683468" t="quoll I thought I would start with Datomic, then implement something for SPARQL, OrientDB… etc"><y>#</y><d>2020-10-14</d><h>13:51</h><r>quoll</r>I thought I would start with Datomic, then implement something for SPARQL, OrientDB… etc</z><z id="t1602683507" t="quoll But I made the mistake of showing my manager, and he got excited, and asked me to develop it for work instead of evenings and weekends. I agreed, so long as it stayed open source, which he was good with"><y>#</y><d>2020-10-14</d><h>13:51</h><r>quoll</r>But I made the mistake of showing my manager, and he got excited, and asked me to develop it for work instead of evenings and weekends. I agreed, so long as it stayed open source, which he was good with</z><z id="t1602683607" t="quoll But then he said that he wanted it to all be open source, and he wasn’t keen on Datomic for that reason. So could I write a simple database to handle it? Sure. I had only stopped working on Mulgara because I don’t like Java, so restarting with Clojure sounded like a good idea (second systems effect be damned!) 🙂"><y>#</y><d>2020-10-14</d><h>13:53</h><r>quoll</r>But then he said that he wanted it to all be open source, and he wasn’t keen on Datomic for that reason. So could I write a simple database to handle it? Sure. I had only stopped working on Mulgara because I don’t like Java, so restarting with Clojure sounded like a good idea (second systems effect be damned!) <b>🙂</b></z><z id="t1602683664" t="quoll Initially, Asami only did 3 things: • indexed data • inner joins • query optimizing"><y>#</y><d>2020-10-14</d><h>13:54</h><r>quoll</r>Initially, Asami only did 3 things:
• indexed data
• inner joins
• query optimizing</z><z id="t1602683686" t="simongray hah, ok, so it’s mainly because your manager dislikes closed source software? That is a fantastic 1st world problem to have."><y>#</y><d>2020-10-14</d><h>13:54</h><r>simongray</r>hah, ok, so it’s mainly because your manager dislikes closed source software? That is a fantastic 1st world problem to have.</z><z id="t1602683703" t="quoll yup"><y>#</y><d>2020-10-14</d><h>13:55</h><r>quoll</r>yup</z><z id="t1602683715" t="quoll But I did it in about a week, so it wasn’t a big deal"><y>#</y><d>2020-10-14</d><h>13:55</h><r>quoll</r>But I did it in about a week, so it wasn’t a big deal</z><z id="t1602683720" t="simongray nice"><y>#</y><d>2020-10-14</d><h>13:55</h><r>simongray</r>nice</z><z id="t1602683736" t="quoll The majority of that was the query planner"><y>#</y><d>2020-10-14</d><h>13:55</h><r>quoll</r>The majority of that was the query planner</z><z id="t1602683781" t="quoll you could argue that it wasn’t needed (Datomic doesn’t have one), but: a) I’d done it before b) rules could potentially create queries that were in suboptimal form. I’ve been bitten by this in the past"><y>#</y><d>2020-10-14</d><h>13:56</h><r>quoll</r>you could argue that it wasn’t needed (Datomic doesn’t have one), but:
a) I’d done it before
b) rules could potentially create queries that were in suboptimal form. I’ve been bitten by this in the past</z><z id="t1602683804" t="quoll Some time later, he called me and asked me to port it to ClojureScript. So it moved into the browser"><y>#</y><d>2020-10-14</d><h>13:56</h><r>quoll</r>Some time later, he called me and asked me to port it to ClojureScript. So it moved into the browser</z><z id="t1602683827" t="quoll Since then, I’ve been getting more requests for more features. Right now it handles a LOT"><y>#</y><d>2020-10-14</d><h>13:57</h><r>quoll</r>Since then, I’ve been getting more requests for more features. Right now it handles a LOT</z><z id="t1602683851" t="quoll That’s when I started a new pet project (evenings and weekends)"><y>#</y><d>2020-10-14</d><h>13:57</h><r>quoll</r>That’s when I started a new pet project (evenings and weekends)</z><z id="t1602683884" t="simongray It seems like a lot of work is happening in this space at the moment with Asami, Datalevin, Datahike, Datascript. Kind of exciting."><y>#</y><d>2020-10-14</d><h>13:58</h><r>simongray</r>It seems like a lot of work is happening in this space at the moment with Asami, Datalevin, Datahike, Datascript. Kind of exciting.</z><z id="t1602683897" t="quoll This is for backend storage. It is loosely based on Mulgara, but with a lot of innovations, and new emphasis"><y>#</y><d>2020-10-14</d><h>13:58</h><r>quoll</r>This is for backend storage. It is loosely based on Mulgara, but with a lot of innovations, and new emphasis</z><z id="t1602683928" t="quoll Honestly, if I’d known about Datascript (which had started), then I would have just used that"><y>#</y><d>2020-10-14</d><h>13:58</h><r>quoll</r>Honestly, if I’d known about Datascript (which had started), then I would have just used that</z><z id="t1602683964" t="quoll Anyway… I mentioned the backend storage, and several managers all got excited about it. So THAT is now my job"><y>#</y><d>2020-10-14</d><h>13:59</h><r>quoll</r>Anyway… I mentioned the backend storage, and several managers all got excited about it. So THAT is now my job</z><z id="t1602683974" t="simongray HAHA"><y>#</y><d>2020-10-14</d><h>13:59</h><r>simongray</r>HAHA</z><z id="t1602683984" t="quoll And for the first time, they’ve given me someone else to help"><y>#</y><d>2020-10-14</d><h>13:59</h><r>quoll</r>And for the first time, they’ve given me someone else to help</z><z id="t1602684029" t="quoll He’s doing the ClojureScript implementation (over IndexedDB)"><y>#</y><d>2020-10-14</d><h>14:00</h><r>quoll</r>He’s doing the ClojureScript implementation (over IndexedDB)</z><z id="t1602684076" t="quoll I’m doing the same thing on memory-mapped files. But it’s behind a set of protocols which makes it all look the same to the index code"><y>#</y><d>2020-10-14</d><h>14:01</h><r>quoll</r>I’m doing the same thing on memory-mapped files. But it’s behind a set of protocols which makes it all look the same to the index code</z><z id="t1602684132" t="quoll I also hope to include other options, like S3 buckets. These will work, because everything is immutable (durable, persistent, full history, etc)"><y>#</y><d>2020-10-14</d><h>14:02</h><r>quoll</r>I also hope to include other options, like S3 buckets. These will work, because everything is immutable (durable, persistent, full history, etc)</z><z id="t1602684154" t="simongray Do you see a future where a common protocol like ring can be developed for all of these Datomic-like databases? So much work is happening in parallel."><y>#</y><d>2020-10-14</d><h>14:02</h><r>simongray</r>Do you see a future where a common protocol like ring can be developed for all of these Datomic-like databases? So much work is happening in parallel.</z><z id="t1602684176" t="quoll That was actually exactly the perspective that Naga has!"><y>#</y><d>2020-10-14</d><h>14:02</h><r>quoll</r>That was actually exactly the perspective that Naga has!</z><z id="t1602684254" t="quoll The protocol that Naga asks Databases to implement is oriented specifically to Naga’s needs, but it works pretty well"><y>#</y><d>2020-10-14</d><h>14:04</h><r>quoll</r>The protocol that Naga asks Databases to implement is oriented specifically to Naga’s needs, but it works pretty well</z><z id="t1602684271" t="simongray I see. So perhaps it’s just a question of willingness to integrate."><y>#</y><d>2020-10-14</d><h>14:04</h><r>simongray</r>I see. So perhaps it’s just a question of willingness to integrate.</z><z id="t1602684371" t="quoll Well, the way I’ve done it in Naga has been as a set of package directories which implement the protocol for each database. Unfortunately, I’ve been busy, so I only have directories for Asami and Datomic"><y>#</y><d>2020-10-14</d><h>14:06</h><r>quoll</r>Well, the way I’ve done it in Naga has been as a set of package directories which implement the protocol for each database.
Unfortunately, I’ve been busy, so I only have directories for Asami and Datomic</z><z id="t1602684378" t="quoll But they both work 🙂"><y>#</y><d>2020-10-14</d><h>14:06</h><r>quoll</r>But they both work <b>🙂</b></z><z id="t1602684387" t="quoll I imagine that it wouldn’t be hard to do Datascript"><y>#</y><d>2020-10-14</d><h>14:06</h><r>quoll</r>I imagine that it wouldn’t be hard to do Datascript</z><z id="t1602684429" t="quoll The main thing that Datascript/Datomic miss is a query API that allows you to do an INSERT/SELECT (which SPARQL has)"><y>#</y><d>2020-10-14</d><h>14:07</h><r>quoll</r>The main thing that Datascript/Datomic miss is a query API that allows you to do an INSERT/SELECT (which SPARQL has)</z><z id="t1602684712" t="simongray I need to get some real work done before heading “home” for today, i.e. moving from the desk to the sofa. Thanks for an interesting conversation. I’m keeping an eye on Asami (and now naga). Really interesting projects."><y>#</y><d>2020-10-14</d><h>14:11</h><r>simongray</r>I need to get some real work done before heading “home” for today, i.e. moving from the desk to the sofa. Thanks for an interesting conversation. I’m keeping an eye on Asami (and now naga). Really interesting projects.</z><z id="t1602684762" t="quoll Thank you"><y>#</y><d>2020-10-14</d><h>14:12</h><r>quoll</r>Thank you</z><z id="t1602684793" t="quoll They look quiet right now because I’m working on the storage branch "><y>#</y><d>2020-10-14</d><h>14:13</h><r>quoll</r>They look quiet right now because I’m working on the storage branch </z><z id="t1602687426" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] : Sounds like you’ve both had a very interesting career, and currently have a dream job. Most managers would never entertain the need to implement a new database; though it sounds like you’ve done it many times. :thumbsup: [:attrs {:href &quot;/_/_/users/UB3R8UYA1&quot;}] spoke here a while back about doing something that sounded similar; providing some common abstraction across RDF and other graph stores / libraries. I definitely see the appeal; but I don’t really understand the real world use case. Why is it necessary for your business? Swapping out an RDF database for a different RDF one can be enough work as it is (due to radically different performance profiles), let alone moving across ecosystems. Or am I misunderstanding the purpose of the abstraction; is it to make more backends look like graphs? Which is a use case I totally get 👌 . Regardless I’d love to hear more about your work"><y>#</y><d>2020-10-14</d><h>14:57</h><r>rickmoynihan</r><a>@U051N6TTC</a>: Sounds like you’ve both had a very interesting career, and currently have a dream job.  Most managers would never entertain the need to implement a new database; though it sounds like you’ve done it many times. <b>:thumbsup:</b>

<a>@UB3R8UYA1</a> spoke here a while back about doing something that sounded similar; providing some common abstraction across RDF and other graph stores / libraries.  I definitely see the appeal; but I don’t really understand the real world use case.  Why is it necessary for your business?  Swapping out an RDF database for a different RDF one can be enough work as it is (due to radically different performance profiles), let alone moving across ecosystems.

Or am I misunderstanding the purpose of the abstraction; is it to make more backends look like graphs?  Which is a use case I totally get <b>👌</b>.

Regardless I’d love to hear more about your work</z><z id="t1602687461" t="quoll only twice: Mulgara and now Asami"><y>#</y><d>2020-10-14</d><h>14:57</h><r>quoll</r>only twice: Mulgara and now Asami</z><z id="t1602687495" t="quoll At work, there is no impetus to be able to swap things out 🙂"><y>#</y><d>2020-10-14</d><h>14:58</h><r>quoll</r>At work, there is no impetus to be able to swap things out <b>🙂</b></z><z id="t1602687527" t="quoll but any libraries that use a graph database have motivation to do it"><y>#</y><d>2020-10-14</d><h>14:58</h><r>quoll</r>but any libraries that use a graph database have motivation to do it</z><z id="t1602687549" t="quoll particularly if the library is supposed to have broader appeal than for just the team developing it"><y>#</y><d>2020-10-14</d><h>14:59</h><r>quoll</r>particularly if the library is supposed to have broader appeal than for just the team developing it</z><z id="t1602687591" t="quoll For instance… there is no need for Asami to have a SPARQL front end, but it’s a ticket, because I’d like to make it more accessible to people"><y>#</y><d>2020-10-14</d><h>14:59</h><r>quoll</r>For instance… there is no need for Asami to have a SPARQL front end, but it’s a ticket, because I’d like to make it more accessible to people</z><z id="t1602687616" t="rickmoynihan yeah ok that’s fair"><y>#</y><d>2020-10-14</d><h>15:00</h><r>rickmoynihan</r>yeah ok that’s fair</z><z id="t1602687623" t="quoll Besides, if [:attrs nil] don’t implement a SPARQL front end, it will be embarrassing!!!"><y>#</y><d>2020-10-14</d><h>15:00</h><r>quoll</r>Besides, if <b>I</b> don’t implement a SPARQL front end, it will be embarrassing!!!</z><z id="t1602687637" t="rickmoynihan lol"><y>#</y><d>2020-10-14</d><h>15:00</h><r>rickmoynihan</r>lol</z><z id="t1602687650" t="quoll For anyone reading… I was on the SPARQL committee"><y>#</y><d>2020-10-14</d><h>15:00</h><r>quoll</r>For anyone reading… I was on the SPARQL committee</z><z id="t1602687656" t="rickmoynihan I don’t know how you could live with yourself… 😆"><y>#</y><d>2020-10-14</d><h>15:00</h><r>rickmoynihan</r>I don’t know how you could live with yourself… <b>😆</b></z><z id="t1602687672" t="quoll exactly!"><y>#</y><d>2020-10-14</d><h>15:01</h><r>quoll</r>exactly!</z><z id="t1602687683" t="rickmoynihan ahh well in that case… I don’t know how you could live with yourself 😁"><y>#</y><d>2020-10-14</d><h>15:01</h><r>rickmoynihan</r>ahh well in that case… I don’t know how you could live with yourself <b>😁</b></z><z id="t1602687892" t="rickmoynihan If you don’t mind me asking, if you could re-live being on that committee, knowing what you do now, what would you do differently?"><y>#</y><d>2020-10-14</d><h>15:04</h><r>rickmoynihan</r>If you don’t mind me asking, if you could re-live being on that committee, knowing what you do now, what would you do differently?</z><z id="t1602691941" t="quoll Well, it was a learning experience for me. A number of interests were on the committee to push the standard in a direction that most suited their existing systems. So rather than introducing technical changes, or working against specific things, I would have focused more on communication with each member of the committee. Not that I think I did a terrible job, but I could have done better"><y>#</y><d>2020-10-14</d><h>16:12</h><r>quoll</r>Well, it was a learning experience for me. A number of interests were on the committee to push the standard in a direction that most suited their existing systems. So rather than introducing technical changes, or working against specific things, I would have focused more on communication with each member of the committee. Not that I think I did a terrible job, but I could have done better</z><z id="t1602692381" t="quoll From a technical perspective, I would have liked to see a tighter definition around aggregates, with algorithmic description."><y>#</y><d>2020-10-14</d><h>16:19</h><r>quoll</r>From a technical perspective, I would have liked to see a tighter definition around aggregates, with algorithmic description.</z><z id="t1602692448" t="quoll But that’s just because I find a bit of flexibility in some of the edge cases there. Also, having a default way to handle things, even if they’re not the ideal optimized approach, would have been nice to have"><y>#</y><d>2020-10-14</d><h>16:20</h><r>quoll</r>But that’s just because I find a bit of flexibility in some of the edge cases there. Also, having a default way to handle things, even if they’re not the ideal optimized approach, would have been nice to have</z><z id="t1602692478" t="quoll That said, that’s essentially what Jena sets out to do. They try to be the reference implementation, and they most certainly don’t take the optimized approach"><y>#</y><d>2020-10-14</d><h>16:21</h><r>quoll</r>That said, that’s essentially what Jena sets out to do. They try to be the reference implementation, and they most certainly don’t take the optimized approach</z><z id="t1602692539" t="quoll The early versions of Jena saved triples as a flat list, and resolved patterns as filters against them 😖"><y>#</y><d>2020-10-14</d><h>16:22</h><r>quoll</r>The early versions of Jena saved triples as a flat list, and resolved patterns as filters against them <b>😖</b></z><z id="t1602692578" t="quoll Andy had some long conversations with me about Mulgara’s storage while he was planning out Fuseki"><y>#</y><d>2020-10-14</d><h>16:22</h><r>quoll</r>Andy had some long conversations with me about Mulgara’s storage while he was planning out Fuseki</z><z id="t1602692791" t="quoll Also [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] : &gt; Sounds like you’ve both had a very interesting career, and currently have a dream job Yes! I have certainly been spoiled! I honestly don’t know how I have managed to keep coming back to these things, but I’m happy that I have. Of course, I’ve done other things in the between, but even those can be informative (for instance, I’ve had opportunities to work with both Datomic and OrientDB)"><y>#</y><d>2020-10-14</d><h>16:26</h><r>quoll</r>Also <a>@rickmoynihan</a>:
&gt; Sounds like you’ve both had a very interesting career, and currently have a dream job
Yes! I have certainly been spoiled! I honestly don’t know how I have managed to keep coming back to these things, but I’m happy that I have. Of course, I’ve done other things in the between, but even those can be informative (for instance, I’ve had opportunities to work with both Datomic and OrientDB)</z><z id="t1602692818" t="quoll Oh! I just thought of something I could have mentioned in the SPARQL committee that continues to frustrate me… transactions!"><y>#</y><d>2020-10-14</d><h>16:26</h><r>quoll</r>Oh! I just thought of something I could have mentioned in the SPARQL committee that continues to frustrate me… transactions!</z><z id="t1602692910" t="quoll It’s possible to send several operations through at once. e.g. An insert; an insert/select; a delete. But there are limits on what you can manage there. There are occasions where transactions are important."><y>#</y><d>2020-10-14</d><h>16:28</h><r>quoll</r>It’s possible to send several operations through at once. e.g. An insert; an insert/select; a delete. But there are limits on what you can manage there. There are occasions where transactions are important.</z><z id="t1602693022" t="quoll Datomic is frustrating that way too, because Naga needs it. (I manage it by using a with database, and once I’m done, I replay the accumulated transactions with transact )"><y>#</y><d>2020-10-14</d><h>16:30</h><r>quoll</r>Datomic is frustrating that way too, because Naga needs it. (I manage it by using a <code>with</code> database, and once I’m done, I replay the accumulated transactions with <code>transact</code>)</z><z id="t1602759926" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] : fascinating, I agree it would have been nice to have a standard for transactions."><y>#</y><d>2020-10-15</d><h>11:05</h><r>rickmoynihan</r><a>@U051N6TTC</a>: fascinating, I agree it would have been nice to have a standard for transactions.</z><z id="t1602614871" t="quoll Especially when the original intent of RDF was to provide semantic linkages (hence the name, “Semantic Web”)"><y>#</y><d>2020-10-13</d><h>18:47</h><w>quoll</w>Especially when the original intent of RDF was to provide semantic linkages (hence the name, “Semantic Web”)</z><z id="t1602615293" t="quoll Also, on some specific questions: &gt; implemented languages as equality-distorting aspects of strings literals Languages change the value. You can consider that as “equality-distorting”, but it can be avoided. For instance… &gt; If I am to query for my own name in an RSF resource how should I refer to it? “Simon”@en, “Simon”@da, and 6000 other entries? Your query could include: WHERE { ?me foaf:name ?name . FILTER(str(?name) = &quot;Simon&quot;) }"><y>#</y><d>2020-10-13</d><h>18:54</h><w>quoll</w>Also, on some specific questions:
&gt; implemented languages as equality-distorting aspects of strings literals
Languages change the value. You can consider that as “equality-distorting”, but it can be avoided. For instance…
&gt; If I am to query for my own name in an RSF resource how should I refer to it? “Simon”@en, “Simon”@da, and 6000 other entries?
Your query could include:
<code>WHERE { ?me foaf:name ?name . FILTER(str(?name) = &quot;Simon&quot;) }</code></z><z id="t1602615357" t="quoll A good implementation (and I’m not saying that your SPARQL store will be) could turn that FILTER operation into an index lookup"><y>#</y><d>2020-10-13</d><h>18:55</h><w>quoll</w>A good implementation (and I’m not saying that your SPARQL store will be) could turn that <code>FILTER</code> operation into an index lookup</z><z id="t1602615403" t="quoll Jena never used to do that, but they may have updated lately. This might be an excuse for me to check in and see how Andy is doing 🙂"><y>#</y><d>2020-10-13</d><h>18:56</h><w>quoll</w>Jena never used to do that, but they may have updated lately. This might be an excuse for me to check in and see how Andy is doing <b>🙂</b></z><z id="t1602616913" t="samir To push the argument further, the concept of equality is quite complex as you can see in https://clojure.org/guides/equality . RDF makes no special treatment regarding equality. AFAIK two terms are equal when they have the identical long notation. SPARQL being a query language makes some decisions regarding equality in some functions. To me it feels like a good compromise as the goal of the semantic web is to enable the articulation of arbitrary knowledge and data domains "><y>#</y><d>2020-10-13</d><h>19:21</h><w>samir</w>To push the argument further, the concept of equality is quite complex as you can see in <a href="https://clojure.org/guides/equality" target="_blank">https://clojure.org/guides/equality</a> . RDF makes no special treatment regarding equality. AFAIK two terms are equal when they have the identical long notation. SPARQL being a query language makes some decisions regarding equality in some functions. To me it feels like a good compromise as the goal of the semantic web is to enable the articulation of arbitrary knowledge and data domains </z><z id="t1602624711" t="rickmoynihan Yeah it’s the lexical form that strictly speaking should be used, in combination with the datatype uri, lang string etc Though some stores will do some implicit coercions, e.g. stardog will by default canonicalise various numeric types e.g. xsd:byte s into xsd:integer unless you switch that off. https://www.w3.org/TR/rdf-concepts/#section-Literal-Equality"><y>#</y><d>2020-10-13</d><h>21:31</h><w>rickmoynihan</w>Yeah it’s the lexical form that strictly speaking should be used, in combination with the datatype uri, lang string etc

Though some stores will do some implicit coercions, e.g. stardog will by default canonicalise various numeric types e.g. <code>xsd:byte</code>s into <code>xsd:integer</code> unless you switch that off.

<a href="https://www.w3.org/TR/rdf-concepts/#section-Literal-Equality" target="_blank">https://www.w3.org/TR/rdf-concepts/#section-Literal-Equality</a></z><z id="t1602659530" t="Steven Deobald Reading this thread has left me with another naive, open-ended question: Are any of you familiar with how linguistics systems deal with the raw components of language? The best tool available for Pali (at the moment) is the DPR: https://www.digitalpalireader.online ...and although it&apos;s incredibly detailed, it just sort of brute-forces word compounds against a massive dictionary. That dictionary includes components used in compounds, but the component relationships and relationships to Sanskrit and Latin are just hard-coded within word definitions, as far as I can tell. Wiktionary has some very basic understanding of word components: https://en.wiktionary.org/wiki/-%E0%A4%A6%E0%A4%BE%E0%A4%B0 https://en.wiktionary.org/wiki/Category:Hindi_words_suffixed_with_-%E0%A4%A6%E0%A4%BE%E0%A4%B0 Their SPARQL endpoint seems to be down (or inaccessible from Kashmir) at the moment: http://wiktionary.dbpedia.org/sparql I don&apos;t think this granularity would ever apply to the data on http://pariyatti.org but it will ultimately be required for Pariyatti&apos;s sister project, https://www.tipitaka.org I see https://linguistics.okfn.org but I&apos;m never sure about the significance of projects like this. Are there others I should be reading about?"><y>#</y><d>2020-10-14</d><h>07:12</h><w>Steven Deobald</w>Reading this thread has left me with another naive, open-ended question: Are any of you familiar with how linguistics systems deal with the raw components of language? The best tool available for Pali (at the moment) is the DPR: <a href="https://www.digitalpalireader.online" target="_blank">https://www.digitalpalireader.online</a> ...and although it&apos;s incredibly detailed, it just sort of brute-forces word compounds against a massive dictionary. That dictionary includes components used in compounds, but the component relationships and relationships to Sanskrit and Latin are just hard-coded within word definitions, as far as I can tell.

Wiktionary has some very basic understanding of word components:
<a href="https://en.wiktionary.org/wiki/-%E0%A4%A6%E0%A4%BE%E0%A4%B0" target="_blank">https://en.wiktionary.org/wiki/-%E0%A4%A6%E0%A4%BE%E0%A4%B0</a>
<a href="https://en.wiktionary.org/wiki/Category:Hindi_words_suffixed_with_-%E0%A4%A6%E0%A4%BE%E0%A4%B0" target="_blank">https://en.wiktionary.org/wiki/Category:Hindi_words_suffixed_with_-%E0%A4%A6%E0%A4%BE%E0%A4%B0</a>

Their SPARQL endpoint seems to be down (or inaccessible from Kashmir) at the moment: <a href="http://wiktionary.dbpedia.org/sparql" target="_blank">http://wiktionary.dbpedia.org/sparql</a>

I don&apos;t think this granularity would ever apply to the data on <a href="http://pariyatti.org" target="_blank">http://pariyatti.org</a> but it will ultimately be required for Pariyatti&apos;s sister project, <a href="https://www.tipitaka.org" target="_blank">https://www.tipitaka.org</a>

I see <a href="https://linguistics.okfn.org" target="_blank">https://linguistics.okfn.org</a> but I&apos;m never sure about the significance of projects like this. Are there others I should be reading about?</z><z id="t1602662132" t="rickmoynihan Linguistics isn’t really my area of expertise, so please take my comments with a pinch of salt. The biggest model I know of in the knowledge representation (KR) of linguistics is wordnet. It’s a long standing project to essentially provide a machine readable thesaurus of natural language terms, and give some idea of their proximity to each other etc. My understanding is that it’s a great dataset, with bindings into many ecosystems and it’s still widely used, especially to add some knowledge of synonyms etc to search engines etc. Being KR these days it’s probably considered old hat, with ML language models taking centre stage; however I think there’s a lot of progress in hybrid approaches that combine KR and ML; so I don’t think it will go anywhere anytime soon. Like you I’d be somewhat sceptical of the long term viability and maintenance of the OKFN stuff. They have their fingers in a lot of pies, and I suspect like many people are forced to chase income streams. That’s not to say that they don’t do good work; they absolutely do."><y>#</y><d>2020-10-14</d><h>07:55</h><w>rickmoynihan</w>Linguistics isn’t really my area of expertise, so please take my comments with a pinch of salt.

The biggest model I know of in the knowledge representation (KR) of linguistics is wordnet.  It’s a long standing project to essentially provide a machine readable thesaurus of natural language terms, and give some idea of their proximity to each other etc.  My understanding is that it’s a great dataset, with bindings into many ecosystems and it’s still widely used, especially to add some knowledge of synonyms etc to search engines etc.  Being KR these days it’s probably considered old hat, with ML language models taking centre stage; however I think there’s a lot of progress in hybrid approaches that combine KR and ML; so I don’t think it will go anywhere anytime soon.

Like you I’d be somewhat sceptical of the long term viability and maintenance of the OKFN stuff.  They have their fingers in a lot of pies, and I suspect like many people are forced to chase income streams.  That’s not to say that they don’t do good work; they absolutely do.</z><z id="t1602662154" t="rickmoynihan https://wordnet.princeton.edu/"><y>#</y><d>2020-10-14</d><h>07:55</h><w>rickmoynihan</w><a href="https://wordnet.princeton.edu/" target="_blank">https://wordnet.princeton.edu/</a></z><z id="t1602672682" t="Steven Deobald Oh yeah, wordnet... I remember that."><y>#</y><d>2020-10-14</d><h>10:51</h><w>Steven Deobald</w>Oh yeah, wordnet... I remember that.</z><z id="t1603183341" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] : I’ve been taking a quick look at asami, and I think if I’d known of this 2 years ago I perhaps wouldn’t have written matcha (which really just started as a quick but very useful hack). What I like (for my needs) is that it’s schemaless/open-world, which means like RDF I can add the schema either before or after as facts in the database. I’m currently looking for something to spike out a toy project that will let me implement an RDF-lite in clojure, but with RDFS like inferencing on domains/ranges subproperties/classes etc. My understanding is that I could plug naga in to asami, and add the rdfs-like rules as datalog horn clauses? I can see that the rdfs rules would be trivial to express this way. I’m guessing these rules aren’t stored in the database itself as facts, but sit alongside it? My main questions are more about asami itself… I skimmed through the excellent docs but couldn’t see a discussion of these points, I’m sure I can easily do some more digging to find out though… 1. What is the main difference between :db/id and :db/ident ? I’m guessing :db/id is a user assigned id, and :db/ident is a database assigned one? Does every statement get assigned a :db/ident regardless of being given a :db/id or is :db/id used as a :db/ident ? 2. I’m assuming the boolean at the end of every stored statement represents addition/retraction? 3. Is it possible to transact triples into asami, i.e. to represent the data as 3/tuple vectors?"><y>#</y><d>2020-10-20</d><h>08:42</h><w>rickmoynihan</w><a>@quoll</a>: I’ve been taking a quick look at asami, and I think if I’d known of this 2 years ago I perhaps wouldn’t have written matcha (which really just started as a quick but very useful hack).

What I like (for my needs) is that it’s schemaless/open-world, which means like RDF I can add the schema either before or after as facts in the database.

I’m currently looking for something to spike out a toy project that will let me implement an RDF-lite in clojure, but with RDFS like inferencing on domains/ranges subproperties/classes etc.

My understanding is that I could plug naga in to asami, and add the rdfs-like rules as datalog horn clauses?  I can see that the rdfs rules would be trivial to express this way.  I’m guessing these rules aren’t stored in the database itself as facts, but sit alongside it?

My main questions are more about asami itself…  I skimmed through the excellent docs but couldn’t see a discussion of these points, I’m sure I can easily do some more digging to find out though…

1. What is the main difference between <code>:db/id</code> and <code>:db/ident</code>?  I’m guessing <code>:db/id</code> is a user assigned id, and <code>:db/ident</code> is a database assigned one?  Does every statement get assigned a <code>:db/ident</code> regardless of being given a <code>:db/id</code> or is <code>:db/id</code> used as a <code>:db/ident</code>?
2. I’m assuming the boolean at the end of every stored statement represents addition/retraction?
3. Is it possible to transact triples into asami, i.e. to represent the data as 3/tuple vectors?</z><z id="t1603287272" t="quoll If I’d known about Datascript in 2016 then I wouldn’t have started Asami… probably. Actually, Asami was just supposed to be in-memory storage and indexing with counts and inner-joins. So maybe I would have. :woman-shrugging:"><y>#</y><d>2020-10-21</d><h>13:34</h><r>quoll</r>If I’d known about Datascript in 2016 then I wouldn’t have started Asami… probably. Actually, Asami was just supposed to be in-memory storage and indexing with counts and inner-joins. So maybe I would have. <b>:woman-shrugging:</b></z><z id="t1603287363" t="quoll Yes, Naga runs rules that are outside of the database. It’s funny that you ask this, because the first rules engine I ever wrote stored the rules in the graph 🙂"><y>#</y><d>2020-10-21</d><h>13:36</h><r>quoll</r>Yes, Naga runs rules that are outside of the database. It’s funny that you ask this, because the first rules engine I ever wrote stored the rules in the graph <b>🙂</b></z><z id="t1603287477" t="rickmoynihan Yeah, I think I saw you say something like that about datascript in another thread… I could be wrong but datascript seems to require you to specify a schema upfront. In particular property cardinality; which I’d rather leave open like in RDF. It looks like Asami has more of this flavour than datascript/datomic?"><y>#</y><d>2020-10-21</d><h>13:37</h><r>rickmoynihan</r>Yeah, I think I saw you say something like that about datascript in another thread…

I could be wrong but datascript seems to require you to specify a schema upfront.  In particular property cardinality; which I’d rather leave open like in RDF.  It looks like Asami has more of this flavour than datascript/datomic?</z><z id="t1603287484" t="quoll Over time I found that people wanted to edit and add to rules, and having these separate made it easier to manage. Also, they wanted to see rules in a document. It’s possible to parse the document, store it in the graph, then read it back out, and emit it for editing, so there’s no complexity there. But it just didn’t seem worth it"><y>#</y><d>2020-10-21</d><h>13:38</h><r>quoll</r>Over time I found that people wanted to edit and add to rules, and having these separate made it easier to manage. Also, they wanted to see rules in a document. It’s possible to parse the document, store it in the graph, then read it back out, and emit it for editing, so there’s no complexity there. But it just didn’t seem worth it</z><z id="t1603287501" t="quoll Asami is definitely like RDF in this way"><y>#</y><d>2020-10-21</d><h>13:38</h><r>quoll</r>Asami is definitely like RDF in this way</z><z id="t1603287520" t="quoll Actually, the differences between RDF and Asami are minimal…"><y>#</y><d>2020-10-21</d><h>13:38</h><r>quoll</r>Actually, the differences between RDF and Asami are minimal…</z><z id="t1603287567" t="quoll Asami allows ANY data type in any column. RDF requires: • Subject: URI or Blank Node • Predicate: URI • Object: URI, Blank Node, Literal"><y>#</y><d>2020-10-21</d><h>13:39</h><r>quoll</r>Asami allows ANY data type in any column. RDF requires:
• Subject: URI or Blank Node
• Predicate: URI
• Object: URI, Blank Node, Literal</z><z id="t1603287647" t="rickmoynihan &gt; Asami is definitely like RDF in this way Yeah that’s actually what I like about it over datascript… I mean tbh I’d much prefer knowing cardinalities are 1 or many in RDF; so it’s not religous argument for me… I just think Asami would suit my needs better."><y>#</y><d>2020-10-21</d><h>13:40</h><r>rickmoynihan</r>&gt; Asami is definitely like RDF in this way
Yeah that’s actually what I like about it over datascript…

I mean tbh I’d much prefer knowing cardinalities are 1 or many in RDF; so it’s not religous argument for me… I just think Asami would suit my needs better.</z><z id="t1603287658" t="quoll RDF also has specific support for Literals. Language tagged, plain (which are now inferred to be xsd string literals), and XSD types. Asami doesn’t care"><y>#</y><d>2020-10-21</d><h>13:40</h><r>quoll</r>RDF also has specific support for Literals. Language tagged, plain (which are now inferred to be xsd string literals), and XSD types. Asami doesn’t care</z><z id="t1603287672" t="rickmoynihan yeah I did the same thing in matcha (allowing any value in any position)"><y>#</y><d>2020-10-21</d><h>13:41</h><r>rickmoynihan</r>yeah I did the same thing in matcha (allowing any value in any position)</z><z id="t1603287705" t="quoll I apologize… I was going to try to go through each thing that you asked. I hope I don’t get mixed up in the conversation instead 🙂"><y>#</y><d>2020-10-21</d><h>13:41</h><r>quoll</r>I apologize… I was going to try to go through each thing that you asked. I hope I don’t get mixed up in the conversation instead <b>🙂</b></z><z id="t1603287779" t="quoll A lot of RDF was just ensuring that things were the correct types. Moving to Scala helped there a bit. But it was a pain. Since starting Asami I decided that I didn’t care. Sure enough, someone started using strings as predicates to connect things. Asami didn’t care :rolling_on_the_floor_laughing:"><y>#</y><d>2020-10-21</d><h>13:42</h><r>quoll</r>A lot of RDF was just ensuring that things were the correct types. Moving to Scala helped there a bit. But it was a pain. Since starting Asami I decided that I didn’t care.
Sure enough, someone started using strings as predicates to connect things. Asami didn’t care <b>:rolling_on_the_floor_laughing:</b></z><z id="t1603287823" t="quoll About your 3 points:"><y>#</y><d>2020-10-21</d><h>13:43</h><r>quoll</r>About your 3 points:</z><z id="t1603288358" t="quoll 1. :db/id is a pseudo property that actually refers to an entity. If you describe an entity with triples, then it’s not needed. e.g. :node-20 :name &quot;Mary&quot; :node-20 :age 32 But if you describe it as an entity (i.e. a Clojure map), then how do you refer to the entity node? {:name &quot;Mary&quot; :age 32} This will allocate a node value that could be anything (typically you won’t actually care). But you may specifically want to refer to :node-20 because you picked that value out of the graph earlier, or something. That can be done with the pseudo property: {:db/id :node-20 :name &quot;Mary&quot; :age 32} That ensures that the :node-20 keyword will be the subject value for all of those triples. :db/ident is an actual real property. It just has a special status when it comes to selecting entities. So: {:db/ident :mary :name &quot;Mary&quot; :age 32} will lead to triples like this: :node-55 :db/ident :mary :node-55 :name &quot;Mary&quot; :node-55 :age 32 The special status is that it can be used to refer to an entity. You can query for entities by that value, and you can insert other entities that refer to that value, so that you get a link to it. In this way it acts a bit like :db/ident"><y>#</y><d>2020-10-21</d><h>13:52</h><r>quoll</r>1. <code>:db/id</code> is a pseudo property that actually refers to an entity. If you describe an entity with triples, then it’s not needed. e.g.
<pre>:node-20 :name &quot;Mary&quot;
:node-20 :age 32</pre>
But if you describe it as an entity (i.e. a Clojure map), then how do you refer to the entity node?
<pre>{:name &quot;Mary&quot;
 :age 32}</pre>
This will allocate a node value that could be anything (typically you won’t actually care). But you may specifically want to refer to <code>:node-20</code>  because you picked that value out of the graph earlier, or something. That can be done with the pseudo property:
<pre>{:db/id :node-20
 :name &quot;Mary&quot;
 :age 32}</pre>
That ensures that the <code>:node-20</code> keyword will be the subject value for all of those triples.

<code>:db/ident</code> is an actual real property. It just has a special status when it comes to selecting entities. So:
<pre>{:db/ident :mary
 :name &quot;Mary&quot;
 :age 32}</pre>
will lead to triples like this:
<pre>:node-55 :db/ident :mary
:node-55 :name &quot;Mary&quot;
:node-55 :age 32</pre>
The special status is that it can be used to refer to an entity. You can query for entities by that value, and you can insert other entities that refer to that value, so that you get a link to it. In this way it acts a bit like <code>:db/ident</code></z><z id="t1603288430" t="quoll 2. Yes, a true is an assertion, a false is a retraction. It’s just to look like Datoms. It’s not actually stored anywhere 🙂"><y>#</y><d>2020-10-21</d><h>13:53</h><r>quoll</r>2. Yes, a true is an assertion, a false is a retraction. It’s just to look like Datoms. It’s not actually stored anywhere <b>🙂</b></z><z id="t1603288608" t="quoll Errrr…. that would be trivial. Until recently, that’s HOW you put data into Asami. But the transact call was supposed to look like Datomic, so it doesn’t accept that. Ironically, the transact function filters the input for 3 things: 1. Maps. These are entities and are turned into triples for insertion (with some retractions if modifications were requested) 2. :db/add These are stripped down into triples and inserted. 3. :db/retract These are stripped down into triples and removed."><y>#</y><d>2020-10-21</d><h>13:56</h><r>quoll</r>Errrr…. that would be trivial. Until recently, that’s HOW you put data into Asami. But the transact call was supposed to look like Datomic, so it doesn’t accept that.
Ironically, the transact function filters the input for 3 things:
1. Maps. These are entities and are turned into triples for insertion (with some retractions if modifications were requested)
2. <code>:db/add</code> These are stripped down into triples and inserted.
3. <code>:db/retract</code> These are stripped down into triples and removed.</z><z id="t1603288641" t="quoll I could add an option to look for triples, if that’s important to you"><y>#</y><d>2020-10-21</d><h>13:57</h><r>quoll</r>I could add an option to look for triples, if that’s important to you</z><z id="t1603288786" t="rickmoynihan 1. ok that’s pretty much what I thought after playing with it a little… map syntax is kinda like coining a blank node. &gt; In this way it acts a bit like :db/ident  Did you mean to say “In this way it acts a bit like :db/id here?”"><y>#</y><d>2020-10-21</d><h>13:59</h><r>rickmoynihan</r>1. ok that’s pretty much what I thought after playing with it a little… map syntax is kinda like coining a blank node.

&gt; In this way it acts a bit like :db/ident 
Did you mean to say “In this way it acts a bit like <code>:db/id</code> here?”</z><z id="t1603289113" t="rickmoynihan &gt; I could add an option to look for triples, if that’s important to you It’s not super important at the minute… was just wondering what the best way to reingest the naga results as a connection"><y>#</y><d>2020-10-21</d><h>14:05</h><r>rickmoynihan</r>&gt; I could add an option to look for triples, if that’s important to you
It’s not super important at the minute… was just wondering what the best way to reingest the naga results as a connection</z><z id="t1603189058" t="rickmoynihan Is it possible (and is there an example of) to use an asami store, with some naga rules, with arbitrary asami queries as a clojure library?"><y>#</y><d>2020-10-20</d><h>10:17</h><w>rickmoynihan</w>Is it possible (and is there an example of) to use an asami store, with some naga rules, with arbitrary asami queries as a clojure library?</z><z id="t1603288807" t="quoll There aren’t really many examples. The Naga CLI was actually written to be an example of using Naga. It probable doesn’t do well for that anymore now that the Asami API has changed so much. It still works though… https://github.com/threatgrid/naga/blob/main/src/naga/cli.clj"><y>#</y><d>2020-10-21</d><h>14:00</h><r>quoll</r>There aren’t really many examples. The Naga CLI was actually written to be an example of using Naga. It probable doesn’t do well for that anymore now that the Asami API has changed so much. It still works though…
<a href="https://github.com/threatgrid/naga/blob/main/src/naga/cli.clj" target="_blank">https://github.com/threatgrid/naga/blob/main/src/naga/cli.clj</a></z><z id="t1603189965" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] : It looks like naga performs inference at index time not query time, is that right?"><y>#</y><d>2020-10-20</d><h>10:32</h><w>rickmoynihan</w><a>@quoll</a>: It looks like naga performs inference at index time not query time, is that right?</z><z id="t1603288834" t="quoll I actually don’t understand what you’re asking."><y>#</y><d>2020-10-21</d><h>14:00</h><r>quoll</r>I actually don’t understand what you’re asking.</z><z id="t1603288852" t="quoll Data is indexed as it is inserted, since it’s the indexes which store the data."><y>#</y><d>2020-10-21</d><h>14:00</h><r>quoll</r>Data is indexed as it is inserted, since it’s the indexes which store the data.</z><z id="t1603288868" t="quoll Naga runs inferences by executing a query, and inserting the results as triples"><y>#</y><d>2020-10-21</d><h>14:01</h><r>quoll</r>Naga runs inferences by executing a query, and inserting the results as triples</z><z id="t1603288918" t="rickmoynihan Thanks you’ve answered the question 🙂 Was essentially just checking that it worked by materialisation, rather than via query rewriting."><y>#</y><d>2020-10-21</d><h>14:01</h><r>rickmoynihan</r>Thanks you’ve answered the question <b>🙂</b>

Was essentially just checking that it worked by materialisation, rather than via query rewriting.</z><z id="t1603290118" t="quoll Yes, it does. I’d like to do query rewriting one day. But if that’s what you need, then may I suggest Stardog? 🙂"><y>#</y><d>2020-10-21</d><h>14:21</h><r>quoll</r>Yes, it does. I’d like to do query rewriting one day. But if that’s what you need, then may I suggest Stardog? <b>🙂</b></z><z id="t1603290658" t="rickmoynihan I’m already using Stardog 🙂 It’s not a requirement either; just a question from a curious mind 🙂"><y>#</y><d>2020-10-21</d><h>14:30</h><r>rickmoynihan</r>I’m already using Stardog <b>🙂</b>

It’s not a requirement either; just a question from a curious mind <b>🙂</b></z><z id="t1603190062" t="rickmoynihan actually I can see that it does :thumbsup:"><y>#</y><d>2020-10-20</d><h>10:34</h><w>rickmoynihan</w>actually I can see that it does <b>:thumbsup:</b></z><z id="t1603190279" t="rickmoynihan 👀 ahh there’s a query function on the Storage protocol"><y>#</y><d>2020-10-20</d><h>10:37</h><w>rickmoynihan</w><b>👀</b> ahh there’s a <code>query</code> function on the <code>Storage</code> protocol</z><z id="t1603190402" t="rickmoynihan looks like that’s a lower level interface though…"><y>#</y><d>2020-10-20</d><h>10:40</h><w>rickmoynihan</w>looks like that’s a lower level interface though…</z><z id="t1603288929" t="quoll Yes. This was before Asami became its own thing. However, you can pull the Asami graph back out of Naga and convert it into a Connection"><y>#</y><d>2020-10-21</d><h>14:02</h><r>quoll</r>Yes. This was before Asami became its own thing. However, you can pull the Asami graph back out of Naga and convert it into a Connection</z><z id="t1603288944" t="quoll I will document this, and probably add a function to make it trivial"><y>#</y><d>2020-10-21</d><h>14:02</h><r>quoll</r>I will document this, and probably add a function to make it trivial</z><z id="t1603288997" t="rickmoynihan yeah that’s essentially what I was looking at doing"><y>#</y><d>2020-10-21</d><h>14:03</h><r>rickmoynihan</r>yeah that’s essentially what I was looking at doing</z><z id="t1603289152" t="quoll If you have an AsamiGraph called asami-graph that you used for Naga, then convert it by saying: (asami.core/as-connection (:graph asami-graph) &quot;my:graph:uri&quot;)"><y>#</y><d>2020-10-21</d><h>14:05</h><r>quoll</r>If you have an AsamiGraph called <code>asami-graph</code> that you used for Naga, then convert it by saying:
<code>(asami.core/as-connection (:graph asami-graph) &quot;my:graph:uri&quot;)</code></z><z id="t1603289284" t="quoll Everything wraps a Graph object. Naga wraps it using the AsamiGraph record. This gives it transactions, query/insert, etc. But the graph that it’s wrapping is just the :graph field. Asami Connection objects wrap the graph as well. And you can manually do that with the as-connection function. This wraps it, and assigns a URI for it (since connections have a URI)"><y>#</y><d>2020-10-21</d><h>14:08</h><r>quoll</r>Everything wraps a <code>Graph</code> object. Naga wraps it using the <code>AsamiGraph</code> record. This gives it transactions, query/insert, etc. But the graph that it’s wrapping is just the <code>:graph</code> field.

Asami <code>Connection</code> objects wrap the graph as well. And you can manually do that with the <code>as-connection</code> function. This wraps it, and assigns a URI for it (since connections have a URI)</z><z id="t1603289737" t="quoll https://github.com/threatgrid/asami/issues/95 https://github.com/threatgrid/asami/issues/96 I’ll look at something for Naga too"><y>#</y><d>2020-10-21</d><h>14:15</h><r>quoll</r><a href="https://github.com/threatgrid/asami/issues/95" target="_blank">https://github.com/threatgrid/asami/issues/95</a>
<a href="https://github.com/threatgrid/asami/issues/96" target="_blank">https://github.com/threatgrid/asami/issues/96</a>
I’ll look at something for Naga too</z><z id="t1603191557" t="rickmoynihan I can do stuff like this: (def rules [(r/r &quot;grand-parent&quot; [?a :grand-parent ?gp] :- [?a :parent ?c] [?c :parent ?gp])]) (def axioms [[:amy :parent :adam] [:amy :parent :mary] [:mary :parent :sandra] [:adam :parent :john]]) (def prog (r/create-program rules axioms)) (sr/register-storage! :memory asami-store/create-store) (let [[store results :as output] (e/run {:type :memory} prog)] (store/resolve-pattern store &apos;[?s ?p ?o])) But is it possible to configure naga as a store itself, and register it with asami? Or do I need to pass the set of results from the query pattern back into a new asami store?"><y>#</y><d>2020-10-20</d><h>10:59</h><w>rickmoynihan</w>I can do stuff like this:

<pre>(def rules [(r/r &quot;grand-parent&quot; [?a :grand-parent ?gp] :- [?a :parent ?c] [?c :parent ?gp])])

  (def axioms
    [[:amy :parent :adam]
     [:amy :parent :mary]
     [:mary :parent :sandra]
     [:adam :parent :john]])

  (def prog (r/create-program rules axioms))

  (sr/register-storage! :memory asami-store/create-store)
  (let [[store results :as output] (e/run  {:type :memory} prog)]
    (store/resolve-pattern store &apos;[?s ?p ?o]))</pre>
But is it possible to configure naga as a store itself, and register it with asami?  Or do I need to pass the set of results from the query pattern back into a new asami store?</z><z id="t1603289012" t="quoll Naga actually just wraps a store. The idea was that it could wrap any graph at all, starting with Datomic and SPARQL. But then I was asked to do something myself (before I got to the SPARQL implementation) and that’s how Asami got started"><y>#</y><d>2020-10-21</d><h>14:03</h><w>quoll</w>Naga actually just wraps a store. The idea was that it could wrap any graph at all, starting with Datomic and SPARQL. But then I was asked to do something myself (before I got to the SPARQL implementation) and that’s how Asami got started</z><z id="t1603290114" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] : One other thing; is it necessary for asami/naga to provide a registry and connection strings etc? Is there an interface where I can just provide a db as a value? Also that way if I wanted mutability or a registry; I could provide my own atoms, rather than you encapsulating them behind an identifier etc?"><y>#</y><d>2020-10-21</d><h>14:21</h><w>rickmoynihan</w><a>@quoll</a>: One other thing; is it necessary for asami/naga to provide a registry and connection strings etc?  Is there an interface where I can just provide a db as a value?

Also that way if I wanted mutability or a registry; I could provide my own atoms, rather than you encapsulating them behind an identifier etc?</z><z id="t1603290173" t="quoll You’re talking about the Graph interface (well… protocol)"><y>#</y><d>2020-10-21</d><h>14:22</h><w>quoll</w>You’re talking about the <code>Graph</code> interface (well… protocol)</z><z id="t1603290187" t="quoll Connection/Database is just a wrapper around this"><y>#</y><d>2020-10-21</d><h>14:23</h><w>quoll</w>Connection/Database is just a wrapper around this</z><z id="t1603290203" t="quoll Connections associate a URI with the Graph instance"><y>#</y><d>2020-10-21</d><h>14:23</h><w>quoll</w>Connections associate a URI with the Graph instance</z><z id="t1603290231" t="quoll and then provide a history as a series of databases. Databases are VERY thin wrappers around a graph"><y>#</y><d>2020-10-21</d><h>14:23</h><w>quoll</w>and then provide a history as a series of databases. Databases are VERY thin wrappers around a graph</z><z id="t1603290265" t="quoll Until a couple of months ago, there was only the Graph. But everyone loves Datomic, so I wrapped it to look like Datomic 🙂"><y>#</y><d>2020-10-21</d><h>14:24</h><w>quoll</w>Until a couple of months ago, there was only the Graph. But everyone loves Datomic, so I wrapped it to look like Datomic <b>🙂</b></z><z id="t1603290344" t="quoll Naga wrapped the graph, but only does so in order to manage transactions. The functions attached to the wrapper were useful for Datomic as well, so I kept them at that level"><y>#</y><d>2020-10-21</d><h>14:25</h><w>quoll</w>Naga wrapped the graph, but only does so in order to manage transactions. The functions attached to the wrapper were useful for Datomic as well, so I kept them at that level</z><z id="t1603290435" t="quoll Asami Graph instances can be inserted into, deleted from, and queried. And until recently, that was everything we did with them. I made it messy by giving it a Connection. But you can still pull the graph out. And if you want to build a connection around a graph, then you can do that too"><y>#</y><d>2020-10-21</d><h>14:27</h><w>quoll</w>Asami <code>Graph</code> instances can be inserted into, deleted from, and queried. And until recently, that was everything we did with them. I made it messy by giving it a Connection. But you can still pull the graph out. And if you want to build a connection around a graph, then you can do that too</z><z id="t1603290467" t="quoll It just occurred to me… This is all VERY Asami specific. Maybe it’s better to talk about this in #asami"><y>#</y><d>2020-10-21</d><h>14:27</h><w>quoll</w>It just occurred to me… This is all VERY Asami specific. Maybe it’s better to talk about this in #asami</z><z id="t1603290488" t="rickmoynihan :thumbsup: sure didn’t realise there was an #asami 🙂 thanks"><y>#</y><d>2020-10-21</d><h>14:28</h><w>rickmoynihan</w><b>:thumbsup:</b> sure didn’t realise there was an #asami <b>🙂</b> thanks</z><z id="t1603290512" t="quoll It’s only been around a short time"><y>#</y><d>2020-10-21</d><h>14:28</h><r>quoll</r>It’s only been around a short time</z><z id="t1603291266" t="rickmoynihan 🙇 forgot to say thanks a million for all the answers [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}]"><y>#</y><d>2020-10-21</d><h>14:41</h><w>rickmoynihan</w><b>🙇</b> forgot to say thanks a million for all the answers <a>@quoll</a></z><z id="t1603291398" t="quoll You’re welcome"><y>#</y><d>2020-10-21</d><h>14:43</h><r>quoll</r>You’re welcome</z><z id="t1603356062" t="simongray [:attrs {:href &quot;/_/_/users/UB3R8UYA1&quot;}] Just watched your IGraph demo on youtube ( https://www.youtube.com/watch?v=BlH__4iNHZE&amp;amp;feature=youtu.be ). I gotta say I’m very impressed by the set of graph libraries you’re developing. I was wondering if there are any plans to convert between your IGraph protocol and the Ubergraph protocol?"><y>#</y><d>2020-10-22</d><h>08:41</h><w>simongray</w><a>@eric.d.scott</a> Just watched your IGraph demo on youtube (<a href="https://www.youtube.com/watch?v=BlH__4iNHZE&amp;amp;feature=youtu.be" target="_blank">https://www.youtube.com/watch?v=BlH__4iNHZE&amp;amp;feature=youtu.be</a>). I gotta say I’m very impressed by the set of graph libraries you’re developing. I was wondering if there are any plans to convert between your IGraph protocol and the Ubergraph protocol?</z><z id="t1603455099" t="Eric Scott Oh thanks! That is indeed an item on my to-do list, but to be honest it might take me a while for that to reach my front burner."><y>#</y><d>2020-10-23</d><h>12:11</h><w>Eric Scott</w>Oh thanks! That is indeed an item on my to-do list, but to be honest it might take me a while for that to   reach my front burner.</z><z id="t1603457744" t="simongray I have no immediate need for it either, but since I already made a datafied representation of the Neo4j query results corresponding to the input vectors used to instantiate an Ubergraph graph, I was thinking it might also be handy to be able to convert to between the two. Your model is different, but I’m not sure how far apart they are. Anyway, you seem to be doing a lot of work on representing RDF in the application space and my dataset is actually RDF too. I feel that Ubergraph maps very closely to the native Neo4j labeled property graph, so it was a great fit to use its vector representation for the datafy protocol. However, I am not married to that particular graph model for RDF at all, just using it out of convenience really (Neo4j is fast, has big ecosystem, there’s an RDF plugin)."><y>#</y><d>2020-10-23</d><h>12:55</h><r>simongray</r>I have no immediate need for it either, but since I already made a datafied representation of the Neo4j query results  corresponding to the input vectors used to instantiate an Ubergraph graph, I was thinking it might also be handy to be able to convert to between the two. Your model is different, but I’m not sure how far apart they are.

Anyway, you seem to be doing a lot of work on representing RDF in the application space and my dataset is actually RDF too. I feel that Ubergraph maps very closely to the native Neo4j labeled property graph, so it was a great fit to use its vector representation for the datafy protocol. However, I am not married to that particular graph model for RDF at all, just using it out of convenience really (Neo4j is fast, has big ecosystem, there’s an RDF plugin).</z><z id="t1603458030" t="simongray I think conversion between different graph representations can be quite useful. I would love to move graph data seamlessly between e.g. Neo4j or one of the Datalog dbs."><y>#</y><d>2020-10-23</d><h>13:00</h><r>simongray</r>I think conversion between different graph representations can be quite useful. I would love to move graph data seamlessly between e.g. Neo4j or one of the  Datalog dbs.</z><z id="t1603458116" t="simongray Still need to learn a lot more about the domain, though."><y>#</y><d>2020-10-23</d><h>13:01</h><r>simongray</r>Still need to learn a lot more about the domain, though.</z><z id="t1603477056" t="Eric Scott I agree. Neo4j have a slightly different structure I think they call &apos;property graphs&apos;, which allow you to annotate specific relationships between specific objects, so it&apos;s difficult to duplicate that with triples."><y>#</y><d>2020-10-23</d><h>18:17</h><r>Eric Scott</r>I agree. Neo4j have a slightly different structure I think they call &apos;property graphs&apos;, which allow you to annotate specific relationships between specific objects, so it&apos;s difficult to duplicate that with triples.</z><z id="t1603637408" t="quoll Yes, that either needs reification 😖 or internal statement IDs (which SPARQL only supports via reification)"><y>#</y><d>2020-10-25</d><h>14:50</h><r>quoll</r>Yes, that either needs reification <b>😖</b> or internal statement IDs (which SPARQL only supports via reification)</z><z id="t1603718973" t="Eric Scott A few months ago I was playing with a representation that had two components [G A], with G being a graph in IGrpah normal form and A being a map &lt;p&gt;-&gt; G[p], where G[p] is a graph of [subject object annotation] triples for some selected P."><y>#</y><d>2020-10-26</d><h>13:29</h><r>Eric Scott</r>A few months ago I was playing with a representation that had two components [G A], with G being a graph in IGrpah normal form and A being a map &lt;p&gt;-&gt; G[p], where G[p] is a graph of [subject object annotation] triples for some selected P.</z><z id="t1603718989" t="Eric Scott That&apos;s simmering on the back burner though."><y>#</y><d>2020-10-26</d><h>13:29</h><r>Eric Scott</r>That&apos;s simmering on the back burner though.</z><z id="t1603726655" t="quoll I’m building some on-disk indices at the moment. The current one is designed for updating (and is therefore better for Datomic-like usage), but the other one is for large imports and doesn’t handle updates as well. That one is more about graph analytics on large RDF datasets. But one feature is that every statement gets an internal ID. So I’m thinking that I need to support implicit reification with it. This makes me wonder if I should consider it for the other index as well. :thinking_face:"><y>#</y><d>2020-10-26</d><h>15:37</h><r>quoll</r>I’m building some on-disk indices at the moment. The current one is designed for updating (and is therefore better for Datomic-like usage), but the other one is for large imports and doesn’t handle updates as well. That one is more about graph analytics on large RDF datasets. But one feature is that every statement gets an internal ID. So I’m thinking that I need to support implicit reification with it. This makes me wonder if I should consider it for the other index as well. <b>:thinking_face:</b></z><z id="t1603540019" t="simongray I just discovered Fluree which is apparently some kind of blockchain RDF triple store written in Clojure that seems to be queried using Datalog: • https://docs.flur.ee/guides/0.15.0/intro/what-is-fluree • https://flur.ee/why-fluree/ • https://github.com/fluree I have been reading the whats and whys, but I don’t really understand the purpose of it. It seems to have a tonne of documentation though. I’m guessing it’s meant for fintech stuff? I am trying to figure out what to think of it. Anybody here know what the deal is?"><y>#</y><d>2020-10-24</d><h>11:46</h><w>simongray</w>I just discovered Fluree which is apparently some kind of blockchain RDF triple store written in Clojure that seems to be queried using Datalog:
• <a href="https://docs.flur.ee/guides/0.15.0/intro/what-is-fluree" target="_blank">https://docs.flur.ee/guides/0.15.0/intro/what-is-fluree</a>
• <a href="https://flur.ee/why-fluree/" target="_blank">https://flur.ee/why-fluree/</a>
• <a href="https://github.com/fluree" target="_blank">https://github.com/fluree</a>
I have been reading the whats and whys, but I don’t really understand the purpose of it. It seems to have a tonne of documentation though. I’m guessing it’s meant for fintech stuff? I am trying to figure out what to think of it. Anybody here know what the deal is?</z><z id="t1603559378" t="samir There are a few use cases where true immutability in a graph database has a significant business value, for example land owning rights for a country. But in times of GDPR, if a company is interested in immutable databases then they mostly need overridable immutability (to delete GDPR relevant data). It is possible to build something like this on a blockchain using anonymous identifiers but then you loose a lot of the advantages of a graph databases…. I don’t know how Fluree is addressing this issue."><y>#</y><d>2020-10-24</d><h>17:09</h><w>samir</w>There are a few use cases where true immutability in a graph database has a significant business value, for example land owning rights for a country. But in times of GDPR, if a company is interested in immutable databases then they mostly need overridable immutability (to delete GDPR relevant data). It is possible to build something like this on a blockchain using anonymous identifiers but then you loose a lot of the advantages of a graph databases…. I don’t know how Fluree is addressing this issue.</z><z id="t1603630987" t="simongray That makes sense. Yeah, will be tough to square this with GDPR."><y>#</y><d>2020-10-25</d><h>13:03</h><r>simongray</r>That makes sense. Yeah, will be tough to square this with GDPR.</z><z id="t1603560009" t="Ivan the usual case with immutable databases (ie, banks) is that PII is encrypted; to delete the information, you delete the key. And this is acceptable by the GDPR and EC."><y>#</y><d>2020-10-24</d><h>17:20</h><w>Ivan</w>the usual case with immutable databases (ie, banks) is that PII is encrypted; to delete the information, you delete the key. And this is acceptable by the GDPR and EC.</z><z id="t1603616152" t="samir Thanks for the clarification!"><y>#</y><d>2020-10-25</d><h>08:55</h><r>samir</r>Thanks for the clarification!</z><z id="t1603636978" t="simongray Hey everyone. I’m trying to build a curated list of graph resources for Clojure. If you have any suggestions, please let me know: https://github.com/simongray/clj-graph-resources"><y>#</y><d>2020-10-25</d><h>14:42</h><w>simongray</w>Hey everyone. I’m trying to build a curated list of graph resources for Clojure. If you have any suggestions, please let me know: <a href="https://github.com/simongray/clj-graph-resources" target="_blank">https://github.com/simongray/clj-graph-resources</a></z><z id="t1603637277" t="quoll Ugh… this is going to make me have to implement RDF import/export and SPARQL sooner, isn’t it? 🙂"><y>#</y><d>2020-10-25</d><h>14:47</h><r>quoll</r>Ugh… this is going to make me have to implement RDF import/export and SPARQL sooner, isn’t it? <b>🙂</b></z><z id="t1605841498" t="Eric Scott Does anyone have any experience declaring builtin Jena extensions in Clojure? (def always-true (proxy [BaseBuiltin Builtin] [] (getName [] &quot;alwaysTrue&quot;) (getArgLength [] 1) (bodyCall [args length context] ;; (throw (ex-info &quot;asdf&quot; {})) Whether this statement is included or not has no effect. (edit) (def ^:dynamic *args* args) (def ^:dynamic *context* context) true))) (.register BuiltinRegistry/theRegistry always-true)"><y>#</y><d>2020-11-20</d><h>03:04</h><w>Eric Scott</w>Does anyone have any experience declaring builtin Jena extensions in Clojure?
<pre>(def always-true (proxy [BaseBuiltin Builtin] []
                   (getName [] &quot;alwaysTrue&quot;)
                   (getArgLength [] 1)
                   (bodyCall [args length context]
                          ;; (throw (ex-info &quot;asdf&quot; {})) Whether this statement is included or not has no effect. (edit)
                         (def ^:dynamic *args* args)
                         (def ^:dynamic *context* context)
                     true)))
(.register BuiltinRegistry/theRegistry always-true)</pre></z><z id="t1605841824" t="Eric Scott I was hoping the code above would give me a little visibility into SWRL rules. It doesn&apos;t complain when I register it, but then when I insert alwaysTrue(?x) into the body of SWRL rule that already works, the code does not appear to be executed, i.e., the dynamic variables are not being bound. Am I missing something?"><y>#</y><d>2020-11-20</d><h>03:10</h><w>Eric Scott</w>I was hoping the code above would give me a little visibility into SWRL rules. It doesn&apos;t complain when I register it, but then when I insert alwaysTrue(?x) into the body of SWRL rule that already works, the code does not appear to be executed, i.e., the dynamic variables are not being bound. Am I missing something?</z><z id="t1605870246" t="rickmoynihan [:attrs {:href &quot;/_/_/users/UB3R8UYA1&quot;}] : it’s not because you’re first raising an exception that’s somehow being swallowed is it? also pretty sure you can remove the ^:dynamic there. That just makes the vars support rebinding thread locally with binding . re def ing should work without for debugging purposes should work without that."><y>#</y><d>2020-11-20</d><h>11:04</h><w>rickmoynihan</w><a>@eric.d.scott</a>: it’s not because you’re first raising an exception that’s somehow being swallowed is it?

also pretty sure you can remove the <code>^:dynamic</code> there.  That just makes the vars support rebinding thread locally with <code>binding</code>.  re <code>def</code> ing should work without for debugging purposes should work without that.</z><z id="t1605875094" t="Eric Scott Oh. Yeah, I actually added the exception later just to see if I could shake something loose. Forgot to take that out of the example. But even without the exception this code shows no signs of being visited."><y>#</y><d>2020-11-20</d><h>12:24</h><w>Eric Scott</w>Oh. Yeah, I actually added the exception later just to see if I could shake something loose. Forgot to take that out of the example. But even without the exception this code shows no signs of being visited.</z><z id="t1605875622" t="Eric Scott The puzzle here for me is why isn&apos;t this method being called? Are there missing metadata tags that are causing some other bodyCall method to be dispatched instead?"><y>#</y><d>2020-11-20</d><h>12:33</h><w>Eric Scott</w>The puzzle here for me is why isn&apos;t this method being called? Are there missing metadata tags that are causing some other bodyCall method to be dispatched instead?</z><z id="t1605879405" t="Eric Scott Ah. Nevermind. I&apos;m an idiot. I was passing in a non-existent variable. Jeesh."><y>#</y><d>2020-11-20</d><h>13:36</h><w>Eric Scott</w>Ah. Nevermind. I&apos;m an idiot. I was passing in a non-existent variable. Jeesh.</z><z id="t1605879678" t="Eric Scott https://comb.io/XCdcbj"><y>#</y><d>2020-11-20</d><h>13:41</h><w>Eric Scott</w><a href="https://comb.io/XCdcbj" target="_blank">https://comb.io/XCdcbj</a></z><z id="t1606107970" t="EmmanuelOga I&apos;ve been experimenting with RDF and graph databases. I think RDF is a great basis for content management systems. I recently found about TerminusDB which on top of the RDF model provides a git-like storage system (so data can be branched and merged, reverted, etc.). I came up with a little proof of concept for rendering content snippets embedding triple data  https://github.com/EmmanuelOga/NeonTetra"><y>#</y><d>2020-11-23</d><h>05:06</h><w>EmmanuelOga</w>I&apos;ve been experimenting with RDF and graph databases. I think RDF is a great basis for content management systems. I recently found about TerminusDB which on top of the RDF model provides a git-like storage system (so data can be branched and merged, reverted, etc.). I came up with a little proof of concept for rendering content snippets embedding triple data <a href="https://github.com/EmmanuelOga/NeonTetra" target="_blank">https://github.com/EmmanuelOga/NeonTetra</a></z><z id="t1606212531" t="rickmoynihan I saw terminusdb a short while back; but I didn’t quite understand exactly what their model is. I also found their feature comparison matrix a bit disingenuous… For example to claim that SQL or SPARQL aren’t “Rich advanced query languages” is a cop out. In what way is datomic/terminusdb’s query language richer and more advanced than those?! Likewise stardog absolutely has ACID transactions: https://www.stardog.com/docs/#_acid_transactions Similarly you might say “like git for data” can also be claimed for stardog too: https://www.stardog.com/docs/man/vcs-commit (though it’s stretching it a bit as the model doesn’t really include proper branching/merging) but it does definitely do MVVC: https://www.stardog.com/docs/6.2.3/#_snapshot_isolation I mean sure there are potentially caveats to each of these. Also don’t mean to criticise terminus, I know almost nothing about it - and am genuinely curious what the differences are, I just found their feature comparison matrix was too flawed to be useful. Is it actually RDF, or just RDF-like?!"><y>#</y><d>2020-11-24</d><h>10:08</h><w>rickmoynihan</w>I saw terminusdb a short while back; but I didn’t quite understand exactly what their model is.

I also found their feature comparison matrix a bit disingenuous…  For example to claim that SQL or SPARQL aren’t “Rich advanced query languages” is a cop out.  In what way is datomic/terminusdb’s query language richer and more advanced than those?!

Likewise stardog absolutely has ACID transactions: <a href="https://www.stardog.com/docs/#_acid_transactions" target="_blank">https://www.stardog.com/docs/#_acid_transactions</a>

Similarly you might say “like git for data” can also be claimed for stardog too: <a href="https://www.stardog.com/docs/man/vcs-commit" target="_blank">https://www.stardog.com/docs/man/vcs-commit</a> (though it’s stretching it a bit as the model doesn’t really include proper branching/merging)
but it does definitely do MVVC: <a href="https://www.stardog.com/docs/6.2.3/#_snapshot_isolation" target="_blank">https://www.stardog.com/docs/6.2.3/#_snapshot_isolation</a>

I mean sure there are potentially caveats to each of these.  Also don’t mean to criticise terminus, I know almost nothing about it - and am genuinely curious what the differences are, I just found their feature comparison matrix was too flawed to be useful.

Is it actually RDF, or just RDF-like?!</z><z id="t1606214256" t="rickmoynihan Looks like it’s not RDF, and strong schema upfront gives it very different properties"><y>#</y><d>2020-11-24</d><h>10:37</h><w>rickmoynihan</w>Looks like it’s not RDF, and strong schema upfront gives it very different properties</z><z id="t1606214360" t="rickmoynihan Oh ok but that’s implemented as URIs with xsd’s just with OWL with closed world semantics"><y>#</y><d>2020-11-24</d><h>10:39</h><w>rickmoynihan</w>Oh ok but that’s implemented as URIs with xsd’s just with OWL with closed world semantics</z><z id="t1606217807" t="simongray [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] So it is then RDF then?"><y>#</y><d>2020-11-24</d><h>11:36</h><w>simongray</w><a>@rickmoynihan</a> So it is then RDF then?</z><z id="t1606218199" t="rickmoynihan Sort of. I think it’s more accurate to say it supports a subset of RDF graphs. i.e. I don’t think you could take any arbitrary RDF from the wild and load it. I think to load data from the wild you’d need to load the ontologies describing all the terms in the data you’re loading first, and they would need to meet the constraints of the ontology under closed world OWL semantics."><y>#</y><d>2020-11-24</d><h>11:43</h><w>rickmoynihan</w>Sort of.

I think it’s more accurate to say it supports a subset of RDF graphs.

i.e. I don’t think you could take any arbitrary RDF from the wild and load it.

I think to load data from the wild you’d need to load the ontologies describing all the terms in the data you’re loading first, and they would need to meet the constraints of the ontology under closed world OWL semantics.</z><z id="t1606218237" t="rickmoynihan I think it’s true to say that any terminus db is valid RDF"><y>#</y><d>2020-11-24</d><h>11:43</h><w>rickmoynihan</w>I think it’s true to say that any terminus db is valid RDF</z><z id="t1606218268" t="rickmoynihan only read their whitepaper though, not sure what the docs say"><y>#</y><d>2020-11-24</d><h>11:44</h><w>rickmoynihan</w>only read their whitepaper though, not sure what the docs say</z><z id="t1606218592" t="rickmoynihan Also they say: &gt; The names for nodes and labels are drawn from a set of IRIs … we have chosen the XSD datatypes as our universe of concrete values. Which seems to leave out blank nodes… so sounds like RDF without blank nodes; which is a good thing to remove."><y>#</y><d>2020-11-24</d><h>11:49</h><w>rickmoynihan</w>Also they say:

&gt; The names for nodes and labels are drawn from a set of IRIs … we have chosen the XSD datatypes as our universe of concrete values.
Which seems to leave out blank nodes… so sounds like RDF without blank nodes; which is a good thing to remove.</z><z id="t1606227559" t="simongray Excuse my ignorance, but why is that a good thing to remove?"><y>#</y><d>2020-11-24</d><h>14:19</h><r>simongray</r>Excuse my ignorance, but why is that a good thing to remove?</z><z id="t1606227970" t="rickmoynihan Well whether it’s good or not depends on your perspective. If you need perfect RDF interoperability it’s probably not a good thing to remove. However blank nodes are essentially just a small developer convenience that adds a boat load of complexity to RDF processing. The big problem is bnode identity; each time you load a bnode it generates a different identifier for it (usually scoped to the document it was loaded with). Hence bnodes destroy the ability to easily tell if two graphs are identical etc… in practice they can also cause problems with data loading, as you can easily get duplicated statements if you load the same file twice (and it contains bnodes) etc… so you lose idempotency and probably other set-theoretic properties too. It’s one of the few things that I think RDF would have been better without. It’s hard to see how you can do proper graph branching and merging etc if you allow them… at least without resorting to extra bodges or rules around handling them."><y>#</y><d>2020-11-24</d><h>14:26</h><r>rickmoynihan</r>Well whether it’s good or not depends on your perspective.  If you need perfect RDF interoperability it’s probably not a good thing to remove.

However blank nodes are essentially just a small developer convenience that adds a boat load of complexity to RDF processing.

The big problem is bnode identity; each time you load a bnode it generates a different identifier for it (usually scoped to the document it was loaded with).  Hence bnodes destroy the ability to easily tell if two graphs are identical etc…  in practice they can also cause problems with data loading, as you can easily get duplicated statements if you load the same file twice (and it contains bnodes) etc… so you lose idempotency and probably other set-theoretic properties too.

It’s one of the few things that I think RDF would have been better without.  It’s hard to see how you can do proper graph branching and merging etc if you allow them… at least without resorting to extra bodges or rules around handling them.</z><z id="t1606230921" t="simongray I see. Thank you for explaining."><y>#</y><d>2020-11-24</d><h>15:15</h><r>simongray</r>I see. Thank you for explaining.</z><z id="t1606218646" t="rickmoynihan I suspect that’s because blank nodes will mess up their “git like semantics”."><y>#</y><d>2020-11-24</d><h>11:50</h><w>rickmoynihan</w>I suspect that’s because blank nodes will mess up their “git like semantics”.</z><z id="t1606218668" t="rickmoynihan though not seen anything that says that explicitly"><y>#</y><d>2020-11-24</d><h>11:51</h><w>rickmoynihan</w>though not seen anything that says that explicitly</z><z id="t1606358255" t="EmmanuelOga TDB is definitely RDF, it allows adding arbitrary quads to the store but has the concept of &quot;documents&quot;, documents are just triples that follow a schema which is based on OWL with closed world semantics"><y>#</y><d>2020-11-26</d><h>02:37</h><w>EmmanuelOga</w>TDB is definitely RDF, it allows adding arbitrary quads to the store but has the concept of &quot;documents&quot;, documents are just triples that follow a schema which is based on OWL with closed world semantics</z><z id="t1606358678" t="EmmanuelOga I may be wrong about this, but I don&apos;t think SPARQL has a RDF serialization? Or at the very least is not defined in terms of triples itself right? Their query language (WOQL) is defined in RDF itself so it can be easier to compose because you don&apos;t necessarily need to parse it and massage the AST to come up with a different query, or run string interpolation, or rely on a SPARQL library to bind input values of a query or that sort of thing."><y>#</y><d>2020-11-26</d><h>02:44</h><w>EmmanuelOga</w>I may be wrong about this, but I don&apos;t think SPARQL has a RDF serialization? Or at the very least is not defined in terms of triples itself right?

Their query language (WOQL) is defined in RDF itself so it can be easier to compose because you don&apos;t necessarily need to parse it and massage the AST to come up with a different query, or run string interpolation, or rely on a SPARQL library to bind input values of a query or that sort of thing.</z><z id="t1606371310" t="quoll That’s correct… the only serialization for SPARQL was as a string"><y>#</y><d>2020-11-26</d><h>06:15</h><r>quoll</r>That’s correct… the only serialization for SPARQL was as a string</z><z id="t1606405137" t="rickmoynihan Well there is SPIN SPARQL syntax… Which lets you write SPARQL queries as RDF data: https://spinrdf.org/sp.html"><y>#</y><d>2020-11-26</d><h>15:38</h><r>rickmoynihan</r>Well there is SPIN SPARQL syntax…  Which lets you write SPARQL queries as RDF data:

<a href="https://spinrdf.org/sp.html" target="_blank">https://spinrdf.org/sp.html</a></z><z id="t1606405221" t="rickmoynihan Which seems to be exactly what you’re asking for; though it’s a W3C member submission so not a formal standard. I think it’s a pretty comprehensive and solid translation of SPARQL though."><y>#</y><d>2020-11-26</d><h>15:40</h><r>rickmoynihan</r>Which seems to be exactly what you’re asking for; though it’s a W3C member submission so not a formal standard.  I think it’s a pretty comprehensive and solid translation of SPARQL though.</z><z id="t1606405368" t="rickmoynihan Personally though I think I’d prefer my query representation to be in a native language data structure; rather than a graph of triples."><y>#</y><d>2020-11-26</d><h>15:42</h><r>rickmoynihan</r>Personally though I think I’d prefer my query representation to be in a native language data structure; rather than a graph of triples.</z><z id="t1606430150" t="EmmanuelOga RDF is language neutral which is a plus for me"><y>#</y><d>2020-11-26</d><h>22:35</h><r>EmmanuelOga</r>RDF is language neutral which is a plus for me</z><z id="t1606430223" t="EmmanuelOga Although yeah sometimes it can feel like a lower common denominator"><y>#</y><d>2020-11-26</d><h>22:37</h><r>EmmanuelOga</r>Although yeah sometimes it can feel like a lower common denominator</z><z id="t1606358701" t="EmmanuelOga In theory all you need to build queries is to send a bunch of triples... I have been using their client libraries so far though so I don&apos;t have a lot of experience building queries manually yet. I did enjoy the dynamic of building queries directly in JavaScript, normally I would build a SPARQL query as a string an then bind some variables to my input values with a library like RDF4J or rdflib."><y>#</y><d>2020-11-26</d><h>02:45</h><w>EmmanuelOga</w>In theory all you need to build queries is to send a bunch of triples... I have been using their client libraries so far though so I don&apos;t have a lot of experience building queries manually yet.
I did enjoy the dynamic of building queries directly in JavaScript, normally I would build a SPARQL query as a string an then bind some variables to my input values with a library like RDF4J or rdflib.</z><z id="t1606358757" t="EmmanuelOga There&apos;s a discord community if you guys feel like checking it out https://discord.gg/Gvdqw97"><y>#</y><d>2020-11-26</d><h>02:45</h><w>EmmanuelOga</w>There&apos;s a discord community if you guys feel like checking it out <a href="https://discord.gg/Gvdqw97" target="_blank">https://discord.gg/Gvdqw97</a></z><z id="t1609945052" t="simongray Thoughts on mobi? https://mobi.inovexcorp.com/docs/ Like with many huge Java projects, I have a hard time pinning down what is really is in concrete terms. It seems to me to be an integrated (open source??) RDF+OWL solution of sorts with frontend application."><y>#</y><d>2021-01-06</d><h>14:57</h><w>simongray</w>Thoughts on mobi? <a href="https://mobi.inovexcorp.com/docs/" target="_blank">https://mobi.inovexcorp.com/docs/</a>
Like with many huge Java projects, I have a hard time pinning down what is really is in concrete terms. It seems to me to be an integrated (open source??) RDF+OWL solution of sorts with frontend application.</z><z id="t1610020428" t="rickmoynihan Looks a bit strange to me too, but it looks like it’s trying to be some kind of IDE for collaboratively working on ontologies, and perhaps other arbitrary RDF data. i.e. it looks to be a pretty minimal linked data browser and editor with some integrated git affordances etc. I think the value add seems pretty minimal — like why integrate git, would it not be better to standalone separately; or just be an editor plugin, or an RDF browser? I find the choice of git curious too; if you can programatically create the RDF with the UI (not 100% sure this is the case), then mixing automated editing of RDF with manual editing of it is gonna cause merge mayhem. If you want to do that, surely merging RDF would be better done at the RDF level."><y>#</y><d>2021-01-07</d><h>11:53</h><w>rickmoynihan</w>Looks a bit strange to me too, but it looks like it’s trying to be some kind of IDE for collaboratively working on ontologies, and perhaps other arbitrary RDF data.

i.e. it looks to be a pretty minimal linked data browser and editor with some integrated git affordances etc.

I think the value add seems pretty minimal — like why integrate git, would it not be better to standalone separately; or just be an editor plugin, or an RDF browser?

I find the choice of git curious too; if you can programatically create the RDF with the UI (not 100% sure this is the case), then mixing automated editing of RDF with manual editing of it is gonna cause merge mayhem.

If you want to do that, surely merging RDF would be better done at the RDF level.</z><z id="t1610020440" t="rickmoynihan Though like you I really don’t understand the usecase."><y>#</y><d>2021-01-07</d><h>11:54</h><w>rickmoynihan</w>Though like you I really don’t understand the usecase.</z><z id="t1610023184" t="simongray Glad I’m not the only one who is confused 🙂"><y>#</y><d>2021-01-07</d><h>12:39</h><w>simongray</w>Glad I’m not the only one who is confused <b>🙂</b></z><z id="t1610123966" t="cldwalker Mornin. Came across a clj editor, http://kanopi.io , which tries to make importing external triples into your “knowledge graph” easier - https://www.youtube.com/watch?v=z1QMkR1eys4&amp;amp;ab_channel=BrianJ.Rubinton . Anyone seen other similar editors? In case it’s not clear in the video, the left pane is for importing [?subject ?predicate current-entity] while the right pane is for [current-entity ?predicate ?object]"><y>#</y><d>2021-01-08</d><h>16:39</h><w>cldwalker</w>Mornin. Came across a clj editor, <a href="http://kanopi.io" target="_blank">http://kanopi.io</a>, which tries to make importing external triples into your “knowledge graph” easier - <a href="https://www.youtube.com/watch?v=z1QMkR1eys4&amp;amp;ab_channel=BrianJ.Rubinton" target="_blank">https://www.youtube.com/watch?v=z1QMkR1eys4&amp;amp;ab_channel=BrianJ.Rubinton</a>. Anyone seen other similar editors? In case it’s not clear in the video, the left pane is for importing <code>[?subject ?predicate current-entity]</code> while the right pane is for <code>[current-entity ?predicate ?object]</code></z><z id="t1610949410" t="EmmanuelOga baby in-memory quadstore in ~150 lines of Kotlin: https://gist.github.com/EmmanuelOga/1d52adb79bdb6092fb698ed5b31fa377 (uses AVL TreeSets for indices)"><y>#</y><d>2021-01-18</d><h>05:56</h><w>EmmanuelOga</w>baby in-memory quadstore in ~150 lines of Kotlin: <a href="https://gist.github.com/EmmanuelOga/1d52adb79bdb6092fb698ed5b31fa377" target="_blank">https://gist.github.com/EmmanuelOga/1d52adb79bdb6092fb698ed5b31fa377</a> (uses AVL TreeSets for indices)</z><z id="t1611480297" t="EmmanuelOga more quadstore experiments: https://twitter.com/EmmanuelOga/status/1353265056381767681"><y>#</y><d>2021-01-24</d><h>09:24</h><w>EmmanuelOga</w>more quadstore experiments: <a href="https://twitter.com/EmmanuelOga/status/1353265056381767681" target="_blank">https://twitter.com/EmmanuelOga/status/1353265056381767681</a></z><z id="t1611480435" t="EmmanuelOga The query API is not that bad.. it is just a bunch of methods filter and props . With this source triples: :me rdf:type schema:Person; schema:name &quot;Emmanuel Oga&quot;; schema:familyName &quot;Oga&quot;; schema:givenName &quot;Emmanuel&quot;; schema:jobTitle &quot;Software Engineer&quot;; schema:knowsLanguage &quot;en-US&quot;, &quot;es-ES&quot;. I can find my name like this: val name = qs.filter(s = &quot;:me&quot;).props(&quot;schema:name&quot;) // -&gt; returns &quot;Emmanuel Oga&quot;"><y>#</y><d>2021-01-24</d><h>09:27</h><w>EmmanuelOga</w>The query API is not that bad.. it is just a bunch of methods <code>filter</code> and <code>props</code>. With this source triples:
<pre>:me
  rdf:type schema:Person;
  schema:name &quot;Emmanuel Oga&quot;;
  schema:familyName &quot;Oga&quot;;
  schema:givenName &quot;Emmanuel&quot;;
  schema:jobTitle &quot;Software Engineer&quot;;
  schema:knowsLanguage &quot;en-US&quot;, &quot;es-ES&quot;.</pre>
I can find my name like this:
<pre>val name = qs.filter(s = &quot;:me&quot;).props(&quot;schema:name&quot;) // -&gt; returns &quot;Emmanuel Oga&quot;</pre></z><z id="t1611480464" t="EmmanuelOga very basic"><y>#</y><d>2021-01-24</d><h>09:27</h><w>EmmanuelOga</w>very basic</z><z id="t1613210434" t="simongray [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] Hey Paula, I wanted to ask some questions here that may also be relevant to others in the channel. I watched the presentation that Zach Oakes gave on O’Doyle Rules ( https://www.youtube.com/watch?v=XONRaJJAhpA ) and that made me much more interested in Rules Engines than I was before. I was wondering how Naga compares to it (and Clara Rules). One thing I’ve noticed is that the Datalog syntax in the README.md is Prolog-based rather than Datomic-style. Is the Datomic style also supported?"><y>#</y><d>2021-02-13</d><h>10:00</h><w>simongray</w><a>@quoll</a> Hey Paula, I wanted to ask some questions here that may also be relevant to others in the channel. I watched the presentation that Zach Oakes gave on O’Doyle Rules (<a href="https://www.youtube.com/watch?v=XONRaJJAhpA" target="_blank">https://www.youtube.com/watch?v=XONRaJJAhpA</a>) and that made me much more interested in Rules Engines than I was before. I was wondering how Naga compares to it (and Clara Rules).

One thing I’ve noticed is that the Datalog syntax in the README.md is Prolog-based rather than Datomic-style. Is the Datomic style also supported?</z><z id="t1613236836" t="Steven Deobald [:attrs {:href &quot;/_/_/users/U4P4NREBY&quot;}] Just a heads-up, Paula mentioned in #asami that she&apos;s on vacation (and offline) for a week or so. Might take her a few days to get back to you."><y>#</y><d>2021-02-13</d><h>17:20</h><r>Steven Deobald</r><a>@U4P4NREBY</a> Just a heads-up, Paula mentioned in #asami that she&apos;s on vacation (and offline) for a week or so. Might take her a few days to get back to you.</z><z id="t1613284499" t="simongray Ah, thank you. I&apos;ll try to ask her another time then 😊"><y>#</y><d>2021-02-14</d><h>06:34</h><r>simongray</r>Ah, thank you. I&apos;ll try to ask her another time then <b>😊</b></z><z id="t1613399942" t="quoll Just saw this now on my phone…"><y>#</y><d>2021-02-15</d><h>14:39</h><r>quoll</r>Just saw this now on my phone…</z><z id="t1613400296" t="quoll I’ll start by raising my standard objection that “Datalog syntax” really [:attrs nil] Prolog syntax (with restrictions). Rich called it Datalog because it has Datalog semantics, but it’s his own syntax."><y>#</y><d>2021-02-15</d><h>14:44</h><r>quoll</r>I’ll start by raising my standard objection that “Datalog syntax” really <b>is</b> Prolog syntax (with restrictions). Rich called it Datalog because it has Datalog semantics, but it’s his own syntax.</z><z id="t1613400406" t="quoll But I know what you mean, so to answer your question, yes. Internally, the rules get converted anyway. If you write your rules in code, then you HAVE to do it with the query syntax (unless you want to build up a whole lot of strings and parse them)"><y>#</y><d>2021-02-15</d><h>14:46</h><r>quoll</r>But I know what you mean, so to answer your question, yes. Internally, the rules get converted anyway. If you write your rules in code, then you HAVE to do it with the query syntax (unless you want to build up a whole lot of strings and parse them)</z><z id="t1613400628" t="quoll The easiest way to create rules is with a macro in naga.rules called r . This follows the Prolog convention of: head :- body"><y>#</y><d>2021-02-15</d><h>14:50</h><r>quoll</r>The easiest way to create rules is with a macro in <code>naga.rules</code> called <code>r</code>. This follows the Prolog convention of:
 head :- body</z><z id="t1613400790" t="quoll The difference is that it’s based on triple-patterns"><y>#</y><d>2021-02-15</d><h>14:53</h><r>quoll</r>The difference is that it’s based on triple-patterns</z><z id="t1613401104" t="quoll So the Datalog rule of: uncle(N,U) :- parent(N,P), brother(P,U). Can be expressed as: &apos;(require naga.rules :refer [r]) (r [?n :uncle ?u] :- [?n :parent ?p] [?p :brother ?u])"><y>#</y><d>2021-02-15</d><h>14:58</h><r>quoll</r>So the Datalog rule of:
<code>uncle(N,U) :- parent(N,P), brother(P,U).</code>
Can be expressed as:
<pre>&apos;(require naga.rules :refer [r])
(r [?n :uncle ?u] :- [?n :parent ?p] [?p :brother ?u])</pre></z><z id="t1613401154" t="quoll Macros mean that I don’t have to quote all the symbols in there 🙂"><y>#</y><d>2021-02-15</d><h>14:59</h><r>quoll</r>Macros mean that I don’t have to quote all the symbols in there <b>🙂</b></z><z id="t1613401201" t="quoll (Adjust the require for ClojureScript, of course)"><y>#</y><d>2021-02-15</d><h>15:00</h><r>quoll</r>(Adjust the <code>require</code> for ClojureScript, of course)</z><z id="t1613401289" t="quoll A Naga program is a seq of such rules. You compile them, then run them on a connection "><y>#</y><d>2021-02-15</d><h>15:01</h><r>quoll</r>A Naga program is a seq of such rules. You compile them, then run them on a connection </z><z id="t1613401962" t="quoll About rule options... Naga was originally Datalog centric, which excluded negations in queries, retracting statements, or existential statements (such as mother(C,M) :- person(C). ) Right now, Naga supports: • existential statements (meaning that new objects can be created, like in the mother example there) • query negations (via not ) • limited retraction. This is when you update a value that already exists, and requires a retract/assert on the value, since everything is multicardinality. It’s done via a &apos; annotation on an attribute."><y>#</y><d>2021-02-15</d><h>15:12</h><r>quoll</r>About rule options... Naga was originally Datalog centric, which excluded negations in queries, retracting statements, or existential statements (such as <code>mother(C,M) :- person(C).</code>)
Right now, Naga supports:
• existential statements (meaning that new objects can be created, like in the mother example there)
• query negations (via <code>not</code>)
• limited retraction. This is when you update a value that already exists, and requires a retract/assert on the value, since everything is multicardinality. It’s done via a <code>&apos;</code> annotation on an attribute.</z><z id="t1613402001" t="quoll The last one looks like we need to extend it to full retractions of statements though"><y>#</y><d>2021-02-15</d><h>15:13</h><r>quoll</r>The last one looks like we need to extend it to full retractions of statements though</z><z id="t1613402094" t="quoll Each of these extensions to Datalog (I mean the real Datalog here) are “dangerous” and can result in an infinite loop. That’s actually why they’re not in the standard Datalog definition."><y>#</y><d>2021-02-15</d><h>15:14</h><r>quoll</r>Each of these extensions to Datalog (I mean the real Datalog here) are “dangerous” and can result in an infinite loop. That’s actually why they’re not in the standard Datalog definition.</z><z id="t1613381933" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U4P4NREBY&quot;}] Naga has a data syntax for rules; it’s not the datomic syntax but instead looks to be a direct translation of the datalog horn clause syntax naga supports: https://github.com/threatgrid/naga/blob/964a71e300c947ead28f1b344ed9bccdbf97ff9b/test/naga/test_rules.cljc#L21 I’m not familiar with datomic rules, and have only played with naga briefly but after looking at them again it looks like the main differences are: 1. (I think) naga rules must be either unary or binary predicates. 2. Naga rules are materialised once at startup; datomic’s are AFAICT materialised at query time. I’m guessing datomics rules through a process similar to macro-expansion just expand into datomic query fragments… presumably with gensyms etc to prevent unintended bindings."><y>#</y><d>2021-02-15</d><h>09:38</h><w>rickmoynihan</w><a>@simongray</a> Naga has a data syntax  for rules; it’s not the datomic syntax but instead looks to be a direct translation of the datalog horn clause syntax naga supports:

<a href="https://github.com/threatgrid/naga/blob/964a71e300c947ead28f1b344ed9bccdbf97ff9b/test/naga/test_rules.cljc#L21" target="_blank">https://github.com/threatgrid/naga/blob/964a71e300c947ead28f1b344ed9bccdbf97ff9b/test/naga/test_rules.cljc#L21</a>

I’m not familiar with datomic rules, and have only played with naga briefly but after looking at them again it looks like the main differences are:

1. (I think) naga rules must be either unary or binary predicates.
2. Naga rules are materialised once at startup; datomic’s are AFAICT materialised at query time.

I’m guessing datomics rules through a process similar to macro-expansion just expand into datomic query fragments… presumably with gensyms etc to prevent unintended bindings.</z><z id="t1613382052" t="simongray Ooh, that Clojure data Prolog style syntax looks interesting."><y>#</y><d>2021-02-15</d><h>09:40</h><w>simongray</w>Ooh, that Clojure data Prolog style syntax looks interesting.</z><z id="t1613382218" t="simongray I was asking about Zach Oakes&apos; O&apos;Doyle Rules, though, which uses Datomic-style triples/facts, but otherwise doesn&apos;t have anything to do with Datomic."><y>#</y><d>2021-02-15</d><h>09:43</h><w>simongray</w>I was asking about Zach Oakes&apos; O&apos;Doyle Rules, though, which uses Datomic-style triples/facts, but otherwise doesn&apos;t have anything to do with Datomic.</z><z id="t1613383622" t="rickmoynihan ah sorry — I don’t know… IIRC clara is based on the RETE algorithm… so you might learn more by looking at the differences between rete and datalog. Not sure about O’Doyle, I only heard of it the other day… Looking at the README it implies it’s possible to create infinite loops in it though; which means it may be turing complete. Datalog guarantees termination (and consequently so should Naga)."><y>#</y><d>2021-02-15</d><h>10:07</h><w>rickmoynihan</w>ah sorry — I don’t know…  IIRC clara is based on the RETE algorithm… so you might learn more by looking at the differences between rete and datalog.

Not sure about O’Doyle, I only heard of it the other day… Looking at the README it implies it’s possible to create infinite loops in it though; which means it may be turing complete.  Datalog guarantees termination (and consequently so should Naga).</z><z id="t1613422366" t="quoll Datalog can be implemented via RETE"><y>#</y><d>2021-02-15</d><h>20:52</h><r>quoll</r>Datalog can be implemented via RETE</z><z id="t1613572786" t="rickmoynihan I wouldn’t doubt that for a second given that RETE is more powerful than datalog; but consequently misses datalogs safety properties (re termination guarantees etc)"><y>#</y><d>2021-02-17</d><h>14:39</h><r>rickmoynihan</r>I wouldn’t doubt that for a second given that RETE is more powerful than datalog; but consequently misses datalogs safety properties (re termination guarantees etc)</z><z id="t1613580810" t="quoll Definitely. It’s essentially a rule scheduler. Datalog imposes policies that ensure boundary conditions on that scheduling"><y>#</y><d>2021-02-17</d><h>16:53</h><r>quoll</r>Definitely. It’s essentially a rule scheduler. Datalog imposes policies that ensure boundary conditions on that scheduling</z></g><g id="s3"><z id="t1613581698" t="quoll Actually, that was an interesting thing for me to learn about. I implemented RETE, and I needed to configure what each rule did, and what the scheduling dependencies between the rules were. At that point, I was describing all of that using an RDF graph. But the rule descriptions were in Horn clauses (because that’s how everyone wrote them at the time, and I was used to writing them that way too). I was thinking that instead of writing Horn clauses on paper, and then translating from Horn to the graph representation, and entering all the graph data, then I should write a parser that could read Horn clauses, and convert them into the graph representation automatically. The only problem was figuring out the connections between them. But then I had my epiphany… I knew how to automatically find those dependencies! (I can even tell you which street in Chicago that I was walking along when I realized this)."><y>#</y><d>2021-02-17</d><h>17:08</h><r>quoll</r>Actually, that was an interesting thing for me to learn about. I implemented RETE, and I needed to configure what each rule did, and what the scheduling dependencies between the rules were. At that point, I was describing all of that using an RDF graph. But the rule descriptions were in Horn clauses (because that’s how everyone wrote them at the time, and I was used to writing them that way too).
I was thinking that instead of writing Horn clauses on paper, and then translating from Horn to the graph representation, and entering all the graph data, then I should write a parser that could read Horn clauses, and convert them into the graph representation automatically. The only problem was figuring out the connections between them. But then I had my epiphany… I knew how to automatically find those dependencies! (I can even tell you which street in Chicago that I was walking along when I realized this).</z><z id="t1613581786" t="quoll So I implemented that, and it worked really well."><y>#</y><d>2021-02-17</d><h>17:09</h><r>quoll</r>So I implemented that, and it worked really well.</z><z id="t1613581818" t="quoll At this point, I realized that due to some of the things I’d done in the implementation of RETE, that I had a fully compliant Datalog engine"><y>#</y><d>2021-02-17</d><h>17:10</h><r>quoll</r>At this point, I realized that due to some of the things I’d done in the implementation of RETE, that I had a fully compliant Datalog engine</z><z id="t1613581834" t="quoll It was a total surprise 🙂"><y>#</y><d>2021-02-17</d><h>17:10</h><r>quoll</r>It was a total surprise <b>🙂</b></z><z id="t1613581879" t="quoll So I put in the effort to learn more about Datalog"><y>#</y><d>2021-02-17</d><h>17:11</h><r>quoll</r>So I put in the effort to learn more about Datalog</z><z id="t1613581891" t="quoll And my implementation experience informed me a lot"><y>#</y><d>2021-02-17</d><h>17:11</h><r>quoll</r>And my implementation experience informed me a lot</z><z id="t1613582153" t="rickmoynihan Sorry, not following what the epiphany was?"><y>#</y><d>2021-02-17</d><h>17:15</h><r>rickmoynihan</r>Sorry, not following what the epiphany was?</z><z id="t1613582176" t="quoll That I could identify the dependencies between rules automatically."><y>#</y><d>2021-02-17</d><h>17:16</h><r>quoll</r>That I could identify the dependencies between rules automatically.</z><z id="t1613582188" t="quoll Those dependencies are an important part of configuring RETE"><y>#</y><d>2021-02-17</d><h>17:16</h><r>quoll</r>Those dependencies are an important part of configuring RETE</z><z id="t1613582189" t="rickmoynihan ok by querying for them?"><y>#</y><d>2021-02-17</d><h>17:16</h><r>rickmoynihan</r>ok by querying for them?</z><z id="t1613582372" t="quoll No. The form of horn clause is: pred_y(v1,v2) :- pred_x(v3,v4), ... If the head of any clause matches any expression in the body of any clause, then this forms a dependency of the latter on the former"><y>#</y><d>2021-02-17</d><h>17:19</h><r>quoll</r>No. The form of horn clause is:
<pre>pred_y(v1,v2) :- pred_x(v3,v4), ...</pre>
If the head of any clause matches any expression in the body of any clause, then this forms a dependency of the latter on the former</z><z id="t1613582409" t="rickmoynihan yup"><y>#</y><d>2021-02-17</d><h>17:20</h><r>rickmoynihan</r>yup</z><z id="t1613582423" t="rickmoynihan (I did a bit of prolog many moons ago)"><y>#</y><d>2021-02-17</d><h>17:20</h><r>rickmoynihan</r>(I did a bit of prolog many moons ago)</z><z id="t1613582479" t="quoll “Matching” means that one for each of the 3 values in pred(v1,v2) then either they contain identical concrete values (atoms, numbers, etc), or when either is variable"><y>#</y><d>2021-02-17</d><h>17:21</h><r>quoll</r>“Matching” means that one for each of the 3 values in <code>pred(v1,v2)</code> then either they contain identical concrete values (atoms, numbers, etc), or when either is variable</z><z id="t1613582503" t="quoll Note that pred is included in that match, which means that 2nd order expressions are also allowed"><y>#</y><d>2021-02-17</d><h>17:21</h><r>quoll</r>Note that <code>pred</code> is included in that match, which means that 2nd order expressions are also allowed</z><z id="t1613582550" t="quoll and the simple translation of pred(v1,v2) to [v1 pred v2] simplifies this significantly"><y>#</y><d>2021-02-17</d><h>17:22</h><r>quoll</r>and the simple translation of <code>pred(v1,v2)</code> to <code>[v1 pred v2]</code> simplifies this significantly</z><z id="t1613582599" t="quoll Also, pred(v) =&gt; [v :type pred]"><y>#</y><d>2021-02-17</d><h>17:23</h><r>quoll</r>Also, <code>pred(v)</code> =&gt; <code>[v :type pred]</code></z><z id="t1613582620" t="rickmoynihan yes I saw that in naga"><y>#</y><d>2021-02-17</d><h>17:23</h><r>rickmoynihan</r>yes I saw that in naga</z><z id="t1613582782" t="quoll and if you really want it… pred(v1,v2,v3...) =&gt; [v1 pred ?gen2] [?gen2 :tg/first v2] [?gen2 :tg/rest ?gen3] [?gen3 :tg/first v3] …"><y>#</y><d>2021-02-17</d><h>17:26</h><r>quoll</r>and if you really want it…
<code>pred(v1,v2,v3...)</code> =&gt; <code>[v1 pred ?gen2] [?gen2 :tg/first v2] [?gen2 :tg/rest ?gen3] [?gen3 :tg/first v3] …</code></z><z id="t1613582810" t="rickmoynihan rdf lists 😬"><y>#</y><d>2021-02-17</d><h>17:26</h><r>rickmoynihan</r>rdf lists <b>😬</b></z><z id="t1613582815" t="rickmoynihan but yeah makes sense"><y>#</y><d>2021-02-17</d><h>17:26</h><r>rickmoynihan</r>but yeah makes sense</z><z id="t1613582837" t="quoll slightly different… I don’t terminate them with nil. I found it was just not needed"><y>#</y><d>2021-02-17</d><h>17:27</h><r>quoll</r>slightly different… I don’t terminate them with nil. I found it was just not needed</z><z id="t1613582855" t="quoll (this is internally, where I can presume a closed world)"><y>#</y><d>2021-02-17</d><h>17:27</h><r>quoll</r>(this is internally, where I can presume a closed world)</z><z id="t1613582971" t="rickmoynihan Is that the epiphany, that n-ary predicate matching is simplified by reducing everything to triples?"><y>#</y><d>2021-02-17</d><h>17:29</h><r>rickmoynihan</r>Is that the epiphany, that n-ary predicate matching is simplified by reducing everything to triples?</z><z id="t1613582986" t="quoll no"><y>#</y><d>2021-02-17</d><h>17:29</h><r>quoll</r>no</z><z id="t1613583007" t="quoll The epiphany was just that I could find the dependencies automatically"><y>#</y><d>2021-02-17</d><h>17:30</h><r>quoll</r>The epiphany was just that I could find the dependencies automatically</z><z id="t1613583026" t="rickmoynihan oh ok 🙂"><y>#</y><d>2021-02-17</d><h>17:30</h><r>rickmoynihan</r>oh ok <b>🙂</b></z><z id="t1613583040" t="quoll Because everything was unary and binary predicates (expressed in triples), then it was easy to see this"><y>#</y><d>2021-02-17</d><h>17:30</h><r>quoll</r>Because everything was unary and binary predicates (expressed in triples), then it was easy to see this</z><z id="t1613583049" t="rickmoynihan yeah"><y>#</y><d>2021-02-17</d><h>17:30</h><r>rickmoynihan</r>yeah</z><z id="t1613583086" t="quoll when it was n-ary predicates, then it wasn’t clear. But since I was working with the triples architecture it became very obvious"><y>#</y><d>2021-02-17</d><h>17:31</h><r>quoll</r>when it was n-ary predicates, then it wasn’t clear. But since I was working with the triples architecture it became very obvious</z><z id="t1613583091" t="rickmoynihan I guess that’s kind of what I meant by querying for them"><y>#</y><d>2021-02-17</d><h>17:31</h><r>rickmoynihan</r>I guess that’s kind of what I meant by querying for them</z><z id="t1613583154" t="quoll weeeeeeell… yes, but… this was while parsing and creating an internal representation. It wasn’t yet in a graph, and therefore wasn’t queryable"><y>#</y><d>2021-02-17</d><h>17:32</h><r>quoll</r>weeeeeeell… yes, but… this was while parsing and creating an internal representation. It wasn’t yet in a graph, and therefore wasn’t queryable</z><z id="t1613583192" t="quoll Naga is my 3rd implementation of this (my first was in Java, and my second was in Clojure)."><y>#</y><d>2021-02-17</d><h>17:33</h><r>quoll</r>Naga is my 3rd implementation of this (my first was in Java, and my second was in Clojure).</z><z id="t1613583235" t="rickmoynihan ok yeah — I guess you’ll want to unify all the clauses to the same identity when they match."><y>#</y><d>2021-02-17</d><h>17:33</h><r>rickmoynihan</r>ok yeah — I guess you’ll want to unify all the clauses to the same identity when they match.</z><z id="t1613583248" t="quoll The dependency identification in Naga is done here: https://github.com/threatgrid/naga/blob/main/src/naga/engine.cljc#L149"><y>#</y><d>2021-02-17</d><h>17:34</h><r>quoll</r>The dependency identification in Naga is done here:
<a href="https://github.com/threatgrid/naga/blob/main/src/naga/engine.cljc#L149" target="_blank">https://github.com/threatgrid/naga/blob/main/src/naga/engine.cljc#L149</a></z><z id="t1613583340" t="quoll Recursive work through the body is done in the function https://github.com/threatgrid/naga/blob/main/src/naga/rules.cljc#L174"><y>#</y><d>2021-02-17</d><h>17:35</h><r>quoll</r>Recursive work through the body is done in the function
<a href="https://github.com/threatgrid/naga/blob/main/src/naga/rules.cljc#L174" target="_blank">https://github.com/threatgrid/naga/blob/main/src/naga/rules.cljc#L174</a></z><z id="t1613583382" t="quoll Back when it was simply: pred1(v1,v2), pred2(v3,v4)… then it was much easier"><y>#</y><d>2021-02-17</d><h>17:36</h><r>quoll</r>Back when it was simply: <code>pred1(v1,v2), pred2(v3,v4)…</code> then it was much easier</z><z id="t1613583438" t="rickmoynihan thanks for the explanation 🙇"><y>#</y><d>2021-02-17</d><h>17:37</h><r>rickmoynihan</r>thanks for the explanation <b>🙇</b></z><z id="t1613583456" t="quoll but now the body can be: [?a :attr ?b] [?b :attr2 &quot;x&quot;] (not (or [?b :attr3 &quot;y&quot;] [?a :attr3 &quot;z&quot;]))"><y>#</y><d>2021-02-17</d><h>17:37</h><r>quoll</r>but now the body can be:
<code>[?a :attr ?b] [?b :attr2 &quot;x&quot;] (not (or [?b :attr3 &quot;y&quot;] [?a :attr3 &quot;z&quot;]))</code></z><z id="t1613583492" t="quoll Also, the head can contain multiple statements as well. So that’s an extra complexity 🙂"><y>#</y><d>2021-02-17</d><h>17:38</h><r>quoll</r>Also, the head can contain multiple statements as well. So that’s an extra complexity <b>🙂</b></z><z id="t1613583685" t="quoll But yeah… you’ll see that the code I just gave you can process rules that have bodies with expressions like this. No “graph” is created to describe the rule, just standard Clojure structures."><y>#</y><d>2021-02-17</d><h>17:41</h><r>quoll</r>But yeah… you’ll see that the code I just gave you can process rules that have bodies with expressions like this. No “graph” is created to describe the rule, just standard Clojure structures.</z><z id="t1613583715" t="quoll It has shortcomings, but in general I’m a little proud of it 😊"><y>#</y><d>2021-02-17</d><h>17:41</h><r>quoll</r>It has shortcomings, but in general I’m a little proud of it <b>😊</b></z><z id="t1613583754" t="rickmoynihan ah ok so the rules aren’t actually stored in the graph"><y>#</y><d>2021-02-17</d><h>17:42</h><r>rickmoynihan</r>ah ok so the rules aren’t actually stored in the graph</z><z id="t1613583784" t="rickmoynihan you just use the representation to materialise the RETE network"><y>#</y><d>2021-02-17</d><h>17:43</h><r>rickmoynihan</r>you just use the representation to materialise the RETE network</z><z id="t1613583786" t="quoll correct"><y>#</y><d>2021-02-17</d><h>17:43</h><r>quoll</r>correct</z><z id="t1613583805" t="quoll Though, that very first implementation in Java did so."><y>#</y><d>2021-02-17</d><h>17:43</h><r>quoll</r>Though, that very first implementation in Java did so.</z><z id="t1613583845" t="quoll But it did the whole analysis of the dependencies first, and only when it had all of the information, then it inserted it all into the graph in one step"><y>#</y><d>2021-02-17</d><h>17:44</h><r>quoll</r>But it did the whole analysis of the dependencies first, and only when it had all of the information, then it inserted it all into the graph in one step</z><z id="t1613583932" t="quoll Strictly speaking, it could have done it in 2 steps… insert the basic rules first, then use queries to extract what was needed to determine dependencies, and then insert those dependencies. But it would actually put more work on the system to do that"><y>#</y><d>2021-02-17</d><h>17:45</h><r>quoll</r>Strictly speaking, it could have done it in 2 steps… insert the basic rules first, then use queries to extract what was needed to determine dependencies, and then insert those dependencies. But it would actually put more work on the system to do that</z><z id="t1613583945" t="rickmoynihan I’d imagine it’d be trivial to implement a subset of owl/rdfs in naga"><y>#</y><d>2021-02-17</d><h>17:45</h><r>rickmoynihan</r>I’d imagine it’d be trivial to implement a subset of owl/rdfs in naga</z><z id="t1613584028" t="quoll Here you go. This is skos, using a subset of OWL"><y>#</y><d>2021-02-17</d><h>17:47</h><r>quoll</r>Here you go. This is skos, using a subset of OWL</z><z id="t1613584053" t="quoll RDFS inferencing was actually my test case for the first implementation (in Java)"><y>#</y><d>2021-02-17</d><h>17:47</h><r>quoll</r>RDFS inferencing was actually my test case for the first implementation (in Java)</z><z id="t1613584071" t="rickmoynihan fantastic"><y>#</y><d>2021-02-17</d><h>17:47</h><r>rickmoynihan</r>fantastic</z><z id="t1613584094" t="quoll Most of it is declarative. Lines 126 onwards are the most interesting"><y>#</y><d>2021-02-17</d><h>17:48</h><r>quoll</r>Most of it is declarative. Lines 126 onwards are the most interesting</z><z id="t1613584180" t="quoll Like I said, I was very proud of this 🙂"><y>#</y><d>2021-02-17</d><h>17:49</h><r>quoll</r>Like I said, I was very proud of this <b>🙂</b></z><z id="t1613584197" t="rickmoynihan ah I forgot you can use predicates as variables"><y>#</y><d>2021-02-17</d><h>17:49</h><r>rickmoynihan</r>ah I forgot you can use predicates as variables</z><z id="t1613584224" t="quoll That’s what I was referring to when I mentioned “2nd order expressions”"><y>#</y><d>2021-02-17</d><h>17:50</h><r>quoll</r>That’s what I was referring to when I mentioned “2nd order expressions”</z><z id="t1613584239" t="rickmoynihan yeah that’s handy here"><y>#</y><d>2021-02-17</d><h>17:50</h><r>rickmoynihan</r>yeah that’s handy here</z><z id="t1613584254" t="rickmoynihan I thought those horn clauses looked a little strange at first!"><y>#</y><d>2021-02-17</d><h>17:50</h><r>rickmoynihan</r>I thought those horn clauses looked a little strange at first!</z><z id="t1613584267" t="quoll 😄"><y>#</y><d>2021-02-17</d><h>17:51</h><r>quoll</r><b>😄</b></z><z id="t1613584274" t="quoll They definitely do!"><y>#</y><d>2021-02-17</d><h>17:51</h><r>quoll</r>They definitely do!</z><z id="t1613584285" t="quoll That’s really tough to pull off in Prolog"><y>#</y><d>2021-02-17</d><h>17:51</h><r>quoll</r>That’s really tough to pull off in Prolog</z><z id="t1613584289" t="rickmoynihan does this perform well?"><y>#</y><d>2021-02-17</d><h>17:51</h><r>rickmoynihan</r>does this perform well?</z><z id="t1613584305" t="quoll (though, I’m still learning how to implement Prolog)"><y>#</y><d>2021-02-17</d><h>17:51</h><r>quoll</r>(though, I’m still learning how to implement Prolog)</z><z id="t1613584322" t="rickmoynihan cut is a mind melter"><y>#</y><d>2021-02-17</d><h>17:52</h><r>rickmoynihan</r>cut is a mind melter</z><z id="t1613584330" t="quoll Oh yes. It performs VERY well. After all, it’s well bounded and it runs in RETE"><y>#</y><d>2021-02-17</d><h>17:52</h><r>quoll</r>Oh yes. It performs VERY well. After all, it’s well bounded and it runs in RETE</z><z id="t1613584504" t="quoll My first attempt at building the rules structures were for Mulgara (that skos code was actually from the Mulgara project). The directory containing it all is: https://github.com/quoll/mulgara/tree/master/src/jar/krule/java/org/mulgara/krule"><y>#</y><d>2021-02-17</d><h>17:55</h><r>quoll</r>My first attempt at building the rules structures were for Mulgara (that skos code was actually from the Mulgara project). The directory containing it all is:
<a href="https://github.com/quoll/mulgara/tree/master/src/jar/krule/java/org/mulgara/krule" target="_blank">https://github.com/quoll/mulgara/tree/master/src/jar/krule/java/org/mulgara/krule</a></z><z id="t1613585148" t="rickmoynihan ah thanks for sharing"><y>#</y><d>2021-02-17</d><h>18:05</h><r>rickmoynihan</r>ah thanks for sharing</z><z id="t1613585154" t="rickmoynihan what is the license for those rules?"><y>#</y><d>2021-02-17</d><h>18:05</h><r>rickmoynihan</r>what is the license for those rules?</z><z id="t1613585174" t="quoll It was part of Mulgara, which is OSL"><y>#</y><d>2021-02-17</d><h>18:06</h><r>quoll</r>It was part of Mulgara, which is OSL</z><z id="t1613585174" t="rickmoynihan ok OSL"><y>#</y><d>2021-02-17</d><h>18:06</h><r>rickmoynihan</r>ok OSL</z><z id="t1613585187" t="quoll But it’s copyright to me. I should explicitly CC them"><y>#</y><d>2021-02-17</d><h>18:06</h><r>quoll</r>But it’s copyright to me. I should explicitly CC them</z><z id="t1613585300" t="quoll I want to point out something else, since I’m here. Naga include adapters to different databases, has a priority queue implementation, and various things in other files. But if you keep it to JUST the structure definitions used for rules, and the code that executes the engine, then: $ wc -l rules.cljc engine.cljc schema/structs.cljc 215 rules.cljc 176 engine.cljc 76 schema/structs.cljc 467 total"><y>#</y><d>2021-02-17</d><h>18:08</h><r>quoll</r>I want to point out something else, since I’m here. Naga include adapters to different databases, has a priority queue implementation, and various things in other files. But if you keep it to JUST the structure definitions used for rules, and the code that executes the engine, then:
<pre>$ wc -l rules.cljc engine.cljc schema/structs.cljc 
     215 rules.cljc
     176 engine.cljc
      76 schema/structs.cljc
     467 total</pre></z><z id="t1613585365" t="quoll Meanwhile, the code that defines the structures and executes the engine (with [:attrs nil] features) in Java is: $ wc -l *.java 139 ConsistencyCheck.java 1079 KruleLoader.java 52 KruleStructureException.java 243 QueryStruct.java 389 Rule.java 360 RuleStructure.java 2262 total"><y>#</y><d>2021-02-17</d><h>18:09</h><r>quoll</r>Meanwhile, the code that defines the structures and executes the engine (with <b>far fewer</b> features) in Java is:
<pre>$ wc -l *.java
     139 ConsistencyCheck.java
    1079 KruleLoader.java
      52 KruleStructureException.java
     243 QueryStruct.java
     389 Rule.java
     360 RuleStructure.java
    2262 total</pre></z><z id="t1613585529" t="quoll I had to trace through the Mulgara code to figure out out rules get executed, how they’re loaded, etc, and it took me over 10 minutes. (it’s in a whole lot of other directories). But that’s all glue to incorporate it into the database system. The meat of it is in that single directory. But the whole thing is embarrassingly complex!"><y>#</y><d>2021-02-17</d><h>18:12</h><r>quoll</r>I had to trace through the Mulgara code to figure out out rules get executed, how they’re loaded, etc, and it took me over 10 minutes. (it’s in a whole lot of other directories). But that’s all glue to incorporate it into the database system. The meat of it is in that single directory. But the whole thing is embarrassingly complex!</z><z id="t1613585810" t="quoll That’s my little love letter to Clojure ❤️"><y>#</y><d>2021-02-17</d><h>18:16</h><r>quoll</r>That’s my little love letter to Clojure <b>❤️</b></z><z id="t1613383679" t="rickmoynihan AFAIK you can do the same with RETE (create infinite recursions)"><y>#</y><d>2021-02-15</d><h>10:07</h><w>rickmoynihan</w>AFAIK you can do the same with RETE (create infinite recursions)</z><z id="t1613402183" t="quoll Ah, I just addressed some of this in the thread above (sorry... I’m slow on my phone)"><y>#</y><d>2021-02-15</d><h>15:16</h><w>quoll</w>Ah, I just addressed some of this in the thread above (sorry... I’m slow on my phone)</z><z id="t1613402252" t="quoll Naga was Datalog, with the restriction of binary/unary predicates. (Higher arity actually isn’t hard to do, but I never needed it :woman-shrugging: )"><y>#</y><d>2021-02-15</d><h>15:17</h><w>quoll</w>Naga was Datalog, with the restriction of binary/unary predicates. (Higher arity actually isn’t hard to do, but I never needed it <b>:woman-shrugging:</b>)</z><z id="t1613402280" t="quoll But it’s been extended in ways that are described in the thread"><y>#</y><d>2021-02-15</d><h>15:18</h><w>quoll</w>But it’s been extended in ways that are described in the thread</z><z id="t1613402309" t="quoll Full query syntax is allowed in the rules too."><y>#</y><d>2021-02-15</d><h>15:18</h><w>quoll</w>Full query syntax is allowed in the rules too.</z><z id="t1613402444" t="quoll Including not , or , filters, bindings, etc. The head can also have multiple triples to be asserted for a single match (allowing multiple attributes for a single generated entity)"><y>#</y><d>2021-02-15</d><h>15:20</h><w>quoll</w>Including <code>not</code>, <code>or</code>, filters, bindings, etc. The head can also have multiple triples to be asserted for a single match (allowing multiple attributes for a single generated entity)</z><z id="t1613402780" t="quoll Also, Naga rules are materialized as needed. If you’re using a macro, then yes, that has to be at startup in ClojureScript (because ClojureScript macros are evaluated as Clojure code during compilation), but not in Clojure. But the r macro is just a convenience around the rule function, which takes: • head: a seq of triple patterns • body: a query :where clause expression • name: optional This function can be called at runtime, even in Cljs."><y>#</y><d>2021-02-15</d><h>15:26</h><w>quoll</w>Also, Naga rules are materialized as needed. If you’re using a macro, then yes, that has to be at startup in ClojureScript (because ClojureScript macros are evaluated as Clojure code during compilation), but not in Clojure. But the <code>r</code> macro is just a convenience around the <code>rule</code> function, which takes:
• head: a seq of triple patterns
• body: a query <code>:where</code> clause expression
• name: optional
This function can be called at runtime, even in Cljs.</z><z id="t1613402823" t="quoll We do it at Cisco. Build rules on the fly, then execute them against a connection to Asami"><y>#</y><d>2021-02-15</d><h>15:27</h><w>quoll</w>We do it at Cisco. Build rules on the fly, then execute them against a connection to Asami</z><z id="t1613641603" t="simongray Just wanted to say thank you for this massive explanation 🙂"><y>#</y><d>2021-02-18</d><h>09:46</h><r>simongray</r>Just wanted to say thank you for this massive explanation <b>🙂</b></z><z id="t1613604147" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] : FYI I think there’s a small bug in the naga README. The example code will raise an Unknown storage configuration error. I managed to fix it locally by adding the line: (naga.store-registry/register-storage! :asami naga.storage.asami.core/create-store)"><y>#</y><d>2021-02-17</d><h>23:22</h><w>rickmoynihan</w><a>@quoll</a>: FYI I think there’s a small bug in the naga README.  The example code will raise an <code>Unknown storage configuration</code> error.

I managed to fix it locally by adding the line:

<code>(naga.store-registry/register-storage! :asami naga.storage.asami.core/create-store)</code></z><z id="t1613605008" t="quoll Thank you! This recently changed, so it must have gone stale."><y>#</y><d>2021-02-17</d><h>23:36</h><w>quoll</w>Thank you! This recently changed, so it must have gone stale.</z><z id="t1613605226" t="rickmoynihan Least I can do"><y>#</y><d>2021-02-17</d><h>23:40</h><w>rickmoynihan</w>Least I can do</z><z id="t1613605300" t="quoll Can I please check… did you include the line: (require &apos;[asami.core :as asami]) ?"><y>#</y><d>2021-02-17</d><h>23:41</h><r>quoll</r>Can I please check… did you include the line:
<code>(require &apos;[asami.core :as asami])</code>
?</z><z id="t1613605380" t="quoll Oh, I see what I’ve done. I forgot to include something else"><y>#</y><d>2021-02-17</d><h>23:43</h><r>quoll</r>Oh, I see what I’ve done. I forgot to include something else</z><z id="t1613605392" t="rickmoynihan yes"><y>#</y><d>2021-02-17</d><h>23:43</h><r>rickmoynihan</r>yes</z><z id="t1613605423" t="rickmoynihan side effects eh? 🙂"><y>#</y><d>2021-02-17</d><h>23:43</h><r>rickmoynihan</r>side effects eh? <b>🙂</b></z><z id="t1613605491" t="quoll No, when I ran the example code I’d already required a different namespace, and I forgot to include it in the example"><y>#</y><d>2021-02-17</d><h>23:44</h><r>quoll</r>No, when I ran the example code I’d already required a different namespace, and I forgot to include it in the example</z><z id="t1613605647" t="quoll Try the example script now"><y>#</y><d>2021-02-17</d><h>23:47</h><r>quoll</r>Try the example script now</z><z id="t1613605724" t="rickmoynihan that :thumbsup: works thanks"><y>#</y><d>2021-02-17</d><h>23:48</h><r>rickmoynihan</r>that <b>:thumbsup:</b> works thanks</z><z id="t1613605742" t="rickmoynihan (I meant the side effects to auto register asami on namespace load)"><y>#</y><d>2021-02-17</d><h>23:49</h><r>rickmoynihan</r>(I meant the side effects to auto register asami on namespace load)</z><z id="t1613605836" t="rickmoynihan Incidentally is it possible to essentially do what is in the README, but without using the connection management and mutable database stuff. i.e. to manage asami and naga as pure values myself, or at least put them in atoms I control?"><y>#</y><d>2021-02-17</d><h>23:50</h><r>rickmoynihan</r>Incidentally is it possible to essentially do what is in the README, but without using the connection management and mutable database stuff.

i.e. to manage asami and naga as pure values myself, or at least put them in atoms I control?</z><z id="t1613605877" t="quoll I just added a comment too. That extra line loads the Asami connector. • It registers the factory function • It extends Asami connections to the ConnectionStore protocol • It implements the Naga Storage protocol"><y>#</y><d>2021-02-17</d><h>23:51</h><r>quoll</r>I just added a comment too. That extra line loads the Asami connector.
• It registers the factory function
• It extends Asami connections to the ConnectionStore protocol
• It implements the Naga Storage protocol</z><z id="t1613606203" t="quoll I’ve kinda pushed the value management into the Connection. The Connection actually refers to all the old values of the database, as well as the latest. So if you called (asami/db connection) before running Naga on it then you’ll get the latest value of the database. Afterward, if you use asami/as-of you can still get that same value. It’s actually transparent inside the Connection object. There’s a vector of every database value"><y>#</y><d>2021-02-17</d><h>23:56</h><r>quoll</r>I’ve kinda pushed the value management into the Connection. The Connection actually refers to all the old values of the database, as well as the latest.
So if you called <code>(asami/db connection)</code> before running Naga on it then you’ll get the latest value of the database. Afterward, if you use <code>asami/as-of</code> you can still get that same value. It’s actually transparent inside the Connection object. There’s a vector of every database value</z><z id="t1613606467" t="quoll This seemed to be the most sensible way to manipulate the database. After all, Datomic follows the same paradigm, where connections are executed against, and new values of the database are created that can be retrieved from the connection"><y>#</y><d>2021-02-18</d><h>00:01</h><r>quoll</r>This seemed to be the most sensible way to manipulate the database. After all, Datomic follows the same paradigm, where connections are executed against, and new values of the database are created that can be retrieved from the connection</z><z id="t1613607743" t="rickmoynihan I was thinking more for transient usecases, i.e. where you just want to compute a value… e.g. load some triples into asami, expand the graph with naga rules, and then spit the data or a query result out… without having to engage in resource management etc"><y>#</y><d>2021-02-18</d><h>00:22</h><r>rickmoynihan</r>I was thinking more for transient usecases, i.e. where you just want to compute a value… e.g. load some triples into asami, expand the graph with naga rules, and then spit the data or a query result out… without having to engage in resource management etc</z><z id="t1613607923" t="rickmoynihan e.g. possibly also in the context of a http request… i.e. querying data out of a sparql triple store with constructs, but using asami perhaps with naga in place of a Jena/RDF4j model to build a response."><y>#</y><d>2021-02-18</d><h>00:25</h><r>rickmoynihan</r>e.g. possibly also in the context of a http request… i.e. querying data out of a sparql triple store with constructs, but using asami perhaps with naga in place of a Jena/RDF4j model to build a response.</z><z id="t1613609940" t="quoll In that case, I would just use a graph URI with asami:mem:// for the scheme. It’s basically doing exactly what you just said. (admittedly, the asami:local:// scheme is still a work in progress. So you HAVE to use asami:mem:// for now anyway). There’s no “resource management”, except that the connection holds an atom for the vector of DBs, and when you do a transaction like Naga does, then it just calls update on the maps that make up the latest DB, and does a conj to the vector in the connections atom."><y>#</y><d>2021-02-18</d><h>00:59</h><r>quoll</r>In that case, I would just use a graph URI with <code>asami:mem://</code> for the scheme. It’s basically doing exactly what you just said.
(admittedly, the <code>asami:local://</code> scheme is still a work in progress. So you HAVE to use <code>asami:mem://</code> for now anyway).
There’s no “resource management”, except that the connection holds an atom for the vector of DBs, and when you do a transaction like Naga does, then it just calls <code>update</code> on the maps that make up the latest DB, and does a <code>conj</code> to the vector in the connections atom.</z><z id="t1613609999" t="quoll My colleagues are doing this all the time. Create a memory graph, throw data into it, and use queries to pull out exactly what they want. Then they throw it all away. 😱"><y>#</y><d>2021-02-18</d><h>00:59</h><r>quoll</r>My colleagues are doing this all the time. Create a memory graph, throw data into it, and use queries to pull out exactly what they want. Then they throw it all away. <b>😱</b></z><z id="t1613610022" t="quoll I was a bit shocked to see them do it, but it’s fast, and they find it useful to do!"><y>#</y><d>2021-02-18</d><h>01:00</h><r>quoll</r>I was a bit shocked to see them do it, but it’s fast, and they find it useful to do!</z><z id="t1613639252" t="rickmoynihan moving to #asami"><y>#</y><d>2021-02-18</d><h>09:07</h><r>rickmoynihan</r>moving to #asami</z><z id="t1613606163" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] : One other thing, it looks like the pabu parser silently fails on the -- comments in the skos datalog example you pasted me. Swapping them out for the c-style ones seems to at least convert the program string into data (not got to trying to run it yet), but should I expect it to work in naga?"><y>#</y><d>2021-02-17</d><h>23:56</h><w>rickmoynihan</w><a>@quoll</a>: One other thing, it looks like the pabu parser silently fails on the <code>--</code> comments in the skos datalog example you pasted me.  Swapping them out for the c-style ones seems to at least convert the program string into data (not got to trying to run it yet), but should I expect it to work in naga?</z><z id="t1613606241" t="quoll You should, but I haven’t done much with pabu for a long time. I thought I handled those comments, sorry"><y>#</y><d>2021-02-17</d><h>23:57</h><w>quoll</w>You should, but I haven’t done much with pabu for a long time. I thought I handled those comments, sorry</z><z id="t1613606257" t="quoll (should these questions be in #asami) instead?"><y>#</y><d>2021-02-17</d><h>23:57</h><w>quoll</w>(should these questions be in #asami) instead?</z><z id="t1623313812" t="simongray Anyone know if https://github.com/arachne-framework is totally abandoned? Luke VanderHart seems to be the only real contributor to the code in all of the libs (respect! that’s a lot of code). I am honestly only really looking at https://github.com/arachne-framework/aristotle , but other than development having stopped in 2018, the thing that is really giving me pause is the lack of licences for most (all?) of the libraries in the project. Not knowing the licence is a red flag to me. In principle, that means it is closed source."><y>#</y><d>2021-06-10</d><h>08:30</h><w>simongray</w>Anyone know if <a href="https://github.com/arachne-framework" target="_blank">https://github.com/arachne-framework</a> is totally abandoned? Luke VanderHart seems to be the only real contributor to the code in all of the libs (respect! that’s a lot of code). I am honestly only really looking at <a href="https://github.com/arachne-framework/aristotle" target="_blank">https://github.com/arachne-framework/aristotle</a>, but other than development having stopped in 2018, the thing that is really giving me pause is the lack of licences for most (all?) of the libraries in the project. Not knowing the licence is a red flag to me. In principle, that means it is closed source.</z><z id="t1623331620" t="cldwalker It seems like an oversight. Seems he just puts licenses on his most popular projects e.g. https://github.com/levand/quiescent or https://github.com/arachne-framework/valuehash"><y>#</y><d>2021-06-10</d><h>13:27</h><r>cldwalker</r>It seems like an oversight. Seems he just puts licenses on his most popular projects e.g. <a href="https://github.com/levand/quiescent" target="_blank">https://github.com/levand/quiescent</a> or <a href="https://github.com/arachne-framework/valuehash" target="_blank">https://github.com/arachne-framework/valuehash</a></z><z id="t1623391341" t="simongray Ok, makes sense."><y>#</y><d>2021-06-11</d><h>06:02</h><r>simongray</r>Ok, makes sense.</z><z id="t1623727784" t="Steven Deobald [:attrs {:href &quot;/_/_/users/U4P4NREBY&quot;}] Unlicensed code is closed source, though (&quot;good intentions are not contracts&quot; and all that). If you&apos;re going to use any of it, I&apos;d ask him to license it first."><y>#</y><d>2021-06-15</d><h>03:29</h><r>Steven Deobald</r><a>@U4P4NREBY</a> Unlicensed code is closed source, though (&quot;good intentions are not contracts&quot; and all that). If you&apos;re going to use any of it, I&apos;d ask him to license it first.</z><z id="t1623742086" t="simongray [:attrs {:href &quot;/_/_/users/U01AVNG2XNF&quot;}] Yes, I realise that 🙂 that’s why I mentioned it. Also, the type of licence matters a great deal tool. Anyway, he reached out in the meantime and slapped an Apache licence on that bad boy."><y>#</y><d>2021-06-15</d><h>07:28</h><r>simongray</r><a>@U01AVNG2XNF</a> Yes, I realise that <b>🙂</b> that’s why I mentioned it. Also, the type of licence matters a great deal tool. Anyway, he reached out in the meantime and slapped an Apache licence on that bad boy.</z><z id="t1623323091" t="quoll Luke seems to have cut back on his online time over the pandemic. He’s still on Twitter occasionally, but he hasn’t been active on GitHub at all"><y>#</y><d>2021-06-10</d><h>11:04</h><w>quoll</w>Luke seems to have cut back on his online time over the pandemic. He’s still on Twitter occasionally, but he hasn’t been active on GitHub at all</z><z id="t1623327340" t="simongray I see. Thank you, [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] ."><y>#</y><d>2021-06-10</d><h>12:15</h><w>simongray</w>I see. Thank you, <a>@quoll</a>.</z><z id="t1623332327" t="quoll Luke said that his work has kept him off open source lately, and that he will follow up. He said that everything should be available to use, but this should prompt him to add an EPL"><y>#</y><d>2021-06-10</d><h>13:38</h><w>quoll</w>Luke said that his work has kept him off open source lately, and that he will follow up. He said that everything should be available to use, but this should prompt him to add an EPL</z><z id="t1623332389" t="quoll So perhaps you could continue to “explore” it for now, with the expectation that it will be available soon"><y>#</y><d>2021-06-10</d><h>13:39</h><w>quoll</w>So perhaps you could continue to “explore” it for now, with the expectation that it will be available soon</z><z id="t1623335694" t="simongray Thanks!"><y>#</y><d>2021-06-10</d><h>14:34</h><w>simongray</w>Thanks!</z><z id="t1624320568" t="Eric Scott https://github.com/ont-app/sparql-endpoint"><y>#</y><d>2021-06-22</d><h>00:09</h><w>Eric Scott</w><a href="https://github.com/ont-app/sparql-endpoint" target="_blank">https://github.com/ont-app/sparql-endpoint</a></z><z id="t1624320711" t="Eric Scott [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] - Really enjoyed your interview on the Defn podcast!"><y>#</y><d>2021-06-22</d><h>00:11</h><w>Eric Scott</w><a>@quoll</a> - Really enjoyed your interview on the Defn podcast!</z><z id="t1624322430" t="quoll Thank you. I honestly have no idea what I said!"><y>#</y><d>2021-06-22</d><h>00:40</h><r>quoll</r>Thank you. I honestly have no idea what I said!</z><z id="t1632751682" t="simongray Using OWL, is it legal to define a sub-property of something while increasing the range of the property compared to the parent property?"><y>#</y><d>2021-09-27</d><h>14:08</h><w>simongray</w>Using OWL, is it legal to define a sub-property of something while increasing the range of the property compared to the parent property?</z><z id="t1632751752" t="simongray e.g. can you legally extend something like lexinfo:partOfSpeech which currently has a range of lexinfo:PartOfSpeech"><y>#</y><d>2021-09-27</d><h>14:09</h><w>simongray</w>e.g. can you legally extend something like <code>lexinfo:partOfSpeech</code> which currently has a range of <code>lexinfo:PartOfSpeech</code></z><z id="t1632751871" t="simongray My use case is that I’m extending the Global WordNet Association’s schema, but they sadly chose to deviate from the standard they otherwise follow (Ontolex) on this matter defining their own wn:partOfSpeech relation. I am trying to establish some kind of equivalence between the lexinfo relation and the GWA one."><y>#</y><d>2021-09-27</d><h>14:11</h><w>simongray</w>My use case is that I’m extending the Global WordNet Association’s schema, but they sadly chose to deviate from the standard they otherwise follow (Ontolex) on this matter defining their own <code>wn:partOfSpeech</code> relation. I am trying to establish some kind of equivalence between the lexinfo relation and the GWA one.</z><z id="t1632753164" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U4P4NREBY&quot;}] : Are you basically asking what are the semantics of rdfs:range with regards to owl?"><y>#</y><d>2021-09-27</d><h>14:32</h><w>rickmoynihan</w><a>@simongray</a>: Are you basically asking what are the semantics of <code>rdfs:range</code> with regards to owl?</z><z id="t1632753492" t="simongray maybe… 😛 I’m trying to figure out whether doing this would blow up some software"><y>#</y><d>2021-09-27</d><h>14:38</h><w>simongray</w>maybe… <b>😛</b> I’m trying to figure out whether doing this would blow up some software</z><z id="t1632753956" t="rickmoynihan I’m no expert; but my understanding is that generally speaking owl will let you do that because AFAIK the semantics are defined in terms of the entailments, and things are only really illegal if they lead to contradictions which will depend on your domain model. So the semantics are I think somewhat subtle…. Firstly as I understand it the semantics of multiple ranges are given: P rdfs:range A, B . foo P bar Then foo a A, B . Which means that foo is in the intersection of classes A and B. As to whether that works for you or not with the right semantics will depend on what the software is doing. Strictly speaking the interpretation under OWL of the entailment won’t be broader; because it will be interpreted as the intersection of A and B."><y>#</y><d>2021-09-27</d><h>14:45</h><w>rickmoynihan</w>I’m no expert; but my understanding is that generally speaking owl will let you do that because AFAIK the semantics are defined in terms of the entailments, and things are only really illegal if they lead to contradictions which will depend on your domain model.

So the semantics are I think somewhat subtle….  Firstly as I understand it the semantics of multiple ranges are given:

<code>P rdfs:range A, B .  foo P bar</code>

Then <code>foo a A, B</code>.  Which means that foo is in the intersection of classes A and B.

As to whether that works for you or not with the right semantics will depend on what the software is doing.  Strictly speaking the interpretation under OWL of the entailment won’t be broader; because it will be interpreted as the intersection of A and B.</z><z id="t1632754013" t="rickmoynihan It’ll only be illegal in OWL if some disjointness conditions are triggered… i.e. if it’s inconsistent with guarantees made in the ontologies."><y>#</y><d>2021-09-27</d><h>14:46</h><w>rickmoynihan</w>It’ll only be illegal in OWL if some disjointness conditions are triggered… i.e. if it’s inconsistent with guarantees made in the ontologies.</z><z id="t1632756465" t="quoll On my phone, so I can&apos;t type much. RDFS is completely open world. You can say almost anything with rdfs:range and not be inconsistent. As for making a subproperty, it must abide by everything that applies to the super property. If S is a subproperty of P , P has range A , and S has range A Union B then this implies that B is a subclass of A"><y>#</y><d>2021-09-27</d><h>15:27</h><w>quoll</w>On my phone, so I can&apos;t type much.
RDFS is completely open world. You can say almost anything with <code>rdfs:range</code> and not be inconsistent.

As for making a subproperty, it must abide by everything that applies to the super property. If <code>S</code> is a subproperty of  <code>P</code>, <code>P</code> has range <code>A</code>, and <code>S</code> has range <code>A</code> Union <code>B</code> then this implies that <code>B</code> is a subclass of <code>A</code></z><z id="t1632756576" t="quoll It can&apos;t allow for P to have a broader range, since &lt;P rdfs:range A&gt; means that any use of P implies the object is of type A"><y>#</y><d>2021-09-27</d><h>15:29</h><w>quoll</w>It can&apos;t allow for <code>P</code> to have a broader range, since <code>&lt;P rdfs:range A&gt;</code> means that any use of <code>P</code> implies the object is of type <code>A</code></z><z id="t1632756632" t="quoll And any use of &lt;x S y&gt; implies the statement &lt;x P y&gt;"><y>#</y><d>2021-09-27</d><h>15:30</h><w>quoll</w>And any use of <code>&lt;x S y&gt;</code> implies the statement <code>&lt;x P y&gt;</code></z><z id="t1632815562" t="simongray Right, that’s what I thought"><y>#</y><d>2021-09-28</d><h>07:52</h><w>simongray</w>Right, that’s what I thought</z><z id="t1632815569" t="simongray Thanks a lot, both of you"><y>#</y><d>2021-09-28</d><h>07:52</h><w>simongray</w>Thanks a lot, both of you</z><z id="t1632817503" t="simongray I also ran this by John McCrae from the Global WordNet Foundation and he said that the way to get around this is just to define the range of the owl:subProperty as owl:subClassOf too. Then you abide by the original range restriction of the parent property. Then to infer the parent property triples you should simple define owl:sameAs :ParentClass for any relevant instances of the subClass."><y>#</y><d>2021-09-28</d><h>08:25</h><w>simongray</w>I also ran this by John McCrae from the Global WordNet Foundation and he said that the way to get around this is just to define the range of the owl:subProperty as owl:subClassOf too. Then you abide by the original range restriction of the parent property. Then to infer the parent property triples you should simple define <code>owl:sameAs :ParentClass</code> for any relevant instances of the subClass.</z><z id="t1632825153" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U4P4NREBY&quot;}] yes, defining :SubClass rdfs:subClassOf :ParentClass and defining a new property :P rdfs:range :ParentClass is a valid thing to do. However your owl:sameAs statement doesn’t make sense to me it seems you’re mixing up instances and classes."><y>#</y><d>2021-09-28</d><h>10:32</h><w>rickmoynihan</w><a>@simongray</a> yes, defining  <code>:SubClass rdfs:subClassOf :ParentClass</code> and defining a new property <code>:P rdfs:range :ParentClass</code> is a valid thing to do.  However your <code>owl:sameAs</code> statement doesn’t make sense to me it seems you’re mixing up instances and classes.</z><z id="t1632825667" t="simongray I guess I meant to write :ParentClassInstance rather than :ParentClass"><y>#</y><d>2021-09-28</d><h>10:41</h><w>simongray</w>I guess I meant to write <code>:ParentClassInstance</code> rather than <code>:ParentClass</code></z><z id="t1635774062" t="Eric Scott https://www.reddit.com/r/semanticweb/comments/qke4gu/til_how_to_roundtrip_blank_nodes_in_jenafuseki/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3"><y>#</y><d>2021-11-01</d><h>13:41</h><w>Eric Scott</w><a href="https://www.reddit.com/r/semanticweb/comments/qke4gu/til_how_to_roundtrip_blank_nodes_in_jenafuseki/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3" target="_blank">https://www.reddit.com/r/semanticweb/comments/qke4gu/til_how_to_roundtrip_blank_nodes_in_jenafuseki/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3</a></z><z id="t1636637441" t="simongray [:attrs {:href &quot;/_/_/users/UB3R8UYA1&quot;}] Very helpful"><y>#</y><d>2021-11-11</d><h>13:30</h><w>simongray</w><a>@eric.d.scott</a> Very helpful</z><z id="t1636679620" t="Eric Scott Here&apos;s a new version of ont-app/igraph-jena which supports round-tripping of blank nodes:"><y>#</y><d>2021-11-12</d><h>01:13</h><w>Eric Scott</w>Here&apos;s a new version of ont-app/igraph-jena which supports round-tripping of blank nodes:</z><z id="t1636679624" t="Eric Scott https://github.com/ont-app/igraph-jena"><y>#</y><d>2021-11-12</d><h>01:13</h><w>Eric Scott</w><a href="https://github.com/ont-app/igraph-jena" target="_blank">https://github.com/ont-app/igraph-jena</a></z><z id="t1639337217" t="Eric Scott Just released version 0.1.4 of https://github.com/ont-app/vocabulary . - contains utilities to map between namespaced Clojure keywords and RDF-style URIs. - has support for #lstr &quot;language reader macro. Thanks to Mathieu Lirzin ( https://github.com/mthl ) for his contribution making the dependencies of this project much lighter-weight."><y>#</y><d>2021-12-12</d><h>19:26</h><w>Eric Scott</w>Just released version 0.1.4 of <a href="https://github.com/ont-app/vocabulary" target="_blank">https://github.com/ont-app/vocabulary</a>.

- contains utilities to map between namespaced Clojure keywords and RDF-style URIs.
- has support for <code>#lstr &quot;language </code> reader macro.

  Thanks to Mathieu Lirzin (<a href="https://github.com/mthl" target="_blank">https://github.com/mthl</a>) for his
  contribution making the dependencies of this project much
  lighter-weight.</z><z id="t1640941376" t="simongray I’ll update my deps once I’m back at work"><y>#</y><d>2022-12-31</d><h>09:02</h><w>simongray</w>I’ll update my deps once I’m back at work</z><z id="t1643707103" t="simongray been fed so much propaganda about how Dublin Core Metadata is the be-all and end-all of data preservation from some of its proponents here at the university where I work, that I can’t help feeling about gleeful that all of those supposedly Permanent URLs(tm) used for the DC namespaces are returning 500 errors for me 😛 to be fair, the ideas are commendable, but in the past year or so I’ve observed several outages attempting to access some popular semantic web namespaces that the whole foundation is starting to seem a a bit shaky…"><y>#</y><d>2022-02-01</d><h>09:18</h><w>simongray</w>been fed so much propaganda about how Dublin Core Metadata is the be-all and end-all of data preservation from some of its proponents here at the university where I work, that I can’t help feeling about gleeful that all of those supposedly Permanent URLs(tm) used for the DC namespaces are returning 500 errors for me <b>😛</b> to be fair, the ideas are commendable, but in the past year or so I’ve observed several outages attempting to access some popular semantic web namespaces that the whole foundation is starting to seem a a bit shaky…</z><z id="t1643711815" t="rickmoynihan 😬 https://dublincore.org/"><y>#</y><d>2022-02-01</d><h>10:36</h><w>rickmoynihan</w><b>😬</b> <a href="https://dublincore.org/" target="_blank">https://dublincore.org/</a></z><z id="t1643717540" t="Ivan people say that if everything was based on dat permanent urls would be more realistic"><y>#</y><d>2022-02-01</d><h>12:12</h><w>Ivan</w>people say that if everything was based on <code>dat</code> permanent urls would be more realistic</z><z id="t1643731342" t="rickmoynihan Looks like it was just an outage / blip."><y>#</y><d>2022-02-01</d><h>16:02</h><w>rickmoynihan</w>Looks like it was just an outage / blip.</z><z id="t1643734483" t="quoll I thought Cloudflare should be caching the dc docs? They’re static"><y>#</y><d>2022-02-01</d><h>16:54</h><w>quoll</w>I thought Cloudflare should be caching the dc docs? They’re static</z><z id="t1643793097" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] They were/are caching the dc docs. I don’t know what the issue was; but there was a cloud flare error page being served. It looked to me like the kind of error you get when you don’t pay your hosting bill."><y>#</y><d>2022-02-02</d><h>09:11</h><w>rickmoynihan</w><a>@quoll</a> They were/are caching the dc docs.  I don’t know what the issue was; but there was a cloud flare error page being served.  It looked to me like the kind of error you get when you don’t pay your hosting bill.</z><z id="t1643793174" t="rickmoynihan oh actually I think I remember, it was a 525 error code — so an SSL error between cloudflare and the origin server. I guess a cert expired, or something like that."><y>#</y><d>2022-02-02</d><h>09:12</h><w>rickmoynihan</w>oh actually I think I remember, it was a 525 error code — so an SSL error between cloudflare and the origin server.

I guess a cert expired, or something like that.</z><z id="t1645004111" t="rickmoynihan If you’ve not seen it, this looks great: https://clojurians.slack.com/archives/C06MAR553/p1644949288597739"><y>#</y><d>2022-02-16</d><h>09:35</h><w>rickmoynihan</w>If you’ve not seen it, this looks great:

<a href="https://clojurians.slack.com/archives/C06MAR553/p1644949288597739" target="_blank">https://clojurians.slack.com/archives/C06MAR553/p1644949288597739</a></z><z id="t1645004383" t="simongray [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] Wow, that looks fantastic. I’ve been using the lisp-like-yet-superficially-datomic’ish query language of Aristotle and it works well for simple queries, but gets unwieldy for complex ones. Having to use SPARQL is a regression in other ways, so this solves that problem in a neat way."><y>#</y><d>2022-02-16</d><h>09:39</h><w>simongray</w><a>@rickmoynihan</a> Wow, that looks fantastic. I’ve been using the lisp-like-yet-superficially-datomic’ish query language of Aristotle and it works well for simple queries, but gets unwieldy for complex ones. Having to use SPARQL is a regression in other ways, so this solves that problem in a neat way.</z><z id="t1645024207" t="Kelvin Going off of what you said, I wrote this lib because I was annoyed that there was literally no viable DSL for SPARQL or RDF queries except for Aristotle, which we previously used but quickly ran into its limitations."><y>#</y><d>2022-02-16</d><h>15:10</h><r>Kelvin</r>Going off of what you said, I wrote this lib because I was annoyed that there was literally no viable DSL for SPARQL or RDF queries except for Aristotle, which we previously used but quickly ran into its limitations.</z><z id="t1645024833" t="simongray Aha, so you get precisely what I mean. Thanks for making this! How stable would you say it is?"><y>#</y><d>2022-02-16</d><h>15:20</h><r>simongray</r>Aha, so you get precisely what I mean. Thanks for making this! How stable would you say it is?</z><z id="t1645025395" t="Kelvin We&apos;re currently using Flint on our current projects at our company, so it&apos;s definitely stable enough."><y>#</y><d>2022-02-16</d><h>15:29</h><r>Kelvin</r>We&apos;re currently using Flint on our current projects at our company, so it&apos;s definitely stable enough.</z><z id="t1645025415" t="Kelvin Though of course as a new library there will be bugs that&apos;ll be uncovered and will need to be ironed out."><y>#</y><d>2022-02-16</d><h>15:30</h><r>Kelvin</r>Though of course as a new library there will be bugs that&apos;ll be uncovered and will need to be ironed out.</z><z id="t1645024053" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U02FU7RMG8M&quot;}] Picking up from the discussion here: https://clojurians.slack.com/archives/C06MAR553/p1645003835679289?thread_ts=1644949288.597739&amp;amp;cid=C06MAR553 https://clojurians.slack.com/archives/C06MAR553/p1645022454071689?thread_ts=1644949288.597739&amp;amp;cid=C06MAR553"><y>#</y><d>2022-02-16</d><h>15:07</h><w>rickmoynihan</w><a>@kelvin063</a> Picking up from the discussion here:
<a href="https://clojurians.slack.com/archives/C06MAR553/p1645003835679289?thread_ts=1644949288.597739&amp;amp;cid=C06MAR553" target="_blank">https://clojurians.slack.com/archives/C06MAR553/p1645003835679289?thread_ts=1644949288.597739&amp;amp;cid=C06MAR553</a>

<a href="https://clojurians.slack.com/archives/C06MAR553/p1645022454071689?thread_ts=1644949288.597739&amp;amp;cid=C06MAR553" target="_blank">https://clojurians.slack.com/archives/C06MAR553/p1645022454071689?thread_ts=1644949288.597739&amp;amp;cid=C06MAR553</a></z><z id="t1645024153" t="rickmoynihan Regarding 1. use of protocols etc… I absolutely agree flint should remain independent of any particular backend. This is one the things I wanted for such a lib too! However that is different to having clean type interop with arbitrary backends via protocols."><y>#</y><d>2022-02-16</d><h>15:09</h><w>rickmoynihan</w>Regarding 1. use of protocols etc…

I absolutely agree flint should remain independent of any particular backend.  This is one the things I wanted for such a lib too!

However that is different to having clean type interop with arbitrary backends via protocols.</z><z id="t1645024306" t="rickmoynihan There are a few ways you could do this… One is to just provide the protocols and extend them to the types you already use; but allow others to extend them to rdf4j/jena/whatever-js-lib you want etc."><y>#</y><d>2022-02-16</d><h>15:11</h><w>rickmoynihan</w>There are a few ways you could do this…

One is to just provide the protocols and extend them to the types you already use; but allow others to extend them to rdf4j/jena/whatever-js-lib you want etc.</z><z id="t1645024370" t="rickmoynihan You can also do conditional requires, that test if certain classes/libraries etc are available and load the protocols when they are."><y>#</y><d>2022-02-16</d><h>15:12</h><w>rickmoynihan</w>You can also do conditional requires, that test if certain classes/libraries etc are available and load the protocols when they are.</z><z id="t1645024534" t="rickmoynihan for example grafter (an old clojure lib for rdf that I still maintain) provides quite extensive type coercions for java and clojure types into xsd… e.g. java.time.LocalDate is an xsd:date ; but java.time.LocalDateTime is an xsd:dateTime"><y>#</y><d>2022-02-16</d><h>15:15</h><w>rickmoynihan</w>for example grafter (an old clojure lib for rdf that I still maintain) provides quite extensive type coercions for java and clojure types into xsd…

e.g. <code>java.time.LocalDate</code> is an <code>xsd:date</code>; but <code>java.time.LocalDateTime</code> is an <code>xsd:dateTime</code></z><z id="t1645024566" t="Kelvin So the protocol approach might look something like this? (defprotocol RDFDataType (-to-string [x])) (defrecord MyCustomDataType [value] RDFDataType (-to-string [x] (str (:value x)))"><y>#</y><d>2022-02-16</d><h>15:16</h><w>Kelvin</w>So the protocol approach might look something like this?
<pre>(defprotocol RDFDataType
  (-to-string [x]))

(defrecord MyCustomDataType [value]
  RDFDataType
  (-to-string [x] (str (:value x)))</pre></z><z id="t1645024971" t="rickmoynihan Yes, though you may also want a datatype-uri protocol too, that provides just the datatype URIs"><y>#</y><d>2022-02-16</d><h>15:22</h><w>rickmoynihan</w>Yes, though you may also want a <code>datatype-uri</code> protocol too, that provides just the datatype URIs</z><z id="t1645025781" t="rickmoynihan We actually have a similar library to this btw; that a colleague of mine wrote; not yet open sourced. The main difference is that it provides a thin layer of macros to aid query building / binding runtime values into the query etc. It more or less provides full sparql 1.1 support. I’m personally on the fence about data vs macros; there are pros and cons. It’s interesting anyway because it works on top of a fine grained data AST. Anyway I mention it because the usecase I often want to support is query generation and splicing runtime values into queries etc… Which is why I was advocating protocols and not representing URIs as strings; in particular though I was wondering if the reason you have the “&lt; https://foo/ &gt;” syntax instead of using (URI. &quot;&quot;) is essentially because of quoting?"><y>#</y><d>2022-02-16</d><h>15:36</h><w>rickmoynihan</w>We actually have a similar library to this btw; that a colleague of mine wrote; not yet open sourced.

The main difference is that it provides a thin layer of macros to aid query building / binding runtime values into the query etc.  It more or less provides full sparql 1.1 support.  I’m personally on the fence about data vs macros; there are pros and cons.

It’s interesting anyway because it works on top of a fine grained data AST.

Anyway I mention it because the usecase I often want to support is query generation and splicing runtime values into queries etc…  Which is why I was advocating protocols and not representing URIs as strings; in particular though I was wondering if the reason you have the “&lt;<a href="https://foo/" target="_blank">https://foo/</a>&gt;”  syntax instead of using <code>(URI. &quot;&quot;)</code> is essentially because of quoting?</z><z id="t1645107700" t="Kelvin So there were a couple of reasons I went with the angle bracket syntax: 1. The thought of using types/protocols simply did not occur to me, as I said in the main thread. 2. I wanted to have a SPARQL-like feel when using Flint, hence variables start with ? , blank nodes start with _ , and yes IRIs are surrounded by angle brackets. 3. Related to the above, I thought that the angle bracket syntax was the easiest for a user to grasp, especially if they already had knowledge of SPARQL. 4. Specific to URI but I don&apos;t want to be dependent on Java classes like that (not really a problem with custom protocols but then that&apos;s another thing for the user to become familiar with)."><y>#</y><d>2022-02-17</d><h>14:21</h><r>Kelvin</r>So there were a couple of reasons I went with the angle bracket syntax:
1. The thought of using types/protocols simply did not occur to me, as I said in the main thread.
2. I wanted to have a SPARQL-like feel when using Flint, hence variables start with <code>?</code>, blank nodes start with <code>_</code>, and yes IRIs are surrounded by angle brackets.
3. Related to the above, I thought that the angle bracket syntax was the easiest for a user to grasp, especially if they already had knowledge of SPARQL.
4. Specific to <code>URI</code> but I don&apos;t want to be dependent on Java classes like that (not really a problem with custom protocols but then that&apos;s another thing for the user to become familiar with).</z><z id="t1645113940" t="rickmoynihan Firstly, I’m really sorry if I’m coming across as belligerent; I genuinely LOVE everything you’ve done with flint, except this small small detail. Again I’m very grateful for you sharing this! 🙇 &gt; 2. I wanted to have a SPARQL-like feel when using Flint, hence variables start with ?, blank nodes start with _, and yes IRIs are surrounded by angle brackets. Yes, I totally love this. I just feel like the problem is that URI’s are the odd ones out. i.e. all of your other types are proper ones… e.g. you don’t parse strings to check if they’re xsd:dateTime ’s, you map the underlying #inst datatype. Years ago in an early version of grafter, I did the same; allowing strings to be treated like URI’s in some contexts, and later regretted it a lot. &gt; 4. Specific to URI but I don’t want to be dependent on Java classes like that (not really a problem with custom protocols but then that’s another thing for the user to become familiar with). Coming at this from a different angle; I think if you’re doing serious RDF work you’ll already be using a robust library for properly handling the datatypes somewhere. In the work I do, we care a lot about the correct unambiguous interpretation of data. A lot of the time query results from one query will be used in a subsequent query; and having to coerce those types is a problem. I think a good division of responsibilities is for flint to provide protocols for that coercion to be delegated to a library like this and call it at the right time (as late as possible). You don’t need to pick a backend or favour any particular one (I’d certainly rather you remained agnostic); however supporting builtin platform provided types and mapping them to xsd/rdf where possible makes sense and needn’t contradict this goal. For example adding support for java.net.URI is unambiguous… and cljc would just use goog.Uri a user could optionally then choose to use a library like: https://github.com/henryw374/uri Similarly another dep can always extend support for rdf4j/jena or rdf.js."><y>#</y><d>2022-02-17</d><h>16:05</h><r>rickmoynihan</r>Firstly, I’m really sorry if I’m coming across as belligerent; I genuinely LOVE everything you’ve done with flint, except this small small detail.  Again I’m very grateful for you sharing this!  <b>🙇</b>

&gt; 2. I wanted to have a SPARQL-like feel when using Flint, hence variables start with ?, blank nodes start with _, and yes IRIs are surrounded by angle brackets.
Yes, I totally love this.  I just feel like the problem is that URI’s are the odd ones out.  i.e. all of your other types are proper ones… e.g. you don’t parse strings to check if they’re <code>xsd:dateTime</code>’s, you map the underlying <code>#inst</code> datatype.

Years ago in an early version of grafter, I did the same; allowing strings to be treated like URI’s in some contexts, and later regretted it a lot.

&gt; 4. Specific to URI but I don’t want to be dependent on Java classes like that (not really a problem with custom protocols but then that’s another thing for the user to become familiar with).
Coming at this from a different angle; I think if you’re doing serious RDF work you’ll already be using a robust library for properly handling the datatypes somewhere.  In the work I do, we care a lot about the correct unambiguous interpretation of data.

A lot of the time query results from one query will be used in a subsequent query; and having to coerce those types is a problem.

I think a good division of responsibilities is for flint to provide protocols for that coercion to be delegated to a library like this and call it at the right time (as late as possible).

You don’t need to pick a backend or favour any particular one (I’d certainly rather you remained agnostic); however supporting builtin platform provided types and mapping them to xsd/rdf where possible makes sense and needn’t contradict this goal.

For example adding support for <code>java.net.URI</code> is unambiguous… and cljc would just use <code>goog.Uri</code> a user could optionally then choose to use a library like: <a href="https://github.com/henryw374/uri" target="_blank">https://github.com/henryw374/uri</a>

Similarly another dep can always extend support for rdf4j/jena or rdf.js.</z><z id="t1645114255" t="rickmoynihan incidentally you can resolve this problem: (-&gt; {:select * :where [[(URI. &quot;&quot;) ?p ?o]]} :where first) ;; =&gt; &apos;(URI. &quot;&quot;) By registering a data_reader and doing: {:select * :where [[#flint/uri &quot;&quot; ?p ?o]]}"><y>#</y><d>2022-02-17</d><h>16:10</h><r>rickmoynihan</r>incidentally you can resolve this problem:

<pre>(-&gt; {:select *
 :where [[(URI. &quot;&quot;) ?p ?o]]}
 :where
 first) ;; =&gt; &apos;(URI. &quot;&quot;)</pre>
By registering a data_reader and doing:

<pre>{:select *
 :where [[#flint/uri &quot;&quot; ?p ?o]]}</pre></z><z id="t1645114354" t="rickmoynihan You shouldn’t coin #uri incase clojure does in the future (I’d love it if it did)… but could probably use #f/uri as a namespace 🙂"><y>#</y><d>2022-02-17</d><h>16:12</h><r>rickmoynihan</r>You shouldn’t coin <code>#uri</code> incase clojure does in the future (I’d love it if it did)… but could probably use <code>#f/uri</code> as a namespace <b>🙂</b></z><z id="t1645137919" t="Kelvin First of all, don&apos;t worry about coming off as belligerent; it&apos;s all constructive criticism, so it&apos;s all good. The main thing that&apos;s holding me back (besides obvious time constraints and such) is that this would be a major change to how Flint queries and updates are written, especially if we break backwards compatibility by getting rid of the &lt;&gt; syntax. (Though I guess it&apos;s an argument for &quot;make this change early in the project&apos;s life.&quot;) I&apos;ll have to talk about it with my coworkers who are already using Flint before making such changes."><y>#</y><d>2022-02-17</d><h>22:45</h><r>Kelvin</r>First of all, don&apos;t worry about coming off as belligerent; it&apos;s all constructive criticism, so it&apos;s all good.

The main thing that&apos;s holding me back (besides obvious time constraints and such) is that this would be a major change to how Flint queries and updates are written, especially if we break backwards compatibility by getting rid of the <code>&lt;&gt;</code> syntax. (Though I guess it&apos;s an argument for &quot;make this change early in the project&apos;s life.&quot;) I&apos;ll have to talk about it with my coworkers who are already using Flint before making such changes.</z><z id="t1645138120" t="Kelvin As a side note, the quoting issue was actually never one of the reasons why I chose the &lt;&gt; syntax. In part because there are worse ways to run into quoting issues, e.g. when you&apos;re building Flint queries from the ground up (and in that case I just quote the symbols individually)."><y>#</y><d>2022-02-17</d><h>22:48</h><r>Kelvin</r>As a side note, the quoting issue was actually never one of the reasons why I chose the <code>&lt;&gt;</code> syntax. In part because there are worse ways to run into quoting issues, e.g. when you&apos;re building Flint queries from the ground up (and in that case I just quote the symbols individually).</z><z id="t1645026177" t="rickmoynihan Sorry I’ll restate this… If you used underlying types, which I think would be more precise/secure and ultimately extensible you could write something like this: (flint/format-query (let [dataset (URI. &quot;&quot;)] `{:select ~&apos;* :where [[~dataset ~&apos;?p ~&apos;?o]]}))"><y>#</y><d>2022-02-16</d><h>15:42</h><w>rickmoynihan</w>Sorry I’ll restate this…

If you used underlying types, which I think would be more precise/secure and ultimately extensible you could write something like this:

<pre>(flint/format-query (let [dataset (URI. &quot;&quot;)]
                      `{:select ~&apos;*
                        :where [[~dataset ~&apos;?p ~&apos;?o]]}))</pre></z><z id="t1645026380" t="rickmoynihan Essentially a lot of SPARQL queries we write end up binding variables in the query with some user supplied data etc. So being able to validate that by reading it into a proper type rather than a string is useful and more consistent, and ensures types can be perfectly mapped/preserved into xsd."><y>#</y><d>2022-02-16</d><h>15:46</h><w>rickmoynihan</w>Essentially a lot of SPARQL queries we write end up binding variables in the query with some user supplied data etc.  So being able to validate that by reading it into a proper type rather than a string is useful and more consistent, and ensures types can be perfectly mapped/preserved into xsd.</z><z id="t1645027092" t="rickmoynihan the ~&apos; unquoting is the sort of thing my colleagues macro layer lets you avoid (though I’m not advocating that)"><y>#</y><d>2022-02-16</d><h>15:58</h><w>rickmoynihan</w>the <code>~&apos;</code> unquoting is the sort of thing my colleagues macro layer lets you avoid (though I’m not advocating that)</z><z id="t1645028273" t="quoll This may be really useful for me to do testing with. I’ll be wrapping Asami in RDF/SPARQL soon, since my new job is back in that sphere, and building strings in Clojure is really annoying"><y>#</y><d>2022-02-16</d><h>16:17</h><w>quoll</w>This may be really useful for me to do testing with. I’ll be wrapping Asami in RDF/SPARQL soon, since my new job is back in that sphere, and building strings in Clojure is really annoying</z><z id="t1645106552" t="rickmoynihan whilst you’re here, so to speak… I was wondering if you could elaborate for me what asami/datomic’s approach to the quoting/binding issue with query data is? i.e. you have a query as data like: &apos;{:select * :where [[?s ?p ?o]]} And you want to bind ?s to a specific URI?"><y>#</y><d>2022-02-17</d><h>14:02</h><r>rickmoynihan</r>whilst you’re here, so to speak…

I was wondering if you could elaborate for me what asami/datomic’s approach to the quoting/binding issue with query data is?

i.e. you have a query as data like:

<pre>&apos;{:select *
  :where [[?s ?p ?o]]}</pre>
And you want to bind <code>?s</code> to a specific URI?</z><z id="t1645113336" t="quoll Well, if the ?s is supposed to be bound to a specific URI, then it would be ideal if the pattern were written that way. e.g. if the uri is . Also, I’m going to pretend that I have a uri reader so I can write it in code as #uri &quot;&quot; Then: &apos;{:select * :where [[#uri &quot;&quot; ?p ?o]]} But I appreciate that you want to see a ?s bound in there. In SPARQL, then you’d just say: &apos;{:select [#uri &quot;&quot; :as ?s ?p ?o] :where [[#uri &quot;&quot; ?p ?o]]} But neither of those answer the question 🙂 The way to do it in Asami is: (q &apos;{:find [?s ?p ?o] :in $ ?s :where [[?s ?p ?o]]} the-db #uri &quot;&quot;) Datomic is nearly the same: (q &apos;{:find [?s ?p ?o] :in $ ?s :where [[?p* :db/ident ?p] [?s ?p* ?o]]} the-db #uri &quot;&quot;) "><y>#</y><d>2022-02-17</d><h>15:55</h><r>quoll</r>Well, if the <code>?s</code> is supposed to be bound to a specific URI, then it would be ideal if the pattern were written that way.
e.g. if the uri is <code></code>. Also, I’m going to pretend that I have a uri reader so I can write it in code as <code>#uri &quot;&quot;</code>
Then:
<pre>&apos;{:select *
  :where [[#uri &quot;&quot; ?p ?o]]}</pre>
But I appreciate that you want to see a <code>?s</code> bound in there.

In SPARQL, then you’d just say:
<pre>&apos;{:select [#uri &quot;&quot; :as ?s ?p ?o]
  :where [[#uri &quot;&quot; ?p ?o]]}</pre>
But neither of those answer the question <b>🙂</b>
The way to do it in Asami is:
<pre>(q &apos;{:find [?s ?p ?o]
     :in $ ?s
     :where [[?s ?p ?o]]}
   the-db #uri &quot;&quot;)</pre>
Datomic is nearly the same:
<pre>(q &apos;{:find [?s ?p ?o]
     :in $ ?s
     :where [[?p* :db/ident ?p] [?s ?p* ?o]]}
   the-db #uri &quot;&quot;)</pre>
</z><z id="t1645113531" t="rickmoynihan Thanks… I was actually asking a slightly different question… How would you resolve this (deliberate) class of problem; in the asami/datomic world? (let [s (URI. &quot;&lt;http://s&gt;&quot;)] &apos;{:select * :where [[s ?p ?o]]}) =&gt; {:select * :where [[s ?p ?o]]} ;; &lt;=== not what we want :-)"><y>#</y><d>2022-02-17</d><h>15:58</h><r>rickmoynihan</r>Thanks…  I was actually asking a slightly different question…

How would you resolve this (deliberate) class of problem; in the asami/datomic world?

<pre>(let [s (URI. &quot;&lt;http://s&gt;&quot;)]
   &apos;{:select *
     :where [[s ?p ?o]]})
=&gt; {:select * :where [[s ?p ?o]]} ;; &lt;=== not what we want :-)</pre></z><z id="t1645114477" t="quoll Sorry, I don’t follow. Are you trying to get the * in the select clause to have 3 bindings (with ?s already set to the required URI)? Also, I’m presuming that you have pseudo-code there, since you didn’t unquote the s in the where clause"><y>#</y><d>2022-02-17</d><h>16:14</h><r>quoll</r>Sorry, I don’t follow.
Are you trying to get the <code>*</code> in the <code>select</code> clause to have 3 bindings (with <code>?s</code> already set to the required URI)?
Also, I’m presuming that you have pseudo-code there, since you didn’t unquote the <code>s</code> in the <code>where</code> clause</z><z id="t1645114518" t="rickmoynihan the lack of unquote is the deliberate mistake"><y>#</y><d>2022-02-17</d><h>16:15</h><r>rickmoynihan</r>the lack of unquote is the deliberate mistake</z><z id="t1645114827" t="rickmoynihan i.e. a correct way to do it is here: https://clojurians.slack.com/archives/C09GHBXRC/p1645026177945909 I’m really just asking how people tend do this dynamic query generation stuff with asami/datomic… in particular binding replacement… e.g. another way to do it in a more restricted case is in rdf4j you can take a query like: SELECT * WHERE { ?s ?p ?o } and provide essentially a map of bindings to rebind {&apos;?s (URI. &quot;&quot;) Which will then essentially rewrite that variable in the query."><y>#</y><d>2022-02-17</d><h>16:20</h><r>rickmoynihan</r>i.e. a correct way to do it is here:

<a href="https://clojurians.slack.com/archives/C09GHBXRC/p1645026177945909" target="_blank">https://clojurians.slack.com/archives/C09GHBXRC/p1645026177945909</a>

I’m really just asking how people tend do this dynamic query generation stuff with asami/datomic… in particular binding replacement… e.g. another way to do it in a more restricted case is in rdf4j you can take a query like:

<pre>SELECT * WHERE { ?s ?p ?o }</pre>
and provide essentially a map of bindings to rebind <code>{&apos;?s (URI. &quot;&quot;)</code>

Which will then essentially rewrite that variable in the query.</z><z id="t1645115010" t="rickmoynihan a small issue with the way I linked is that the use of syntax quote makes the query noisier as you need to use ~&apos; to avoid materialising namespace qualified symbols. I’m personally ok with it; but if you do dislike it you can also avoid it with a thin layer of macros."><y>#</y><d>2022-02-17</d><h>16:23</h><r>rickmoynihan</r>a small issue with the way I linked is that the use of syntax quote makes the query noisier as you need to use <code>~&apos;</code> to avoid materialising namespace qualified symbols.

I’m personally ok with it; but if you do dislike it you can also avoid it with a thin layer of macros.</z><z id="t1645115122" t="quoll I’m still a bit confused as to how the binding with the :in clause doesn’t work?"><y>#</y><d>2022-02-17</d><h>16:25</h><r>quoll</r>I’m still a bit confused as to how the binding with the <code>:in</code> clause doesn’t work?</z><z id="t1645115250" t="quoll internally, Asami literally creates a map of {?s (URI. &quot;&quot;)} when you do this"><y>#</y><d>2022-02-17</d><h>16:27</h><r>quoll</r>internally, Asami literally creates a map of <code>{?s (URI. &quot;&quot;)}</code> when you do this</z><z id="t1645115251" t="rickmoynihan ok that bit does answer the question — and is essentially the same approach as in rdf4j etc"><y>#</y><d>2022-02-17</d><h>16:27</h><r>rickmoynihan</r>ok that bit does answer the question — and is essentially the same approach as in rdf4j etc</z><z id="t1645115369" t="rickmoynihan Thanks. I was basically asking, because I suspected the :in clause in datomic/asami did this… I was wondering because I thought this sort of thing might be an acceptable solution for flint — treating URI types more rigorously etc… cc [:attrs {:href &quot;/_/_/users/U02FU7RMG8M&quot;}]"><y>#</y><d>2022-02-17</d><h>16:29</h><r>rickmoynihan</r>Thanks.  I was basically asking, because I suspected the <code>:in</code> clause in datomic/asami did this…

I was wondering because I thought this sort of thing might be an acceptable solution for flint — treating URI types more rigorously etc… cc <a>@kelvin063</a></z><z id="t1645115528" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] is there a way in the api to do that kind of substitution on fragments of a query; or must it always be applied at the top?"><y>#</y><d>2022-02-17</d><h>16:32</h><r>rickmoynihan</r><a>@U051N6TTC</a> is there a way in the api to do that kind of substitution on fragments of a query; or must it always be applied at the top?</z><z id="t1645115559" t="quoll Come to think of it, it’s not quite a map. Asami results are just seqs of vectors, with metadata for the column names. So: ^{:cols &apos;[?s ?p ?o]} ([s1 p1 o1] [s2 p2 o2] [s3 p3 o3]) What actually happens with the binding is that it starts resolving the query with: ^{:cols &apos;[?s]} ([&lt;&gt;]) So it’s a single-row result with one column. This then gets joined to the result of [?s ?p ?o] which shares that ?s variable."><y>#</y><d>2022-02-17</d><h>16:32</h><r>quoll</r>Come to think of it, it’s not quite a map. Asami results are just seqs of vectors, with metadata for the column names. So:
<pre>^{:cols &apos;[?s ?p ?o]}
([s1 p1 o1]
 [s2 p2 o2]
 [s3 p3 o3])</pre>
What actually happens with the binding is that it starts resolving the query with:
<pre>^{:cols &apos;[?s]}
([&lt;&gt;])</pre>
So it’s a single-row result with one column. This then gets joined to the result of <code>[?s ?p ?o]</code> which shares that <code>?s</code> variable.</z><z id="t1645115640" t="rickmoynihan :thumbsup: makes perfect sense"><y>#</y><d>2022-02-17</d><h>16:34</h><r>rickmoynihan</r><b>:thumbsup:</b> makes perfect sense</z><z id="t1645115641" t="quoll Not sure what you mean by “fragments of a query”?"><y>#</y><d>2022-02-17</d><h>16:34</h><r>quoll</r>Not sure what you mean by “fragments of a query”?</z><z id="t1645115693" t="quoll You can provide bindings to start with, so it need not be one-column/one-row bindings. You can pre-bind multiple columns and rows, and start the query from that point"><y>#</y><d>2022-02-17</d><h>16:34</h><r>quoll</r>You can provide bindings to start with, so it need not be one-column/one-row bindings. You can pre-bind multiple columns and rows, and start the query from that point</z><z id="t1645115758" t="rickmoynihan say you’re dynamically generating a query; and have a sub function that will ultimately generate a FILTER NOT EXISTS { bgps } block… but some of those bgps contain lexically scoped bindings you want rewritten"><y>#</y><d>2022-02-17</d><h>16:35</h><r>rickmoynihan</r>say you’re dynamically generating a query; and have a sub function that will ultimately generate a <code>FILTER NOT EXISTS { bgps }</code> block… but some of those bgps contain lexically scoped bindings you want rewritten</z><z id="t1645116484" t="quoll Hmmm, that’s interesting. I’ve never thought about prebinding things you want removed."><y>#</y><d>2022-02-17</d><h>16:48</h><r>quoll</r>Hmmm, that’s interesting. I’ve never thought about prebinding things you want removed.</z><z id="t1645116519" t="rickmoynihan Yeah; it’s a useful trick"><y>#</y><d>2022-02-17</d><h>16:48</h><r>rickmoynihan</r>Yeah; it’s a useful trick</z><z id="t1645116585" t="quoll OK… in Asami there is currently no way. It would be easy enough to do (from a query resolution perspective), but I’d need to figure out the API to get the data into the right place"><y>#</y><d>2022-02-17</d><h>16:49</h><r>quoll</r>OK… in Asami there is currently no way. It would be easy enough to do (from a query resolution perspective), but I’d need to figure out the API to get the data into the right place</z><z id="t1645116621" t="quoll especially if you had, say, 2 or more NOT EXISTS, then how do you get the bindings to the right one? 🙂"><y>#</y><d>2022-02-17</d><h>16:50</h><r>quoll</r>especially if you had, say, 2 or more NOT EXISTS, then how do you get the bindings to the right one? <b>🙂</b></z><z id="t1645116662" t="rickmoynihan lexical scope is one way"><y>#</y><d>2022-02-17</d><h>16:51</h><r>rickmoynihan</r>lexical scope is one way</z><z id="t1645117040" t="rickmoynihan e.g. `{:select * :where [[~(sub-query arg1)] [~(sub-query arg2)]]}"><y>#</y><d>2022-02-17</d><h>16:57</h><r>rickmoynihan</r>e.g.

<pre>`{:select * 
 :where [[~(sub-query arg1)]
         [~(sub-query arg2)]]}</pre></z><z id="t1645117094" t="rickmoynihan then the subqueries internally replace through the discussed mechanism"><y>#</y><d>2022-02-17</d><h>16:58</h><r>rickmoynihan</r>then the subqueries internally replace through the discussed mechanism</z><z id="t1645117882" t="quoll For this kind of thing, I build the query in code, and not as a series of quote/unquoting operations"><y>#</y><d>2022-02-17</d><h>17:11</h><r>quoll</r>For this kind of thing, I build the query in code, and not as a series of quote/unquoting operations</z><z id="t1645117921" t="quoll (assoc query :where (concat first-part second-part)) kind of thing"><y>#</y><d>2022-02-17</d><h>17:12</h><r>quoll</r><code>(assoc query :where (concat first-part second-part))</code> kind of thing</z><z id="t1645118126" t="rickmoynihan yeah at some point of complexity that’s always required; the question is how far can you get without sacrificing too much in the way of intent/expressivity 🙂"><y>#</y><d>2022-02-17</d><h>17:15</h><r>rickmoynihan</r>yeah at some point of complexity that’s always required; the question is how far can you get without sacrificing too much in the way of intent/expressivity <b>🙂</b></z><z id="t1645118184" t="quoll I think it needs to be a part of the query grammar"><y>#</y><d>2022-02-17</d><h>17:16</h><r>quoll</r>I think it needs to be a part of the query grammar</z><z id="t1645106377" t="rickmoynihan Oh, exciting! Whats the new job [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] ?"><y>#</y><d>2022-02-17</d><h>13:59</h><w>rickmoynihan</w>Oh, exciting!  Whats the new job <a>@quoll</a>?</z><z id="t1645112630" t="quoll My new role is “Semantic Architect” at a medical devices company"><y>#</y><d>2022-02-17</d><h>15:43</h><w>quoll</w>My new role is “Semantic Architect” at a medical devices company</z><z id="t1645112683" t="quoll I won’t be using Asami, but I’ll be in the SPARQL/RDF/OWL sphere"><y>#</y><d>2022-02-17</d><h>15:44</h><w>quoll</w>I won’t be using Asami, but I’ll be in the SPARQL/RDF/OWL sphere</z><z id="t1645112694" t="quoll In fact… I won’t be developing software at all!"><y>#</y><d>2022-02-17</d><h>15:44</h><w>quoll</w>In fact… I won’t be developing software at all!</z><z id="t1645112712" t="rickmoynihan Noooo!!!!! 😢 😢 😢 😭 😭 😭"><y>#</y><d>2022-02-17</d><h>15:45</h><w>rickmoynihan</w>Noooo!!!!! <b>😢</b> <b>😢</b> <b>😢</b> <b>😭</b> <b>😭</b> <b>😭</b></z><z id="t1645112716" t="quoll It’ll be my job to design and have others develop"><y>#</y><d>2022-02-17</d><h>15:45</h><w>quoll</w>It’ll be my job to design and have others develop</z><z id="t1645112735" t="rickmoynihan will any of it be in clojure? 🙂"><y>#</y><d>2022-02-17</d><h>15:45</h><w>rickmoynihan</w>will any of it be in clojure? <b>🙂</b></z><z id="t1645112744" t="quoll Not at all"><y>#</y><d>2022-02-17</d><h>15:45</h><w>quoll</w>Not at all</z><z id="t1645112800" t="quoll But, I enjoy writing software. I plan to keep working on Asami in my own time. That’s OK. It hasn’t always been supported by Cisco, and a good chunk of it was written in my own time anyway"><y>#</y><d>2022-02-17</d><h>15:46</h><w>quoll</w>But, I enjoy writing software. I plan to keep working on Asami in my own time. That’s OK. It hasn’t always been supported by Cisco, and a good chunk of it was written in my own time anyway</z><z id="t1645112800" t="rickmoynihan I’m very happy for you; but your work in this area will be missed! 🙂"><y>#</y><d>2022-02-17</d><h>15:46</h><w>rickmoynihan</w>I’m very happy for you; but your work in this area will be missed! <b>🙂</b></z><z id="t1645112986" t="rickmoynihan No idea if this is relevant to you in your new role [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] ; but we recently made this website: https://csvw.org/ And have a standards compliant processor for csv2rdf"><y>#</y><d>2022-02-17</d><h>15:49</h><w>rickmoynihan</w>No idea if this is relevant to you in your new role <a>@quoll</a> ; but we recently made this website:

<a href="https://csvw.org/" target="_blank">https://csvw.org/</a>

And have a standards compliant processor for csv2rdf</z><z id="t1645166287" t="simongray This is very interesting. I need to publish DanNet as both RDF and CSV. For the CSV, I was just going to make a simple 3-column file of all the triples in the graph, but I guess I need to take a look at this?"><y>#</y><d>2022-02-18</d><h>06:38</h><r>simongray</r>This is very interesting. I need to publish DanNet as both RDF and CSV. For the CSV, I was just going to make a simple 3-column file of all the triples in the graph, but I guess I need to take a look at this?</z><z id="t1645176936" t="rickmoynihan Yeah 3 column representations aren’t particularly useful 🙂 You might also find our csvw spec compliant processor useful: https://github.com/Swirrl/csv2rdf"><y>#</y><d>2022-02-18</d><h>09:35</h><r>rickmoynihan</r>Yeah 3 column representations aren’t particularly useful <b>🙂</b>

You might also find our csvw spec compliant processor useful: <a href="https://github.com/Swirrl/csv2rdf" target="_blank">https://github.com/Swirrl/csv2rdf</a></z><z id="t1645113570" t="quoll I have to get refamiliarized with all of this stuff"><y>#</y><d>2022-02-17</d><h>15:59</h><w>quoll</w>I  have to get refamiliarized with all of this stuff</z><z id="t1645113578" t="quoll And I have to learn SHACL"><y>#</y><d>2022-02-17</d><h>15:59</h><w>quoll</w>And I have to learn SHACL</z><z id="t1645115093" t="rickmoynihan SHACL is very similar to clojure spec"><y>#</y><d>2022-02-17</d><h>16:24</h><w>rickmoynihan</w>SHACL is very similar to clojure spec</z><z id="t1645139771" t="Ivan This is also my take on it. Have you seen shapetrees? https://shapetrees.org/"><y>#</y><d>2022-02-17</d><h>23:16</h><r>Ivan</r>This is also my take on it.
Have you seen shapetrees?

<a href="https://shapetrees.org/" target="_blank">https://shapetrees.org/</a></z><z id="t1645177346" t="rickmoynihan I haven’t; what is the TLDR? I’m struggling to see the wood for the trees, if you’ll forgive the pun 😆"><y>#</y><d>2022-02-18</d><h>09:42</h><r>rickmoynihan</r>I haven’t; what is the TLDR?  I’m struggling to see the wood for the trees, if you’ll forgive the pun <b>😆</b></z><z id="t1645116881" t="quoll I have a cursory overview of it. I don’t know specifics though. But it definitely seems to be the pragmatic approach that was missing before"><y>#</y><d>2022-02-17</d><h>16:54</h><w>quoll</w>I have a cursory overview of it. I don’t know specifics though. But it definitely seems to be the pragmatic approach that was missing before</z><z id="t1645120103" t="Steven Deobald Congrats, [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] !"><y>#</y><d>2022-02-17</d><h>17:48</h><w>Steven Deobald</w>Congrats, <a>@quoll</a>!</z><z id="t1645123085" t="Kelvin Congrats! Besides Asami, are there any other Legend of Korra libs you plan to do?"><y>#</y><d>2022-02-17</d><h>18:38</h><w>Kelvin</w>Congrats! Besides Asami, are there any other Legend of Korra libs you plan to do?</z><z id="t1645123447" t="quoll Heh… The whole Avatar theme was actually started by someone else at Cisco, and I just worked in with it. Perhaps I need to come up with my own naming scheme"><y>#</y><d>2022-02-17</d><h>18:44</h><w>quoll</w>Heh… The whole Avatar theme was actually started by someone else at Cisco, and I just worked in with it. Perhaps I need to come up with my own naming scheme</z><z id="t1645125772" t="Steven Deobald Asami (or its new name, if there&apos;s a new scheme) really feels like it deserves a logo &amp; emoji sometime, too. 🙂"><y>#</y><d>2022-02-17</d><h>19:22</h><r>Steven Deobald</r>Asami (or its new name, if there&apos;s a new scheme) really feels like it deserves a logo &amp; emoji sometime, too. <b>🙂</b></z><z id="t1645138171" t="Kelvin As a fan of LoK, I hope that we don&apos;t change Asami&apos;s name."><y>#</y><d>2022-02-17</d><h>22:49</h><r>Kelvin</r>As a fan of LoK, I hope that we don&apos;t change Asami&apos;s name.</z><z id="t1645166287" t="simongray This is very interesting. I need to publish DanNet as both RDF and CSV. For the CSV, I was just going to make a simple 3-column file of all the triples in the graph, but I guess I need to take a look at this?"><y>#</y><d>2022-02-18</d><h>06:38</h><w>simongray</w>This is very interesting. I need to publish DanNet as both RDF and CSV. For the CSV, I was just going to make a simple 3-column file of all the triples in the graph, but I guess I need to take a look at this?</z><z id="t1645194914" t="Eric Scott Wow [:attrs {:href &quot;/_/_/users/U02FU7RMG8M&quot;}] this is great!"><y>#</y><d>2022-02-18</d><h>14:35</h><w>Eric Scott</w>Wow <a>@kelvin063</a> this is great!</z><z id="t1645212976" t="Kelvin Thank you! Indeed, IGraph has been an inspiration for Flint: • It has inspired some of Flint&apos;s syntactic conventions, namely the triple normal form map. • More broadly, Flint and IGraph both share the idea of being a Clojure data layer on top of RDF/SPARQL."><y>#</y><d>2022-02-18</d><h>19:36</h><r>Kelvin</r>Thank you! Indeed, IGraph has been an inspiration for Flint:
• It has inspired some of Flint&apos;s syntactic conventions, namely the triple normal form map.
• More broadly, Flint and IGraph both share the idea of being a Clojure data layer on top of RDF/SPARQL.</z><z id="t1645281210" t="Eric Scott I noticed that you&apos;re using # inst. The ont-app/vocabulary module defines an # lstr reader macro. I know [:attrs {:href &quot;/_/_/users/U4P4NREBY&quot;}] has also found that useful. =ont-app/vocabulary has a bunch of other stuff that might be a bit more weight than you&apos;d like to take on. Would it be of use to you if I broke # lstr into its own project?"><y>#</y><d>2022-02-19</d><h>14:33</h><r>Eric Scott</r>I noticed that you&apos;re using # inst.  The ont-app/vocabulary module defines an # lstr reader macro.  I know <a>@U4P4NREBY</a> has also found that useful.  =ont-app/vocabulary  has a bunch of  other stuff that might be a bit more weight than you&apos;d like to take on.  Would it be of use to you if I broke # lstr into its own project?</z><z id="t1645363087" t="simongray [:attrs {:href &quot;/_/_/users/UB3R8UYA1&quot;}] Yeah, ont-app/vocabulary is really nice, as are your other RDF libs. Speaking of… I just submitted a PR to ont-app/vocabulary with a tiny fix to a bug I found."><y>#</y><d>2022-02-20</d><h>13:18</h><r>simongray</r><a>@UB3R8UYA1</a> Yeah, ont-app/vocabulary is really nice, as are your other RDF libs. Speaking of… I just submitted a PR to ont-app/vocabulary with a tiny fix to a bug I found.</z><z id="t1645364305" t="Eric Scott Oh. Thanks!"><y>#</y><d>2022-02-20</d><h>13:38</h><r>Eric Scott</r>Oh. Thanks!</z><z id="t1645364453" t="Eric Scott It had not occurred to me that a lstr would have \n&apos;s in them."><y>#</y><d>2022-02-20</d><h>13:40</h><r>Eric Scott</r>It had not occurred to me that a lstr would have \n&apos;s in them.</z><z id="t1645364488" t="simongray me neither, but I’ve found it in both ontolex and vann so far"><y>#</y><d>2022-02-20</d><h>13:41</h><r>simongray</r>me neither, but I’ve found it in both ontolex and vann so far</z><z id="t1645364566" t="Eric Scott It does seem that # lstr could live independently of the rest of ont-app/vocabulary, yeah?"><y>#</y><d>2022-02-20</d><h>13:42</h><r>Eric Scott</r>It does seem that # lstr could live independently of the rest of ont-app/vocabulary, yeah?</z><z id="t1645364662" t="simongray I guess it’s a trade-off. I do like mini-libs, but they can also get too small sometimes."><y>#</y><d>2022-02-20</d><h>13:44</h><r>simongray</r>I guess it’s a trade-off. I do like mini-libs, but they can also get too small sometimes.</z><z id="t1645365466" t="Eric Scott Yeah, maybe I&apos;ll let that season for a bit."><y>#</y><d>2022-02-20</d><h>13:57</h><r>Eric Scott</r>Yeah, maybe I&apos;ll let that season for a bit.</z><z id="t1645373307" t="Eric Scott https://github.com/ont-app/vocabulary v. 0.1.6 is now committed! Thanks Simon!"><y>#</y><d>2022-02-20</d><h>16:08</h><r>Eric Scott</r><a href="https://github.com/ont-app/vocabulary" target="_blank">https://github.com/ont-app/vocabulary</a> v. 0.1.6 is now committed! Thanks Simon!</z><z id="t1645373344" t="Eric Scott Strangely (s) is not supported in cljs"><y>#</y><d>2022-02-20</d><h>16:09</h><r>Eric Scott</r>Strangely (s) is not supported in cljs</z><z id="t1645375188" t="simongray it isn’t? hm… that doesn’t solve my issue then… :S"><y>#</y><d>2022-02-20</d><h>16:39</h><r>simongray</r>it isn’t? hm… that doesn’t solve my issue then… :S</z><z id="t1645375396" t="simongray In that case, let me make another PR"><y>#</y><d>2022-02-20</d><h>16:43</h><r>simongray</r>In that case, let me make another PR</z><z id="t1645375484" t="simongray it is an easily solvable problem after all. Initially, I just made a non-capturing group of (?:.|\s) , but then I decided to use the flag instead as it is the “cleaner” solution."><y>#</y><d>2022-02-20</d><h>16:44</h><r>simongray</r>it is an easily solvable problem after all. Initially, I just made a non-capturing group  of <code>(?:.|\s)</code>, but then I decided to use the flag instead as it is the “cleaner” solution.</z><z id="t1645375527" t="simongray perhaps you could put (?:.|\s) instead of . in the CLJS regex?"><y>#</y><d>2022-02-20</d><h>16:45</h><r>simongray</r>perhaps you could put <code>(?:.|\s)</code>instead of <code>.</code> in the CLJS regex?</z><z id="t1645375579" t="simongray that would match any character as well as any whitespace, so essentially the same as a DOTALL ."><y>#</y><d>2022-02-20</d><h>16:46</h><r>simongray</r>that would match any character as well as any whitespace, so essentially the same as a DOTALL <code>.</code></z><z id="t1645375960" t="simongray This should fix it https://github.com/ont-app/vocabulary/pull/17"><y>#</y><d>2022-02-20</d><h>16:52</h><r>simongray</r>This should fix it <a href="https://github.com/ont-app/vocabulary/pull/17" target="_blank">https://github.com/ont-app/vocabulary/pull/17</a></z><z id="t1645377109" t="Eric Scott Awesome! Thanks! Clearly your regex-fu is top-notch. I&apos;ll incorporate this when I get back to my desk."><y>#</y><d>2022-02-20</d><h>17:11</h><r>Eric Scott</r>Awesome! Thanks! Clearly your regex-fu  is top-notch.  I&apos;ll incorporate this when I get back to my desk.</z><z id="t1645381686" t="simongray No rush 😄"><y>#</y><d>2022-02-20</d><h>18:28</h><r>simongray</r>No rush <b>😄</b></z><z id="t1645457094" t="Eric Scott Done. Thanks again!"><y>#</y><d>2022-02-21</d><h>15:24</h><r>Eric Scott</r>Done. Thanks again!</z><z id="t1645434996" t="simongray I think my web version of DanNet has become a pretty nice way to browse any RDF graph. It’s just a basic resource look-up in Apache Jena along with potential rdfs:label values for the matching predicates and objects, coupled with content language negotiation. This seems to work quite well as a way to generate a browsable UI, e.g."><y>#</y><d>2022-02-21</d><h>09:16</h><w>simongray</w>I think my web version of DanNet has become a pretty nice way to browse any RDF graph. It’s just a basic resource look-up in Apache Jena along with potential rdfs:label values for the matching predicates and objects, coupled with content language negotiation. This seems to work quite well as a way to generate a browsable UI, e.g.</z><z id="t1645541544" t="rickmoynihan Ok, I’m trying to determine what the “correct” behaviour (and whether such a thing exists) in this circumstance. And I’m wondering if anyone (in particular [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] — who worked on SPARQL 1.1) has any ideas. Basically RDF defines a bunch of quad formats; e.g. trig/n-quads/json-ld. Of these it turns out that in all the formats the final graph clause is actually optional; in which case the triples are considered to be in the default graph. Now picture an ingestion API where there is a ?graph parameter; and a file of RDF; where the semantics for triples are “to load them into the specified ?graph “. The question is what are the best semantics for the ?graph parameter when loading a file in a quads format, where some of the statements are quads, and some are triples. Should the ?graph parameter: 1. Overwrite all statements and load them into the same ?graph 2. Leave the graphs on quads in the data alone; but apply the ?graph clause to just the triples in the file. I’ve looked at the SPARQL 1.1 graph store protocol, and it doesn’t cover this case. And I can see good arguments for each option. The stardog database in this case takes option 1."><y>#</y><d>2022-02-22</d><h>14:52</h><w>rickmoynihan</w>Ok, I’m trying to determine what the “correct” behaviour (and whether such a thing exists) in this circumstance.

And I’m wondering if anyone (in particular <a>@quoll</a> — who worked on SPARQL 1.1) has any ideas.

Basically RDF defines a bunch of quad formats; e.g. trig/n-quads/json-ld.  Of these it turns out that in all the formats the final graph clause is actually optional; in which case the triples are considered to be in the default graph.

Now picture an ingestion API where there is a <code>?graph</code> parameter; and a file of RDF; where the semantics for triples are “to load them into the specified <code>?graph</code>“.

The question is what are the best semantics for the <code>?graph</code> parameter when loading a file in a quads format, where some of the statements are quads, and some are triples.

Should the <code>?graph</code> parameter:

1. Overwrite all statements and load them into the same <code>?graph</code>
2. Leave the graphs on quads in the data alone; but apply the <code>?graph</code> clause to just the triples in the file.
I’ve looked at the SPARQL 1.1 graph store protocol, and it doesn’t cover this case.  And I can see good arguments for each option.  The stardog database in this case takes option 1.</z><z id="t1645541693" t="rickmoynihan I can’t seem to find analogous behaviour to this specified in any of the RDF standards"><y>#</y><d>2022-02-22</d><h>14:54</h><w>rickmoynihan</w>I can’t seem to find analogous behaviour to this specified in any of the RDF standards</z><z id="t1645541971" t="quoll My gut reaction to this is that the API parameter defines the default graph to load into when a graph is not specified"><y>#</y><d>2022-02-22</d><h>14:59</h><w>quoll</w>My gut reaction to this is that the API parameter defines the default graph to load into when a graph is not specified</z><z id="t1645542074" t="quoll Since the file is saying “Here is the graph that you’re supposed to load into!”"><y>#</y><d>2022-02-22</d><h>15:01</h><w>quoll</w>Since the file is saying “Here is the graph that you’re supposed to load into!”</z><z id="t1645542160" t="quoll But when it isn’t? That’s when you fall back to “default”. And there is already precedent for setting the default in some way in the API (e.g. in queries). So that’s what I’m thinking would be happening here"><y>#</y><d>2022-02-22</d><h>15:02</h><w>quoll</w>But when it isn’t? That’s when you fall back to “default”. And there is already precedent for setting the default in some way in the API (e.g. in queries). So that’s what I’m thinking would be happening here</z><z id="t1645542175" t="quoll It would seem inconsistent otherwise"><y>#</y><d>2022-02-22</d><h>15:02</h><w>quoll</w>It would seem inconsistent otherwise</z><z id="t1645633632" t="rickmoynihan Thanks a million for the reply 🙇 I think I agree with you that this is the best behaviour; and I also thought this was analogous to the default graph and how that often works. However I think the other behaviour is defensibly consistent too. i.e. “if you have a file of triples you’re putting them all into the specified graph”; which is consistent with “if you have a file of quads you’re putting them all into the specified graph”. So I’m not sure the consistency argument helps a lot… for me I think it comes down to which is the most useful behaviour, and I think that the most useful is option 2. Somebody can always find themselves in a situation where they’d prefer it worked the other way; but I think option 1 verges into transformation territory, which isn’t really it’s purpose… so it feels reasonable to require users who need transformation to just do it first."><y>#</y><d>2022-02-23</d><h>16:27</h><r>rickmoynihan</r>Thanks a million for the reply <b>🙇</b>

I think I agree with you that this is the best behaviour; and I also thought this was analogous to the default graph and how that often works.

However I think the other behaviour is defensibly consistent too.  i.e. “if you have a file of triples you’re putting them all into the specified graph”; which is consistent with “if you have a file of quads you’re putting them all into the specified graph”.

So I’m not sure the consistency argument helps a lot… for me I think it comes down to which is the most useful behaviour, and I think that the most useful is option 2.

Somebody can always find themselves in a situation where they’d prefer it worked the other way; but I think option 1 verges into transformation territory, which isn’t really it’s purpose… so it feels reasonable to require users who need transformation to just do it first.</z><z id="t1645638345" t="quoll I was referring to “consistent” in relation to the approach that SPARQL takes. The “default” graph is a thing that can be overridden by the API. However, if a document defines a graph, then that will NEVER be overridden."><y>#</y><d>2022-02-23</d><h>17:45</h><r>quoll</r>I was referring to “consistent” in relation to the approach that SPARQL takes. The “default” graph is a thing that can be overridden by the API. However, if a document defines a graph, then that will NEVER be overridden.</z><z id="t1645638397" t="quoll Basically, if your data says one thing (e.g. in this case, the graph), nothing in SPARQL will ever change it"><y>#</y><d>2022-02-23</d><h>17:46</h><r>quoll</r>Basically, if your data says one thing (e.g. in this case, the graph), nothing in SPARQL will ever change it</z><z id="t1645638447" t="rickmoynihan :thumbsup: yeah that makes sense, thanks for clarifying"><y>#</y><d>2022-02-23</d><h>17:47</h><r>rickmoynihan</r><b>:thumbsup:</b> yeah that makes sense, thanks for clarifying</z><z id="t1645638463" t="quoll np"><y>#</y><d>2022-02-23</d><h>17:47</h><r>quoll</r>np</z><z id="t1645542205" t="quoll If you want to override the graph specified in the file, then I think it should be an explicit “override-graph” parameter in the API"><y>#</y><d>2022-02-22</d><h>15:03</h><w>quoll</w>If you want to override the graph specified in the file, then I think it should be an explicit “override-graph” parameter in the API</z><z id="t1645542209" t="quoll not something ambiguous"><y>#</y><d>2022-02-22</d><h>15:03</h><w>quoll</w>not something ambiguous</z><z id="t1645558751" t="Kelvin Just pushed Flint v0.1.1 up, which consists of a number of bugfixes. (I know some people wanted to know how stable the lib is, so hopefully the fact that it&apos;s being actively maintained is reassuring.) https://github.com/yetanalytics/flint/blob/main/CHANGELOG.md"><y>#</y><d>2022-02-22</d><h>19:39</h><w>Kelvin</w>Just pushed Flint v0.1.1 up, which consists of a number of bugfixes. (I know some people wanted to know how stable the lib is, so hopefully the fact that it&apos;s being actively maintained is reassuring.) <a href="https://github.com/yetanalytics/flint/blob/main/CHANGELOG.md" target="_blank">https://github.com/yetanalytics/flint/blob/main/CHANGELOG.md</a></z><z id="t1646058770" t="Kelvin I know people have mentioned the Fluree project in this channel before, and I stumbled upon them through their https://github.com/fluree/json-ld library."><y>#</y><d>2022-02-28</d><h>14:32</h><w>Kelvin</w>I know people have mentioned the Fluree project in this channel before, and I stumbled upon them through their <a href="https://github.com/fluree/json-ld" target="_blank">https://github.com/fluree/json-ld</a> library.</z><z id="t1646058825" t="Kelvin My question is for any Fluree devs out there is what&apos;s the state of the project? It looks like it&apos;s in active development, but it doesn&apos;t seem 100% complete, especially for non-internal use."><y>#</y><d>2022-02-28</d><h>14:33</h><w>Kelvin</w>My question is for any Fluree devs out there is what&apos;s the state of the project? It looks like it&apos;s in active development, but it doesn&apos;t seem 100% complete, especially for non-internal use.</z><z id="t1651761881" t="winsome 👋 Hello! I&apos;m a Fluree dev and I&apos;m looking to polish this up - is there anything in particular that you&apos;re looking for that&apos;s missing?"><y>#</y><d>2022-05-05</d><h>14:44</h><r>winsome</r><b>👋</b> Hello! I&apos;m a Fluree dev and I&apos;m looking to polish this up - is there anything in particular that you&apos;re looking for that&apos;s missing?</z><z id="t1651765997" t="Kelvin The biggest thing about the json-ld lib is that it does not look like it&apos;s ready for use outside of Fluree. There is very little documentation, and what documentation there is is for dev installation. As such, it&apos;s difficult to answer questions like: • How do you use the API of the library? • What are the expected inputs and outputs of the API functions? • To what extent does it conform to the json-ld spec? • If there are things that are missing, will they be added in the future?"><y>#</y><d>2022-05-05</d><h>15:53</h><r>Kelvin</r>The biggest thing about the json-ld lib is that it does not look like it&apos;s ready for use outside of Fluree. There is very little documentation, and what documentation there is is for dev installation. As such, it&apos;s difficult to answer questions like:
• How do you use the API of the library?
• What are the expected inputs and outputs of the API functions?
• To what extent does it conform to the json-ld spec?
• If there are things that are missing, will they be added in the future?</z><z id="t1651766073" t="Kelvin I will also add that the library is not on Clojars or similar. Which in and of itself isn&apos;t an issue (because one can use Git SHAs) but it adds to the perception that this is still an internal lib in development."><y>#</y><d>2022-05-05</d><h>15:54</h><r>Kelvin</r>I will also add that the library is not on Clojars or similar. Which in and of itself isn&apos;t an issue (because one can use Git SHAs) but it adds to the perception that this is still an internal lib in development.</z><z id="t1651766476" t="Kelvin If the documentation is improved, it would be much easier to determine the strengths and weaknesses of the library API/code itself"><y>#</y><d>2022-05-05</d><h>16:01</h><r>Kelvin</r>If the documentation is improved, it would be much easier to determine the strengths and weaknesses of the library API/code itself</z><z id="t1651891058" t="winsome Thanks for the feedback, I&apos;ll let you know when updates come out, it should address the questions above."><y>#</y><d>2022-05-07</d><h>02:37</h><r>winsome</r>Thanks for the feedback, I&apos;ll let you know when updates come out, it should address the questions above.</z><z id="t1646058864" t="simongray I am on the look-out for a library to produce JSON-LD, so I would like to know this as well. Seems to be lacking documentation too."><y>#</y><d>2022-02-28</d><h>14:34</h><w>simongray</w>I am on the look-out for a library to produce JSON-LD, so I would like to know this as well. Seems to be lacking documentation too.</z><z id="t1648067930" t="Kelvin [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] I just discovered that you were one of the editors of the SPARQL Update spec!"><y>#</y><d>2022-03-23</d><h>20:38</h><w>Kelvin</w><a>@quoll</a> I just discovered that you were one of the editors of the SPARQL Update spec!</z><z id="t1648067946" t="Kelvin It might be common knowledge in this group but I find this really cool"><y>#</y><d>2022-03-23</d><h>20:39</h><w>Kelvin</w>It might be common knowledge in this group but I find this really cool</z><z id="t1648109946" t="simongray I think Paula has mentioned a few times that she was involved with it. 🙂 Another profile in the Clojure community who seems to have been involved in foundational semweb stuff is Jack Rusher. I was reading some W3C documents and randomly discovered that he was the author. Mostly know him for his genart stuff, but he has quite a varied resumé."><y>#</y><d>2022-03-24</d><h>08:19</h><r>simongray</r>I think Paula has mentioned a few times that she was involved with it. <b>🙂</b>

Another  profile in the Clojure community who seems to have been involved in foundational semweb stuff is Jack Rusher. I was reading some W3C  documents and randomly discovered that he was the author. Mostly know him for his genart stuff, but he has quite a varied resumé.</z><z id="t1648198121" t="rickmoynihan Haha yeah it’s very cool to have [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] here… Also, Alex Miller used to work for Revelytix (acquired by Teradata) who implemented a bunch of RDF products: https://www.crunchbase.com/organization/revelytix"><y>#</y><d>2022-03-25</d><h>08:48</h><r>rickmoynihan</r>Haha yeah it’s very cool to have <a>@quoll</a> here…

Also, Alex Miller used to work for Revelytix (acquired by Teradata) who implemented a bunch of RDF products:

<a href="https://www.crunchbase.com/organization/revelytix" target="_blank">https://www.crunchbase.com/organization/revelytix</a></z><z id="t1648091500" t="quoll And thanks. I liked doing it"><y>#</y><d>2022-03-24</d><h>03:11</h><w>quoll</w>And thanks. I liked doing it</z><z id="t1648094390" t="quoll I was massaging some table data dumped as CSV into ttl. I have a lot more tables to work on, so I cleaned it up (a little bit anyway. It&apos;s not too clean) and https://github.com/quoll/myrtle . But I should have asked before I made a mess… are there other tools like this for Clojure?"><y>#</y><d>2022-03-24</d><h>03:59</h><w>quoll</w>I was massaging some table data dumped as CSV into ttl. I have a lot more tables to work on, so I cleaned it up (a little bit anyway. It&apos;s not too clean) and <a href="https://github.com/quoll/myrtle" target="_blank">https://github.com/quoll/myrtle</a>.
But I should have asked before I made a mess… are there other tools like this for Clojure?</z><z id="t1648199633" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] : I mentioned this before, and it’s not a programming API for arbitrary transformations but you might be interested in our standards based implementation of the W3C’s CSV on the Web (CSVW) spec for csv2rdf: https://www.w3.org/TR/csv2rdf/ Two colleague’s of mine (one from Swirrl and the other from a client of ours at the Office for National Statistics) is actually doing a talk on it at the Open Data Institute today at 13:00 GMT. Registration is necessary to view the live-stream but it’s free: https://theodi.org/event/odi-fridays-csv-on-the-web/ Our csv2rdf command line tool implemented in clojure is available here: https://github.com/Swirrl/csv2rdf and there are some native-image binaries for macos and linux available too. We’ve also started trying to create some more “consumer friendly” documentation and guides here: https://csvw.org/"><y>#</y><d>2022-03-25</d><h>09:13</h><w>rickmoynihan</w><a>@quoll</a>: I mentioned this before, and it’s not a programming API for arbitrary transformations but you might be interested in our standards based implementation of the W3C’s CSV on the Web (CSVW) spec for csv2rdf:

<a href="https://www.w3.org/TR/csv2rdf/" target="_blank">https://www.w3.org/TR/csv2rdf/</a>

Two colleague’s of mine (one from Swirrl and the other from a client of ours at the Office for National Statistics) is actually doing a talk on it at the Open Data Institute today at 13:00 GMT.  Registration is necessary to view the live-stream but it’s free:

<a href="https://theodi.org/event/odi-fridays-csv-on-the-web/" target="_blank">https://theodi.org/event/odi-fridays-csv-on-the-web/</a>

Our csv2rdf command line tool implemented in clojure is available here:

<a href="https://github.com/Swirrl/csv2rdf" target="_blank">https://github.com/Swirrl/csv2rdf</a>

and there are some native-image binaries for macos and linux available too.

We’ve also started trying to create some more “consumer friendly” documentation and guides here:

<a href="https://csvw.org/" target="_blank">https://csvw.org/</a></z><z id="t1648201147" t="rickmoynihan CSVW’s csv2rdf is quite good, when: 1. You have your data arranged in a normalised form, e.g. 3NF or tidy(ish)-data. 2. You want to then map columns to predicates and cells to URIs or datatypes. 3. You don’t need complex transformations; as part of the RDFisation step. You can use virtual columns and URI templates to help build URIs/values "><y>#</y><d>2022-03-25</d><h>09:39</h><w>rickmoynihan</w>CSVW’s csv2rdf is quite good, when:

1. You have your data arranged in a normalised form, e.g. 3NF or tidy(ish)-data.
2. You want to then map columns to predicates and cells to URIs or datatypes.
3. You don’t need complex transformations; as part of the RDFisation step.  You can use virtual columns and URI templates to help build URIs/values
</z><z id="t1648201162" t="rickmoynihan Basically you just need to write a metadata file (which is a restricted subset/dialect of jsonld) that can include some metadata and a TableSchema, which describes how to annotate the columns and cells and essentially convert them into URI’s/RDF."><y>#</y><d>2022-03-25</d><h>09:39</h><w>rickmoynihan</w>Basically you just need to write a metadata file (which is a restricted subset/dialect of jsonld) that can include some metadata and a TableSchema, which describes how to annotate the columns and cells and essentially convert them into URI’s/RDF.</z><z id="t1648201163" t="rickmoynihan The standard itself is quite opaque about what CSVW really is… I’d summarise the standard as providing the following: 1. A means of annotating CSV files on the web (or locally). 2. A json(ld) hypertext format for connecting tables together across the web; essentially allowing anyone to annotate arbitrary CSV files from the web and convert them into a connected web-spanning relational database. 3. A standards based way to describe various CSV dialects 4. A way to annotate proper datatypes on top of CSV 5. A way to convert CSV into RDF statements You obviously don’t need to use it for all of those purposes; as it is also a reasonable way to just do 5."><y>#</y><d>2022-03-25</d><h>09:39</h><w>rickmoynihan</w>The standard itself is quite opaque about what CSVW really is…

I’d summarise the standard as providing the following:

1. A means of annotating CSV files on the web (or locally).
2. A json(ld) hypertext format for connecting tables together across the web; essentially allowing anyone to annotate arbitrary CSV files from the web and convert them into a connected web-spanning relational database.
3. A standards based way to describe various CSV dialects
4. A way to annotate proper datatypes on top of CSV
5. A way to convert CSV into RDF statements
You obviously don’t need to use it for all of those purposes; as it is also a reasonable way to just do 5.</z><z id="t1648203149" t="quoll Thanks Rick. #5 may be useful here"><y>#</y><d>2022-03-25</d><h>10:12</h><w>quoll</w>Thanks Rick. #5 may be useful here</z><z id="t1648216833" t="rickmoynihan Slides from the talk I posted earlier are here: https://docs.google.com/presentation/d/1AAoz16Wb1DggId6R-eYJv2fWlroMw-m7CMekv-4q06Y/edit#slide=id.gf3bb74a837_4_137 I believe a recording will be available at some point soon"><y>#</y><d>2022-03-25</d><h>14:00</h><w>rickmoynihan</w>Slides from the talk I posted earlier are here:

<a href="https://docs.google.com/presentation/d/1AAoz16Wb1DggId6R-eYJv2fWlroMw-m7CMekv-4q06Y/edit#slide=id.gf3bb74a837_4_137" target="_blank">https://docs.google.com/presentation/d/1AAoz16Wb1DggId6R-eYJv2fWlroMw-m7CMekv-4q06Y/edit#slide=id.gf3bb74a837_4_137</a>

I believe a recording will be available at some point soon</z><z id="t1648548939" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] : FYI Here is that CSVW talk: https://www.youtube.com/watch?v=uMEFHSsww6k"><y>#</y><d>2022-03-29</d><h>10:15</h><w>rickmoynihan</w><a>@quoll</a>: FYI Here is that CSVW talk:

<a href="https://www.youtube.com/watch?v=uMEFHSsww6k" target="_blank">https://www.youtube.com/watch?v=uMEFHSsww6k</a></z><z id="t1649081144" t="Kelvin SPARQL question - for escaping newlines and returns in strings, is the correct escaped char \\\n or \\\\n ?"><y>#</y><d>2022-04-04</d><h>14:05</h><w>Kelvin</w>SPARQL question - for escaping newlines and returns in strings, is the correct escaped char <code>\\\n</code> or <code>\\\\n</code>?</z><z id="t1649081165" t="Kelvin I thought it was the latter from reading the SPARQL grammar, but apparently Apache Jena thinks it&apos;s the former and I misunderstood the grammar"><y>#</y><d>2022-04-04</d><h>14:06</h><w>Kelvin</w>I thought it was the latter from reading the SPARQL grammar, but apparently Apache Jena thinks it&apos;s the former and I misunderstood the grammar</z><z id="t1649081296" t="Kelvin For reference here are the relevant branches and terminals in the SPARQL CFG: [156] STRING_LITERAL1 ::= &quot;&apos;&quot; ( ([^#x27#x5C#xA#xD]) | ECHAR )* &quot;&apos;&quot; [157] STRING_LITERAL2 ::= &apos;&quot;&apos; ( ([^#x22#x5C#xA#xD]) | ECHAR )* &apos;&quot;&apos; [158] STRING_LITERAL_LONG1 ::= &quot;&apos;&apos;&apos;&quot; ( ( &quot;&apos;&quot; | &quot;&apos;&apos;&quot; )? ( [^&apos;\] | ECHAR ) )* &quot;&apos;&apos;&apos;&quot; [159] STRING_LITERAL_LONG2 ::= &apos;&quot;&quot;&quot;&apos; ( ( &apos;&quot;&apos; | &apos;&quot;&quot;&apos; )? ( [^&quot;\] | ECHAR ) )* &apos;&quot;&quot;&quot;&apos; [160] ECHAR ::= &apos;\&apos; [tbnrf\&quot;&apos;]"><y>#</y><d>2022-04-04</d><h>14:08</h><w>Kelvin</w>For reference here are the relevant branches and terminals in the SPARQL CFG:
<pre>[156]  	STRING_LITERAL1	  ::=  	&quot;&apos;&quot; ( ([^#x27#x5C#xA#xD]) | ECHAR )* &quot;&apos;&quot;
[157]  	STRING_LITERAL2	  ::=  	&apos;&quot;&apos; ( ([^#x22#x5C#xA#xD]) | ECHAR )* &apos;&quot;&apos;
[158]  	STRING_LITERAL_LONG1	  ::=  	&quot;&apos;&apos;&apos;&quot; ( ( &quot;&apos;&quot; | &quot;&apos;&apos;&quot; )? ( [^&apos;\] | ECHAR ) )* &quot;&apos;&apos;&apos;&quot;
[159]  	STRING_LITERAL_LONG2	  ::=  	&apos;&quot;&quot;&quot;&apos; ( ( &apos;&quot;&apos; | &apos;&quot;&quot;&apos; )? ( [^&quot;\] | ECHAR ) )* &apos;&quot;&quot;&quot;&apos;
[160]  	ECHAR	  ::=  	&apos;\&apos; [tbnrf\&quot;&apos;]</pre></z><z id="t1649102001" t="quoll So long as the SPARQL parser sees a \ character followed by a n character. In Clojure, that would be encoded in a string of: \\n If your Clojure/Java string looks like \\\n then this will be a \ character followed by a newline character (ASCII 0xA). It should print like this: =&gt; (println &quot;\\\n&quot;) \ nil =&gt; You&apos;ll see that this sequence is not part of ECHAR. However, depending on the parser, a \ followed by a non-special character can result in that character just being allowed through (this makes it easier to deal with \\ , \&quot; , and \&apos; particularly, for instance, when allowing \&quot; to pass through as a &quot; when you&apos;re in a &apos; delimited string, since no \ is needed in that case). So it doesn&apos;t surprise me if it works."><y>#</y><d>2022-04-04</d><h>19:53</h><w>quoll</w>So long as the SPARQL parser sees a <code>\</code> character followed by a <code>n</code> character. In Clojure, that would be encoded in a string of: <code>\\n</code>
If your Clojure/Java string looks like <code>\\\n</code> then this will be a <code>\</code> character followed by a newline character (ASCII 0xA). It should print like this:
<pre>=&gt; (println &quot;\\\n&quot;)
\

nil
=&gt;</pre>
You&apos;ll see that this sequence is not part of ECHAR. However, depending on the parser, a <code>\</code> followed by a non-special character can result in that character just being allowed through (this makes it easier to deal with <code>\\</code>, <code>\&quot;</code>, and <code>\&apos;</code> particularly, for instance, when allowing <code>\&quot;</code> to pass through as a <code>&quot;</code> when you&apos;re in a <code>&apos;</code> delimited string, since no <code>\</code> is needed in that case). So it doesn&apos;t surprise me if it works.</z><z id="t1649102367" t="Kelvin Ok what is truly bizarre is how Jena treats different numbers of backslashes: (import &apos;[org.apache.jena.query QueryFactory]) (QueryFactory/create &quot;SELECT ?x WHERE { ?x ?y \&quot;foo\nbar\&quot;}&quot;) =&gt; Bad! (QueryFactory/create &quot;SELECT ?x WHERE { ?x ?y \&quot;foo\\nbar\&quot;}&quot;) =&gt; Good! (QueryFactory/create &quot;SELECT ?x WHERE { ?x ?y \&quot;foo\\\nbar\&quot;}&quot;) =&gt; Bad! (QueryFactory/create &quot;SELECT ?x WHERE { ?x ?y \&quot;foo\\\\nbar\&quot;}&quot;) =&gt; Good!"><y>#</y><d>2022-04-04</d><h>19:59</h><w>Kelvin</w>Ok what is truly bizarre is how Jena treats different numbers of backslashes:
<pre>(import &apos;[org.apache.jena.query QueryFactory])

(QueryFactory/create &quot;SELECT ?x WHERE { ?x ?y \&quot;foo\nbar\&quot;}&quot;) =&gt; Bad!
(QueryFactory/create &quot;SELECT ?x WHERE { ?x ?y \&quot;foo\\nbar\&quot;}&quot;) =&gt; Good!
(QueryFactory/create &quot;SELECT ?x WHERE { ?x ?y \&quot;foo\\\nbar\&quot;}&quot;) =&gt; Bad!
(QueryFactory/create &quot;SELECT ?x WHERE { ?x ?y \&quot;foo\\\\nbar\&quot;}&quot;) =&gt; Good!</pre></z><z id="t1649102437" t="Kelvin The first one makes sense since the \n is unescaped. What I don&apos;t get is why Jena treats \\n differently from \\\n even though they both have 2 chars in Clojure!"><y>#</y><d>2022-04-04</d><h>20:00</h><w>Kelvin</w>The first one makes sense since the <code>\n</code> is unescaped. What I don&apos;t get is why Jena treats <code>\\n</code> differently from <code>\\\n</code> even though they both have 2 chars in Clojure!</z><z id="t1649102750" t="quoll The second one (`\\n`) is correct. The next one (`\\\n`) is bad, because sparql sees ascii chars: 0x5c, 0xa. The 0x5c character \ is not allowed to be followed by anything except one of: tbnrf\&quot;&apos; The final one is where sparql sees ascii chars: 0x5c, 0x5c, 0xa. That&apos;s 3 characters: \\n . This is valid SPARQL, but probably not what you want."><y>#</y><d>2022-04-04</d><h>20:05</h><w>quoll</w>The second one (`\\n`) is correct.
The next one (`\\\n`) is bad, because sparql sees ascii chars: 0x5c, 0xa. The 0x5c character <code>\</code> is not allowed to be followed by anything except one of: <code>tbnrf\&quot;&apos;</code>
The final one is where sparql sees ascii chars: 0x5c, 0x5c, 0xa. That&apos;s 3 characters: <code>\\n</code>. This is valid SPARQL, but probably not what you want.</z><z id="t1649104433" t="Kelvin I realize that another complication is that chars in the SPARQL grammar rules are unescaped"><y>#</y><d>2022-04-04</d><h>20:33</h><r>Kelvin</r>I realize that another complication is that chars in the SPARQL grammar rules are unescaped</z><z id="t1649104476" t="Kelvin Took me a while to realize that in ECHAR := &apos;\&apos; [tbnrf\&quot;&apos;] the second \ and the &quot; were two separate chars, not an escaped &quot;"><y>#</y><d>2022-04-04</d><h>20:34</h><r>Kelvin</r>Took me a while to realize that in <code>ECHAR := &apos;\&apos; [tbnrf\&quot;&apos;]</code> the second <code>\</code> and the <code>&quot;</code> were two separate chars, not an escaped <code>&quot;</code></z><z id="t1649105014" t="quoll Ah yes! That&apos;s a gotcha. It&apos;s a bit weird, since it&apos;s not how that regex must be written: #&quot;\\[tbnrf\\\&quot;&apos;]&quot; The EBNF syntax does not seem to have escape characters itself, but I&apos;m not sure of that. So maybe if they&apos;d written it as: ECHAR := &apos;\&apos; [tbnrf&quot;&apos;\] Then perhaps it could have been clearer? But then I&apos;m wondering, &quot;Is the closing bracket being escaped?&quot;"><y>#</y><d>2022-04-04</d><h>20:43</h><r>quoll</r>Ah yes! That&apos;s a gotcha. It&apos;s a bit weird, since it&apos;s not how that regex must be written:
<code>#&quot;\\[tbnrf\\\&quot;&apos;]&quot;</code>
The EBNF syntax does not seem to have escape characters itself, but I&apos;m not sure of that. So maybe if they&apos;d written it as:
<code>ECHAR := &apos;\&apos; [tbnrf&quot;&apos;\]</code>
Then perhaps it could have been clearer? But then I&apos;m wondering, &quot;Is the closing bracket being escaped?&quot;</z><z id="t1649105091" t="quoll Anyway, I was actually looking at exactly this just a few nights ago 🙂"><y>#</y><d>2022-04-04</d><h>20:44</h><r>quoll</r>Anyway, I was actually looking at exactly this just a few nights ago <b>🙂</b></z><z id="t1649105108" t="quoll I&apos;m trying to write a SPARQL-&gt;Asami wrapper"><y>#</y><d>2022-04-04</d><h>20:45</h><r>quoll</r>I&apos;m trying to write a SPARQL-&gt;Asami wrapper</z><z id="t1649105133" t="quoll I have a long way to go 😞"><y>#</y><d>2022-04-04</d><h>20:45</h><r>quoll</r>I have a long way to go <b>😞</b></z><z id="t1649105345" t="quoll (I&apos;m also trying to write a fast TTL parser in another project, and that&apos;s having to do similar things)"><y>#</y><d>2022-04-04</d><h>20:49</h><r>quoll</r>(I&apos;m also trying to write a fast TTL parser in another project, and that&apos;s having to do similar things)</z><z id="t1649106913" t="Kelvin Will the wrapper be named Korra? 😉"><y>#</y><d>2022-04-04</d><h>21:15</h><r>Kelvin</r>Will the wrapper be named Korra? <b>😉</b></z><z id="t1649108346" t="quoll No. That naming scheme came about from someone else. Since I don&apos;t have to worry about that anymore, I&apos;m calling it &quot;Twylyte&quot;."><y>#</y><d>2022-04-04</d><h>21:39</h><r>quoll</r>No. That naming scheme came about from someone else. Since I don&apos;t have to worry about that anymore, I&apos;m calling it &quot;Twylyte&quot;.</z><z id="t1649102833" t="Kelvin &gt; This is valid SPARQL, but probably not what you want. Because SPARQL and Java/Clojure tread escaped chars slightly differently?"><y>#</y><d>2022-04-04</d><h>20:07</h><w>Kelvin</w>&gt; This is valid SPARQL, but probably not what you want.
Because SPARQL and Java/Clojure tread escaped chars slightly differently?</z></g><g id="s4"><z id="t1649102878" t="quoll No, it&apos;s because Clojure has already escaped your characters before the string even gets to SPARQL"><y>#</y><d>2022-04-04</d><h>20:07</h><w>quoll</w>No, it&apos;s because Clojure has already escaped your characters before the string even gets to SPARQL</z><z id="t1649102967" t="quoll In the second one, you have the string in Clojure code: &quot;SELECT ?x WHERE { ?x ?y \&quot;foo\\nbar\&quot;}&quot; Those escapes are interpreted, and you end up with: SELECT ?x WHERE { ?x ?y &quot;foo\nbar&quot;} This is what the SPARQL parser will get"><y>#</y><d>2022-04-04</d><h>20:09</h><w>quoll</w>In the second one, you have the string in Clojure code:
<code>&quot;SELECT ?x WHERE { ?x ?y \&quot;foo\\nbar\&quot;}&quot;</code>
Those escapes are interpreted, and you end up with:
<code>SELECT ?x WHERE { ?x ?y &quot;foo\nbar&quot;}</code>
This is what the SPARQL parser will get</z><z id="t1649102999" t="Kelvin I see - that confirms my suspicions that Flint&apos;s valid string regex is wrong"><y>#</y><d>2022-04-04</d><h>20:09</h><w>Kelvin</w>I see - that confirms my suspicions that Flint&apos;s valid string regex is wrong</z><z id="t1649103006" t="Kelvin Right now Flint thinks \n and \\n are wrong whereas \\\n and \\\\n are correct"><y>#</y><d>2022-04-04</d><h>20:10</h><w>Kelvin</w>Right now Flint thinks <code>\n</code> and <code>\\n</code> are wrong whereas <code>\\\n</code> and <code>\\\\n</code> are correct</z><z id="t1649103046" t="Kelvin So thank you for the correction - escape characters (especially escaping escape characters) is one of the parts of Clojure that greatly confuses me"><y>#</y><d>2022-04-04</d><h>20:10</h><w>Kelvin</w>So thank you for the correction - escape characters (especially escaping escape characters) is one of the parts of Clojure that greatly confuses me</z><z id="t1649103059" t="quoll Assuming those a in Clojure code, the first one is valid in a long string, and the second is valid in a short string or a long string"><y>#</y><d>2022-04-04</d><h>20:10</h><w>quoll</w>Assuming those a in Clojure code, the first one is valid in a long string, and the second is valid in a short string or a long string</z><z id="t1649103104" t="quoll If you&apos;re working at a repl, just println the string. That shows you the unescaped version (fixed from prn )"><y>#</y><d>2022-04-04</d><h>20:11</h><w>quoll</w>If you&apos;re working at a repl, just <code>println</code> the string. That shows you the unescaped version (fixed from <code>prn</code>)</z><z id="t1649103115" t="Kelvin Flint coerces all strings into short strings for simplicity&apos;s sake"><y>#</y><d>2022-04-04</d><h>20:11</h><w>Kelvin</w>Flint coerces all strings into short strings for simplicity&apos;s sake</z><z id="t1649103209" t="quoll Ugh. Sorry. I meant println (not prn )"><y>#</y><d>2022-04-04</d><h>20:13</h><w>quoll</w>Ugh. Sorry. I meant <code>println</code> (not <code>prn</code>)</z><z id="t1649103238" t="quoll =&gt; (println &quot;SELECT ?x WHERE { ?x ?y \&quot;foo\\nbar\&quot;}&quot;) SELECT ?x WHERE { ?x ?y &quot;foo\nbar&quot;} "><y>#</y><d>2022-04-04</d><h>20:13</h><w>quoll</w><pre>=&gt; (println &quot;SELECT ?x WHERE { ?x ?y \&quot;foo\\nbar\&quot;}&quot;)
SELECT ?x WHERE { ?x ?y &quot;foo\nbar&quot;}</pre>
</z><z id="t1649103556" t="Kelvin Ah I realized that char-array helps a lot with this too: (seq (char-array &quot;\n&quot;)) =&gt; (\newline) (seq (char-array &quot;\\n&quot;)) =&gt; (\\ \n) (seq (char-array &quot;\\\n&quot;)) =&gt; (\\ \newline) (seq (char-array &quot;\\\\n&quot;)) =&gt; (\\ \\ \n)"><y>#</y><d>2022-04-04</d><h>20:19</h><w>Kelvin</w>Ah I realized that <code>char-array</code> helps a lot with this too:
<pre>(seq (char-array &quot;\n&quot;)) =&gt; (\newline)
(seq (char-array &quot;\\n&quot;)) =&gt; (\\ \n)
(seq (char-array &quot;\\\n&quot;)) =&gt; (\\ \newline)
(seq (char-array &quot;\\\\n&quot;)) =&gt; (\\ \\ \n)</pre></z><z id="t1649105248" t="Kelvin And of course when I went and made a PR I realized I also have to deal with ClojureScript. Fun."><y>#</y><d>2022-04-04</d><h>20:47</h><w>Kelvin</w>And of course when I went and made a PR I realized I also have to deal with ClojureScript. Fun.</z><z id="t1649106487" t="Kelvin Well I finally got a https://github.com/yetanalytics/flint/pull/22 up. Expect a new version in the near future, if not tomorrow."><y>#</y><d>2022-04-04</d><h>21:08</h><w>Kelvin</w>Well I finally got a <a href="https://github.com/yetanalytics/flint/pull/22" target="_blank">https://github.com/yetanalytics/flint/pull/22</a> up. Expect a new version in the near future, if not tomorrow.</z><z id="t1649123617" t="Al Baker interesting -- have you taken a look at clj-sparql ? 🙂"><y>#</y><d>2022-04-05</d><h>01:53</h><w>Al Baker</w>interesting -- have you taken a look at clj-sparql ? <b>🙂</b></z><z id="t1649123674" t="Al Baker could probably use Flint and clj-sparql together... I think I had some examples of the various sparql builder/DSLs that folks have built"><y>#</y><d>2022-04-05</d><h>01:54</h><w>Al Baker</w>could probably use Flint and clj-sparql together... I think I had some examples of the various sparql builder/DSLs that folks have built</z><z id="t1649123682" t="Al Baker it&apos;s just wrapper around ARQ"><y>#</y><d>2022-04-05</d><h>01:54</h><w>Al Baker</w>it&apos;s just wrapper around ARQ</z><z id="t1649164169" t="Kelvin I have! (Though the fact that it hasn&apos;t been updated since 2015, with outstanding TODOs, ultimately dissuaded me from using it.)"><y>#</y><d>2022-04-05</d><h>13:09</h><w>Kelvin</w>I have! (Though the fact that it hasn&apos;t been updated since 2015, with outstanding TODOs, ultimately dissuaded me from using it.)</z><z id="t1649164232" t="Kelvin What I had not looked at was https://github.com/boutros/matsu , which seems to be a SPARQL DSL created long before Flint!"><y>#</y><d>2022-04-05</d><h>13:10</h><w>Kelvin</w>What I had not looked at was <a href="https://github.com/boutros/matsu" target="_blank">https://github.com/boutros/matsu</a>, which seems to be a SPARQL DSL created long before Flint!</z><z id="t1649164250" t="Kelvin (Though again it hasn&apos;t been actively maintained since 2014.)"><y>#</y><d>2022-04-05</d><h>13:10</h><w>Kelvin</w>(Though again it hasn&apos;t been actively maintained since 2014.)</z><z id="t1649165054" t="Kelvin An incomplete list of difference between Matsu and Flint that I noticed: • Matsu uses an expression-based syntax (e.g. (query (select ...) (where ...)) ) while Flint uses a map-based syntax at the top level. • Matsu uses tuples for prefixed IRIs (e.g. [:foaf :mbox] ) whereas Flint uses qualified keywords (e.g. :foaf/mbox ). • In Matsu, one can register global prefixes, whereas in Flint you cannot. Flint&apos;s approach is more functional, but I can see how Matsu&apos;s approach can be more convenient. • In Matsu, IRIs are encoded as URI instances, while in Flint they&apos;re regular strings wrapped with angle brackets. The former was the approach [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] suggested for Flint. • Matsu is missing support for some SPARQL features, such as subqueries, graph management updates (eg CREATE and DROP), and certain expressions. Not an intentional omission since they&apos;re all listed in TODOs, but the fact that they&apos;re not present would&apos;ve made Matsu problematic for the applications I&apos;ve been working on."><y>#</y><d>2022-04-05</d><h>13:24</h><w>Kelvin</w>An incomplete list of difference between Matsu and Flint that I noticed:
• Matsu uses an expression-based syntax (e.g. <code>(query (select ...) (where ...))</code>) while Flint uses a map-based syntax at the top level.
• Matsu uses tuples for prefixed IRIs (e.g. <code>[:foaf :mbox]</code>) whereas Flint uses qualified keywords (e.g. <code>:foaf/mbox</code>).
• In Matsu, one can register global prefixes, whereas in Flint you cannot. Flint&apos;s approach is more functional, but I can see how Matsu&apos;s approach can be more convenient.
• In Matsu, IRIs are encoded as <code>URI</code> instances, while in Flint they&apos;re regular strings wrapped with angle brackets. The former was the approach <a>@rickmoynihan</a> suggested for Flint.
• Matsu is missing support for some SPARQL features, such as subqueries, graph management updates (eg CREATE and DROP), and certain expressions. Not an intentional omission since they&apos;re all listed in TODOs, but the fact that they&apos;re not present would&apos;ve made Matsu problematic for the applications I&apos;ve been working on.</z><z id="t1649170735" t="quoll On the global prefixes, personally, I really like them, despite SPARQL (and TTL, etc) not having them. I mean, does anyone ever really want to redefine xsd: rdf: rdfs: owl: or skos: ?"><y>#</y><d>2022-04-05</d><h>14:58</h><r>quoll</r>On the global prefixes, personally, I really like them, despite SPARQL (and TTL, etc) not having them. I mean, does anyone ever really want to redefine <code>xsd:</code> <code>rdf:</code> <code>rdfs:</code> <code>owl:</code> or <code>skos:</code>?</z><z id="t1649171781" t="quoll Since I&apos;m here, I might as well comment on some of the other points: • The expression-based syntax looks a lot like the internal structure that Jena uses, but I think that the map works better in Clojure. Order doesn&apos;t actually matter, but there should never be more than 8 keys (I think?) so I think you&apos;ll get an array map, which will preserve the ordering (useful for printing). • I like using keywords for QNames. Theoretically, I think there are some QNames that may not be valid keyword literals? But a) I can&apos;t think of any, and b) you can always construct illegal keywords with the keyword function, since it just saves a pair of strings. 🙂 • I commented on liking global prefixes. • I agree with [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] on using URIs. Similarly to keywords, java.net.URI does not do any checking or translating, which means that they can handle IRIs just fine: =&gt; (.getPath (java.net.URI. &quot;&quot;)) &quot;/wiki/Ῥόδος&quot; • Nice that you&apos;ve done all of the SPARQL operations :thumbsup: "><y>#</y><d>2022-04-05</d><h>15:16</h><r>quoll</r>Since I&apos;m here, I might as well comment on some of the other points:
• The expression-based syntax looks a lot like the internal structure that Jena uses, but I think that the map works better in Clojure. Order doesn&apos;t actually matter, but there should never be more than 8 keys (I think?) so I think you&apos;ll get an array map, which will preserve the ordering (useful for printing).
• I like using keywords for QNames. Theoretically, I think there are some QNames that may not be valid keyword literals? But a) I can&apos;t think of any, and b) you can always construct illegal keywords with the <code>keyword</code> function, since it just saves a pair of strings. <b>🙂</b>
• I commented on liking global prefixes.
• I agree with <a>@rickmoynihan</a> on using URIs. Similarly to keywords, java.net.URI does not do any checking or translating, which means that they can handle IRIs just fine:
<pre>=&gt; (.getPath (java.net.URI. &quot;&quot;))
&quot;/wiki/Ῥόδος&quot;</pre>
• Nice that you&apos;ve done all of the SPARQL operations <b>:thumbsup:</b> </z><z id="t1649173460" t="quoll BTW, I take it back about the keywords. It looks like I can type any QName character at a repl and it can make a keyword. .e.g :a.b.c/x%0a˿"><y>#</y><d>2022-04-05</d><h>15:44</h><r>quoll</r>BTW, I take it back about the keywords. It looks like I can type any QName character at a repl and it can make a keyword. .e.g <code>:a.b.c/x%0a˿</code></z><z id="t1649173530" t="quoll Oh, no... the PN_LOCAL_ESC sequences won&apos;t work 🙂"><y>#</y><d>2022-04-05</d><h>15:45</h><r>quoll</r>Oh, no... the PN_LOCAL_ESC sequences won&apos;t work <b>🙂</b></z><z id="t1649173552" t="quoll (Those would make for hairy QNames, and are definitely not CURIs)"><y>#</y><d>2022-04-05</d><h>15:45</h><r>quoll</r>(Those would make for hairy QNames, and are definitely not CURIs)</z><z id="t1649176784" t="Kelvin • Flint actually re-orders top-level maps, so you could intentionally misorder your clauses ( https://github.com/yetanalytics/flint/blob/main/src/test/com/yetanalytics/flint/spec/update_test.cljc#L58-L67 ) and it will still be formatted in the right order. • (Not much to say about qkws.) • Honestly I could see future Flint having a prefix registry similar to Matsu, which (like in Matsu) can be augmented with additional prefixes directly in the :prefixes map. But to maintain the functional spirit it would be totally optional. • Seems like people really like that URI feature. My main issue is that point is that Flint would need to keep the &lt;&gt; system for IRIs for backwards compatibility - not that such things are new of course. • Indeed, and with expressions in particular this is where I&apos;d say Flint is superior to Matsu&apos;s approach (where the latter has to define separate functions for each SPARQL expr)."><y>#</y><d>2022-04-05</d><h>16:39</h><r>Kelvin</r>• Flint actually re-orders top-level maps, so you could intentionally misorder your clauses (<a href="https://github.com/yetanalytics/flint/blob/main/src/test/com/yetanalytics/flint/spec/update_test.cljc#L58-L67" target="_blank">https://github.com/yetanalytics/flint/blob/main/src/test/com/yetanalytics/flint/spec/update_test.cljc#L58-L67</a>) and it will still be formatted in the right order.
• (Not much to say about qkws.)
• Honestly I could see future Flint having a prefix registry similar to Matsu, which (like in Matsu) can be augmented with additional prefixes directly in the <code>:prefixes</code> map. But to maintain the functional spirit it would be totally optional.
• Seems like people really like that URI feature. My main issue is that point is that Flint would need to keep the <code>&lt;&gt;</code> system for IRIs for backwards compatibility - not that such things are new of course.
• Indeed, and with expressions in particular this is where I&apos;d say Flint is superior to Matsu&apos;s approach (where the latter has to define separate functions for each SPARQL expr).</z><z id="t1649435777" t="Eric Scott This probably qualifies as a shameless plug, but one of IGraph&apos;s sister projects is https://github.com/ont-app/vocabulary , which uses clojure namespace metadata to hold mappings to rdf equivalents, and has support for translating between URIs, clojure keywords, and qnames."><y>#</y><d>2022-04-08</d><h>16:36</h><r>Eric Scott</r>This probably qualifies as a shameless plug, but one of IGraph&apos;s sister projects is <a href="https://github.com/ont-app/vocabulary" target="_blank">https://github.com/ont-app/vocabulary</a> , which uses clojure namespace metadata to hold mappings to rdf equivalents, and has support for translating between URIs, clojure keywords, and qnames.</z><z id="t1650038878" t="rickmoynihan &gt; Seems like people really like that URI feature. My main issue is that point is that Flint would need to keep the &lt;&gt; system for IRIs for backwards compatibility - not that such things are new of course. [:attrs {:href &quot;/_/_/users/U02FU7RMG8M&quot;}] : My fear with that is that type correctness still suffers. i.e. strings can be misinterpreted as URIs. Would it not be possible to define set a flag which disables the parsing of strings into URI’s? &gt; Flint’s approach is more functional, but I can see how Matsu’s approach can be more convenient. I’m a strong supporter of what you’ve done here. Passing the prefixes in at the top each time is much better than having a global registry, or storing stuff on namespaces/vars. If someone wants a global registry/singleton, let them define it. Having such things can cause problems for folk because you’re complecting namespace loading with side effects. Doing those things is fine of course, but it’s an application concern to do it, not a library concern. e.g. integrant systems want to componentise by moving the require tree into a DAG expressed in edn configuration."><y>#</y><d>2022-04-15</d><h>16:07</h><r>rickmoynihan</r>&gt; Seems like people really like that URI feature. My main issue is that point is that Flint would need to keep the &lt;&gt; system for IRIs for backwards compatibility - not that such things are new of course.
<a>@U02FU7RMG8M</a>: My fear with that is that type correctness still suffers.  i.e. strings can be misinterpreted as URIs.

Would it not be possible to define set a flag which disables the parsing of strings into URI’s?

&gt;  Flint’s approach is more functional, but I can see how Matsu’s approach can be more convenient.
I’m a strong supporter of what you’ve done here.  Passing the prefixes in at the top each time is much better than having a global registry, or storing stuff on namespaces/vars.  If someone wants a global registry/singleton, let them define it.  Having such things can cause problems for folk because you’re complecting namespace loading with side effects.  Doing those things is fine of course, but it’s an application concern to do it, not a library concern.

e.g. integrant systems want to componentise by moving the require tree into a DAG expressed in edn configuration.</z><z id="t1651343822" t="Bart Kleijngeld Hello everyone, I&apos;m trying to sell Clojure as the language of choice at work, and I was hoping to get some inspiration from people here 🙂 ."><y>#</y><d>2022-04-30</d><h>18:37</h><w>Bart Kleijngeld</w>Hello everyone,

I&apos;m trying to sell Clojure as the language of choice at work, and I was hoping to get some inspiration from people here <b>🙂</b>.</z><z id="t1651343841" t="Bart Kleijngeld We&apos;re building an (open-source) application which takes as input a data model described by RDF files, and puts out a generated schema of several possible types (Apache AVRO for example, but we want to expand to all sorts things such as SQL schemas). So it gets quite &quot;meta&quot;, and we deal with generation of all sorts of data structures from whatever input set of files we may have. We left Java, and are currently trying a functional (FP) approach in Kotlin. It&apos;s not too bad, but I&apos;m not happy about a lot of things. I gave Clojure a try in my spare time and I fell in love immediately. Now, I&apos;m trying to make an objective case for why it is more useful for our purposes, and I would love to hear your thoughts. Both in response to what I&apos;m thinking, as any ideas of your own. First of all, I think static typing isn&apos;t useful here, but gets in the way. Modeling highly abstract data using types is tricky. It isn&apos;t as flexible as a heterogenous map with a spec, and moreover it&apos;s hard (if not impossible) to generate classes from data at runtime, as we need to. Of course, you can choose not to model using types, but if that&apos;s the route you&apos;re taking, I feel you&apos;re basically choosing Clojure and maps. Especially since maps in Kotlin are not heterogenous. Furthermore, functional programming in Clojure feels great. You can express yourself very strongly, and there&apos;s no hassle with weird syntax, complicated casting of types and noisy syntax (this is my experience in Kotlin). Transforming data has never felt so powerful. Another reason for choosing Clojure is the helpful development workflow using the REPL. Especially when building out data, it can be very useful to check out intermediate results before moving on. With Kotlin, this feedback is not only much slower, but it also requires me to run some main/test function, and worst of all make sure my types are in order (since the intermediate result often has a different type). I&apos;ve noticed myself being greatly productive in Clojure thanks to this, and I feel that especially if the data gets complex, this is very helpful. Finally, since it&apos;s runs on the JVM we can use all sorts of Semantic Web libraries built in Java (like Eclipse&apos;s RDF4j) without issues."><y>#</y><d>2022-04-30</d><h>18:37</h><r>Bart Kleijngeld</r>We&apos;re building an (open-source) application which takes as input a data model described by RDF files, and puts out a generated schema of several possible types (Apache AVRO for example, but we want to expand to all sorts things such as SQL schemas). So it gets quite &quot;meta&quot;, and we deal with generation of all sorts of data structures from whatever input set of files we may have.

We left Java, and are currently trying a functional (FP) approach in Kotlin. It&apos;s not too bad, but I&apos;m not happy about a lot of things. I gave Clojure a try in my spare time and I fell in love immediately. Now, I&apos;m trying to make an objective case for why it is more useful for our purposes, and I would love to hear your thoughts. Both in response to what I&apos;m thinking, as any ideas of your own.

First of all, I think static typing isn&apos;t useful here, but gets in the way. Modeling highly abstract data using types is tricky. It isn&apos;t as flexible as a heterogenous map with a spec, and moreover it&apos;s hard (if not impossible) to generate classes from data at runtime, as we need to.

Of course, you can choose not to model using types, but if that&apos;s the route you&apos;re taking, I feel you&apos;re basically choosing Clojure and maps. Especially since maps in Kotlin are not heterogenous.

Furthermore, functional programming in Clojure feels great. You can express yourself very strongly, and there&apos;s no hassle with weird syntax, complicated casting of types and noisy syntax (this is my experience in Kotlin). Transforming data has never felt so powerful.

Another reason for choosing Clojure is the helpful development workflow using the REPL. Especially when building out data, it can be very useful to check out intermediate results before moving on. With Kotlin, this feedback is not only much slower, but it also requires me to run some main/test function, and worst of all make sure my types are in order (since the intermediate result often has a different type).

I&apos;ve noticed myself being greatly productive in Clojure thanks to this, and I feel that especially if the data gets complex, this is very helpful.

Finally, since it&apos;s runs on the JVM we can use all sorts of Semantic Web libraries built in Java (like Eclipse&apos;s RDF4j) without issues.</z><z id="t1651343877" t="Bart Kleijngeld If anyone&apos;s stuck with me so far, thanks already :). I would love to know if you think my arguments make sense and are sufficiently objective with regards to the application we&apos;re building. Besides feedback on my points above, I would appreciate other input as well. Particularly with regards to the following remarks/questions I&apos;m anticipating from my manager: 1. Sourcing people for Kotlin is doable, but Clojure is an esoteric niche. 2. Isn&apos;t Kotlin a good enough choice to warrant continuing to develop in it (note: this project is very much in its infancy) 3. Static types provide a better guarantee for mature software. I don&apos;t like the idea of dynamic typing, it feels unsafe."><y>#</y><d>2022-04-30</d><h>18:37</h><r>Bart Kleijngeld</r>If anyone&apos;s stuck with me so far, thanks already :). I would love to know if you think my arguments make sense and are sufficiently objective with regards to the application we&apos;re building.

Besides feedback on my points above, I would appreciate other input as well. Particularly with regards to the following remarks/questions I&apos;m anticipating from my manager:

1. Sourcing people for Kotlin is doable, but Clojure is an esoteric niche.
2. Isn&apos;t Kotlin a good enough choice to warrant continuing to develop in it (note: this project is very much in its infancy)
3. Static types provide a better guarantee for mature software. I don&apos;t like the idea of dynamic typing, it feels unsafe.</z><z id="t1651362729" t="quoll Your comment on static typing stands out to me. I worked with RDF on Java for many years, and while static typing is fine for RDF itself (subjects can be IRIs, or blank nodes. Predicates must be IRIs. Objects can be IRIs, blank nodes, or literals. Then the language-coded literals for string, and all of the menagerie of data types associated with literals), it is horrible when it comes to modeling data that is expressed in RDF. Objects need to be flexible. Every system I&apos;ve worked with which tried to define the structure so that it would conform to a statically typed language, it&apos;s caused pain. You can just use a map, but then you lose all the type information entirely."><y>#</y><d>2022-04-30</d><h>23:52</h><r>quoll</r>Your comment on static typing stands out to me.

I worked with RDF on Java for many years, and while static typing is fine for RDF itself (subjects can be IRIs, or blank nodes. Predicates must be IRIs. Objects can be IRIs, blank nodes, or literals. Then the language-coded literals for string, and all of the menagerie of data types associated with literals), it is horrible when it comes to modeling data that is expressed in RDF. Objects need to be flexible. Every system I&apos;ve worked with which tried to define the structure so that it would conform to a statically typed language, it&apos;s caused pain. You can just use a map, but then you lose all the type information entirely.</z><z id="t1651362988" t="quoll Clojure uses maps naturally, and it&apos;s built into the syntax of the language. Even better: • Records can be defined to have a particular set of keys, but other keys can be added ad-hoc. This is ideal for both having a type description (complete with protocols), while staying flexible enough to accept any new data that is encountered. • Clojure Spec is optional, but if it&apos;s used it can give more type information, providing not only key/value structure, but also the ability to programmatically define the structure. • Other systems like Malli can also provide optional type information."><y>#</y><d>2022-04-30</d><h>23:56</h><r>quoll</r>Clojure uses maps naturally, and it&apos;s built into the syntax of the language. Even better:
• Records can be defined to have a particular set of keys, but other keys can be added ad-hoc. This is ideal for both having a type description (complete with protocols), while staying flexible enough to accept any new data that is encountered.
• Clojure Spec is optional, but if it&apos;s used it can give more type information, providing not only key/value structure, but also the ability to programmatically define the structure.
• Other systems like Malli can also provide optional type information.</z><z id="t1651363569" t="quoll Addressing your concerns: 1. There are lots of people who can write Clojure, and it&apos;s constantly growing. There are also a legion of people out there who are not writing Clojure right now, but are trying to find that first opportunity to do so. Even so, Clojure is not a difficult language to learn (as you know) and a competent programmer can switch to it rapidly. Even better, if they&apos;ve already learned an FP language on the JVM (I came from Scala). 2. Kotlin is fine. After all, anything can be written in any language. But using a language that removes obstacles can make the process faster and easier. This is where Clojure has demonstrated it&apos;s utility in general. My personal experience is that RDF is a domain that particularly benefits from it. 3. See Spec and Malli. They provide optional types, with most of the benefits you get from types in statically typed languages, but offer so much more. Full algebraic data types (e.g. Haskell, Scala) may also offer more than basic data types, but then you&apos;re stuck with a rigid system that makes open world data extremely difficult to work with. These optional type systems in Clojure have been the best I&apos;ve worked with for handling open world data."><y>#</y><d>2022-05-01</d><h>00:06</h><r>quoll</r>Addressing your concerns:
1. There are lots of people who can write Clojure, and it&apos;s constantly growing. There are also a legion of people out there who are not writing Clojure right now, but are trying to find that first opportunity to do so. Even so, Clojure is not a difficult language to learn (as you know) and a competent programmer can switch to it rapidly. Even better, if they&apos;ve already learned an FP language on the JVM (I came from Scala).
2. Kotlin is fine. After all, anything can be written in any language. But using a language that removes obstacles can make the process faster and easier. This is where Clojure has demonstrated it&apos;s utility in general. My personal experience is that RDF is a domain that particularly benefits from it.
3. See Spec and Malli. They provide optional types, with most of the benefits you get from types in statically typed languages, but offer so much more. Full algebraic data types (e.g. Haskell, Scala) may also offer more than basic data types, but then you&apos;re stuck with a rigid system that makes open world data extremely difficult to work with. These optional type systems in Clojure have been the best I&apos;ve worked with for handling open world data.</z><z id="t1651389062" t="Bart Kleijngeld Thank you very much. I think there&apos;s a lot in there that I can use to state my case."><y>#</y><d>2022-05-01</d><h>07:11</h><r>Bart Kleijngeld</r>Thank you very much. I think there&apos;s a lot in there that I can use to state my case.</z><z id="t1651583995" t="rickmoynihan FWIW: We’ve been using clojure and RDF together for nearly 8 1/2 years. I still think they’re a good fit. Philosophically clojure and RDF are cut from the same cloth. RDF was a big inspiration to Rich in the design of clojure (and subsequently datomic), and there are many, many areas where this inspiration shows itself. Being open for extension, namespaced keywords, composing composites out of reified properties upon which you attach meaning/specification etc, having an accretive data model etc. I’m not against static types; but I agree with [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] that their use in RDF derived data models can be awkward. The reason is that in RDF classes are composed from their properties (and aren’t containers for properties)… Indeed in RDFS/OWL classes are often inferred from their properties. Type systems tend to containerize your representations; where as in clojure you can naturally express the properties as compositions in maps, and tools like spec/malli will help specify it (in a property centric way). I actually gave a talk on the topic of reified properties last week…. I’ll share it here once the recording is available."><y>#</y><d>2022-05-03</d><h>13:19</h><r>rickmoynihan</r>FWIW: We’ve been using clojure and RDF together for nearly 8 1/2 years.  I still think they’re a good fit.

Philosophically clojure and RDF are cut from the same cloth.  RDF was a big inspiration to Rich in the design of clojure (and subsequently datomic), and there are many, many areas where this inspiration shows itself.

Being open for extension, namespaced keywords, composing composites out of reified properties upon which you attach meaning/specification etc, having an accretive data model etc.

I’m not against static types; but I agree with <a>@U051N6TTC</a> that their use in RDF derived data models can be awkward.  The reason is that in RDF classes are composed from their properties (and aren’t containers for properties)… Indeed in RDFS/OWL classes are often inferred from their properties.

Type systems tend to containerize your representations; where as in clojure you can naturally express the properties as compositions in maps, and tools like spec/malli will help specify it (in a property centric way).

I actually gave a talk on the topic of reified properties last week…. I’ll share it here once the recording is available.</z><z id="t1651585730" t="Bart Kleijngeld Thanks for weighing in [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] , that&apos;s definitely useful 🙂 . Can&apos;t wait to see your talk!"><y>#</y><d>2022-05-03</d><h>13:48</h><r>Bart Kleijngeld</r>Thanks for weighing in <a>@U06HHF230</a>, that&apos;s definitely useful <b>🙂</b>. Can&apos;t wait to see your talk!</z><z id="t1652269559" t="Eric Scott Sorry I&apos;m late to the party. If you&apos;re not already familiar with it &quot;Uncle Bob&quot; (of &quot;Clean Code&quot; fame) wrote a great piece of Clojure evangelism: https://blog.cleancoder.com/uncle-bob/2019/08/22/WhyClojure.html , which might be worth passing along to you colleagues."><y>#</y><d>2022-05-11</d><h>11:45</h><r>Eric Scott</r>Sorry I&apos;m late to the party.  If you&apos;re not already familiar with it &quot;Uncle Bob&quot; (of &quot;Clean Code&quot; fame) wrote a great piece of Clojure evangelism: <a href="https://blog.cleancoder.com/uncle-bob/2019/08/22/WhyClojure.html" target="_blank">https://blog.cleancoder.com/uncle-bob/2019/08/22/WhyClojure.html</a>, which might be worth passing along to you colleagues.</z><z id="t1652270035" t="Bart Kleijngeld That just blew my mind. Uncle Bob loves LISP? Wow. That definitely is a good source to attract the attention from colleagues, thanks [:attrs {:href &quot;/_/_/users/UB3R8UYA1&quot;}] ! Enjoy your vacation 😉"><y>#</y><d>2022-05-11</d><h>11:53</h><r>Bart Kleijngeld</r>That just blew my mind. Uncle Bob loves LISP? Wow. That definitely is a good source to attract the attention from colleagues, thanks <a>@UB3R8UYA1</a>! Enjoy your vacation <b>😉</b></z><z id="t1652293213" t="Eric Scott I&apos;m actually still a working stiff. More of a state of mind :-)"><y>#</y><d>2022-05-11</d><h>18:20</h><r>Eric Scott</r>I&apos;m actually still a working stiff. More of a state of mind :-)</z><z id="t1652479765" t="quoll https://www.functionalgeekery.com/episode-1-robert-c-martin/ 9 years ago where he talked about how SICP changed his life. He also talked about Clojure in positive terms"><y>#</y><d>2022-05-13</d><h>22:09</h><r>quoll</r><a href="https://www.functionalgeekery.com/episode-1-robert-c-martin/" target="_blank">https://www.functionalgeekery.com/episode-1-robert-c-martin/</a> 9 years ago where he talked about how SICP changed his life. He also talked about Clojure in positive terms</z><z id="t1652520582" t="Bart Kleijngeld Yes it seems he&apos;s really sold on Clojure. Ever since that blog [:attrs {:href &quot;/_/_/users/UB3R8UYA1&quot;}] referred to, it seems he speaks positively about dynamic typing and particularly Clojure in many more blogs (and indeed podcasts) . I have to say I respect the open mind here. I mean, he was heavily invested in OOP advocacy and seems revered up to the point of a God-like figure at times. It takes character to then change your mind so drastically."><y>#</y><d>2022-05-14</d><h>09:29</h><r>Bart Kleijngeld</r>Yes it seems he&apos;s really sold on Clojure. Ever since that blog <a>@UB3R8UYA1</a> referred to, it seems he speaks positively about dynamic typing and particularly Clojure in many more blogs (and indeed podcasts) . I have to say I respect the open mind here. I mean, he was heavily invested in OOP advocacy and seems revered up to the point of a God-like figure at times. It takes character to then change your mind so drastically.</z><z id="t1656341313" t="Bart Kleijngeld [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] &gt; I actually gave a talk on the topic of reified properties last week…. I’ll share it here once the recording is available. Did you manage to get hold of that recording? I would still be interested. This Thursday is going to be the first of (hopefully few) conversations with my technical manager to try and convince him of Clojure."><y>#</y><d>2022-06-27</d><h>14:48</h><r>Bart Kleijngeld</r><a>@U06HHF230</a>
&gt; I actually gave a talk on the topic of reified properties last week…. I’ll share it here once the recording is available.
Did you manage to get hold of that recording? I would still be interested. This Thursday is going to be the first of (hopefully few) conversations with my technical manager to try and convince him of Clojure.</z><z id="t1656355636" t="rickmoynihan Ah thanks for reminding me… It was just a short talk, and was purposefully pitched at a high/conceptual level for a government audience of developers…. https://www.youtube.com/watch?v=HtYWAZZWiAs I wish the organisers would have titled the video properly too but 🤷"><y>#</y><d>2022-06-27</d><h>18:47</h><r>rickmoynihan</r>Ah thanks for reminding me…

It was just a short talk, and was purposefully pitched at a high/conceptual level for a government audience of developers….

<a href="https://www.youtube.com/watch?v=HtYWAZZWiAs" target="_blank">https://www.youtube.com/watch?v=HtYWAZZWiAs</a>

I wish the organisers would have titled the video properly too but <b>🤷</b></z><z id="t1656355716" t="rickmoynihan The talk’s title was: Unpopular ways to make better APIs; forgotten and emerging knowledge in API design."><y>#</y><d>2022-06-27</d><h>18:48</h><r>rickmoynihan</r>The talk’s title was:

Unpopular ways to make better APIs; forgotten and emerging knowledge in API design.</z><z id="t1656395855" t="Bart Kleijngeld Your talk confirms a lot of ideas that I&apos;ve been forming myself in the last few months, and then some! Very informative, thanks for sharing. 🙂"><y>#</y><d>2022-06-28</d><h>05:57</h><r>Bart Kleijngeld</r>Your talk confirms a lot of ideas that I&apos;ve been forming myself in the last few months, and then some! Very informative, thanks for sharing. <b>🙂</b></z><z id="t1651444338" t="apbleonard In an existential funk trying to think through the idea of decorating our existing inter microservice JSON API payloads with JSON-LD to translate from the ad hoc property names we&apos;ve built up to a &quot;better&quot; &quot;more carefully defined&quot; set of names whose namespaces reflect bounded contexts and everything is amazing, for consumption by top level integrators that mix data from all wings of the business. That line of thinking led me to look for guidelines on how RDF properties should be named - which led me to these https://www-sop.inria.fr/acacia/personnel/phmartin/RDF/conventions.html#reversingRelations . There seems to be a lot of handy tips and unwritten rules in here that help constrain the wide open spaces of creativity we typically leave ourselves when it comes to property naming. Section 1.2 &quot;Singular nouns for names whenever possible&quot; is particularly enlightening to me. Are these conventions captured anywhere else - in a standard or a book, RDF related or otherwise? Would be a nice reference to have :)"><y>#</y><d>2022-05-01</d><h>22:32</h><w>apbleonard</w>In an existential funk trying to think through the idea of decorating our existing inter microservice JSON API payloads with JSON-LD to translate from the ad hoc property names we&apos;ve built up to a &quot;better&quot; &quot;more carefully defined&quot; set of names whose namespaces reflect bounded contexts and everything is amazing, for consumption by top level integrators that mix data from all wings of the business. That line of thinking led me to look for guidelines on how RDF properties should be named - which led me to these <a href="https://www-sop.inria.fr/acacia/personnel/phmartin/RDF/conventions.html#reversingRelations" target="_blank">https://www-sop.inria.fr/acacia/personnel/phmartin/RDF/conventions.html#reversingRelations</a>.  There seems to be a lot of handy tips and unwritten rules in here that help constrain the wide open spaces of creativity we typically leave ourselves when it comes to property naming. Section 1.2 &quot;Singular nouns for names whenever possible&quot; is particularly enlightening to me. Are these conventions captured anywhere else - in a standard or a book, RDF related or otherwise? Would be a nice reference to have :)</z><z id="t1651569632" t="Ivan that was as mouthful! 😄 thanks for posting this, seems interesting. Semi-related, &quot; https://elementsofclojure.com/ &quot; by Tellman has a chapter on naming - a more philosophical approach to one of the hardest problems in computer science."><y>#</y><d>2022-05-03</d><h>09:20</h><r>Ivan</r>that was as mouthful! <b>😄</b>

thanks for posting this, seems interesting. Semi-related, &quot;<a href="https://elementsofclojure.com/" target="_blank">https://elementsofclojure.com/</a>&quot; by Tellman has a chapter on naming - a more philosophical approach to one of the hardest problems in computer science.</z><z id="t1651579051" t="apbleonard Yes I have that one in hard copy :) Zach Tellman doesn&apos;t go as far as Martin to suggest suffixing with &quot;of&quot; for inverse relations and using nouns like &quot;owner&quot; and not &quot;hasOwner&quot; for properties, to be interpreted as &quot;entity X [:attrs nil] [:attrs nil] property Y value Z&quot;, which I&apos;ve never seen before. He does say &quot;when naming functions which only transform data only use nouns wherever possible,&quot; which always surprised me, but perhaps is in keeping with the thought here. Hide how you are generating the new property and just name the transformation after the new property itself - again an unadulterated noun."><y>#</y><d>2022-05-03</d><h>11:57</h><r>apbleonard</r>Yes I have that one in hard copy :) Zach Tellman doesn&apos;t go as far as Martin to suggest suffixing with &quot;of&quot; for inverse relations and using nouns like &quot;owner&quot; and not &quot;hasOwner&quot; for properties, to be interpreted as &quot;entity X <b>has</b> <b>for</b> property Y value Z&quot;, which I&apos;ve never seen before. He does say &quot;when naming functions which only transform data only use nouns wherever possible,&quot; which always surprised me, but perhaps is in keeping with the thought here. Hide how you are generating the new property and just name the transformation after the new property itself - again an unadulterated noun.</z><z id="t1652167912" t="simongray Anybody read any interesting blogs about RDF/semantic web stuff? I recently got back into RSS feeds."><y>#</y><d>2022-05-10</d><h>07:31</h><w>simongray</w>Anybody read any interesting blogs about RDF/semantic web stuff? I recently got back into RSS feeds.</z><z id="t1652407537" t="Al Baker we have a Stardog Labs blog, recently posted on data science usage and being able to pull a random set out of the graph with a query: https://www.stardog.com/labs/blog"><y>#</y><d>2022-05-13</d><h>02:05</h><w>Al Baker</w>we have a Stardog Labs blog, recently posted on data science usage and being able to pull a random set out of the graph with a query: <a href="https://www.stardog.com/labs/blog" target="_blank">https://www.stardog.com/labs/blog</a></z><z id="t1652472332" t="abdullahibra hi guys, Are there any strengths points to choose Datomic over RDF storage or the reverse?"><y>#</y><d>2022-05-13</d><h>20:05</h><w>abdullahibra</w>hi guys, Are there any strengths points to choose Datomic over RDF storage or the reverse?</z><z id="t1652481534" t="quoll • Datomic is native Clojure, so it&apos;s definitely comfortable to use. Keywords are nice. • It also has the benefit of immutable Databases, which matches the Clojure philosophy. • Each database version has a timestamp, and you can find data at any time. Every statement also stores the transaction and time it was asserted. This is nice too. • One nice feature is the ability to provision over many types of backends, though once you&apos;ve settled on one then it doesn&apos;t matter too much. • Datomic has no query planner. This is both a blessing and a curse. It lets you specify the order of query evaluation, so you can get it right when a SPARQL system may not provide that for you. On the other hand, it forces you to choose your evaluation plan, and people get it wrong too. On the minus side: • Datomic seems oriented around rapid updates. Loading large datasets is horribly slow. Most RDF stores are likely to perform better for analytics. • There is no standard data format. Loading data needs some code. RDF databases can load the various RDF formats. • SPARQL has some extra query semantics, (though Datomic does allow filtering and binding with user space functions). • Lots of integration tools for RDF/SPARQL. Notsomuch for Datomic. • RDF stores can accept ontologies and/or rules, and thereby do reasoning. Datomic rules can be powerful, but not THAT powerful. • Speed: some of the RDF stores are just blazingly fast"><y>#</y><d>2022-05-13</d><h>22:38</h><w>quoll</w>• Datomic is native Clojure, so it&apos;s definitely comfortable to use. Keywords are nice.
• It also has the benefit of immutable Databases, which matches the Clojure philosophy.
• Each database version has a timestamp, and you can find data at any time. Every statement also stores the transaction and time it was asserted. This is nice too.
• One nice feature is the ability to provision over many types of backends, though once you&apos;ve settled on one then it doesn&apos;t matter too much.
• Datomic has no query planner. This is both a blessing and a curse. It lets you specify the order of query evaluation, so you can get it right when a SPARQL system may not provide that for you. On the other hand, it forces you to choose your evaluation plan, and people get it wrong too.
On the minus side:
• Datomic seems oriented around rapid updates. Loading large datasets is horribly slow. Most RDF stores are likely to perform better for analytics.
• There is no standard data format. Loading data needs some code. RDF databases can load the various RDF formats.
• SPARQL has some extra query semantics, (though Datomic does allow filtering and binding with user space functions).
• Lots of integration tools for RDF/SPARQL. Notsomuch for Datomic.
• RDF stores can accept ontologies and/or rules, and thereby do reasoning. Datomic rules can be powerful, but not THAT powerful.
• Speed: some of the RDF stores are just blazingly fast</z><z id="t1652508069" t="abdullahibra That’s amazing answer, Thank you"><y>#</y><d>2022-05-14</d><h>06:01</h><r>abdullahibra</r>That’s amazing answer, Thank you</z><z id="t1652508365" t="abdullahibra Regarding the point of RDF accepts ontologies/rules and reasoning and it seems superior to Datomic on that. if it is possible to give an example to clarify this point, it would be great."><y>#</y><d>2022-05-14</d><h>06:06</h><r>abdullahibra</r>Regarding the point of RDF accepts ontologies/rules and reasoning and it seems superior to Datomic on that.
if it is possible to give an example to clarify this point, it would be great.</z><z id="t1652528774" t="quoll This is “how long is a piece of string”, but an example I have at work is Laterality. We have. Class that indicates that something has a value for an attribute called laterality . It can only be from the pair: left or right . Then there can be cases where one of those values is excluded (eg, the heart is not on the right). If you ask for everything with a laterality of left you’ll get those things that were declared to have Laterality and can&apos;t be on the right."><y>#</y><d>2022-05-14</d><h>11:46</h><r>quoll</r>This is “how long is a piece of string”, but an example I have at work is Laterality. We have. Class that indicates that something has a value for an attribute called <code>laterality</code>. It can only be from the pair: <code>left</code> or <code>right</code>. Then there can be cases where one of those values is excluded (eg, the heart is not on the right). If you ask for everything with a <code>laterality</code> of <code>left</code> you’ll get those things that were declared to have Laterality and can&apos;t be on the right.</z><z id="t1652528837" t="quoll It&apos;s a little obscure, but it&apos;s an example that shows some extra reasoning capability."><y>#</y><d>2022-05-14</d><h>11:47</h><r>quoll</r>It&apos;s a little obscure, but it&apos;s an example that shows some extra reasoning capability.</z><z id="t1652528952" t="quoll We can also use SHACL to declaratively confirm data structures are correct, rather than Datomic’s transaction functions (more of a philosophical question there)"><y>#</y><d>2022-05-14</d><h>11:49</h><r>quoll</r>We can also use SHACL to declaratively confirm data structures are correct, rather than Datomic’s transaction functions (more of a philosophical question there)</z><z id="t1652529335" t="quoll But ontologies let us say things like, “show me all the patients who are over 50, have some kind of carcinoma, and never smoked”. Datomic could do part of that, but the “carcinoma” information isn&apos;t explicit. Instead some will have squamous cell carcinomas, others have basal cell carcinomas, and it will even include leukemia. But it won&apos;t include “melanoma”. This info comes from a complex taxonomy of related terms that an ontology reasoner can give us"><y>#</y><d>2022-05-14</d><h>11:55</h><r>quoll</r>But ontologies let us say things like, “show me all the patients who are over 50, have some kind of carcinoma, and never smoked”. Datomic could do part of that, but the “carcinoma” information isn&apos;t explicit. Instead some will have squamous cell carcinomas, others have basal cell carcinomas, and it will even include leukemia. But it won&apos;t include “melanoma”.
This info comes from a complex taxonomy of related terms that an ontology reasoner can give us</z><z id="t1652531071" t="abdullahibra Thanks a lot for your explanation :)"><y>#</y><d>2022-05-14</d><h>12:24</h><r>abdullahibra</r>Thanks a lot for your explanation :)</z><z id="t1652481564" t="quoll That&apos;s off the top of my head just now on my phone. I’m sure others here can say a lot too!"><y>#</y><d>2022-05-13</d><h>22:39</h><w>quoll</w>That&apos;s off the top of my head just now on my phone. I’m sure others here can say a lot too!</z><z id="t1652508463" t="abdullahibra what is preferable open source datastore to you guys? apache jena?"><y>#</y><d>2022-05-14</d><h>06:07</h><w>abdullahibra</w>what is preferable open source datastore to you guys? apache jena?</z><z id="t1652776293" t="rickmoynihan Fuseki/Jena is I think almost certainly the most popular opensource store, and is very actively developed. RDF4j has a native store option too and it is also actively developed. Both are really good long running and active projects. Personally I find RDF4j’s code much cleaner and easier to read and use than Jena’s, though Jena tends to have more features etc. There are some limits on the capabilities of the opensource triplestores. Typically in terms of number of triples stored (and also performance). Not sure what the limits are these days…. but IIRC about 7 years ago we tried storing about 7-800 million triples in RDF4j’s native store, and it would start returning wrong query results… iirc the developers said they’d tested it up to about 3 or 400 million. The commercial stores tend to do better with large quantities of data… e.g. stardog."><y>#</y><d>2022-05-17</d><h>08:31</h><r>rickmoynihan</r>Fuseki/Jena is I think almost certainly the most popular opensource store, and is very actively developed.

RDF4j has a native store option too and it is also actively developed.

Both are really good long running and active projects.  Personally I find RDF4j’s code much cleaner and easier to read and use than Jena’s, though Jena tends to have more features etc.

There are some limits on the capabilities of the opensource triplestores.  Typically in terms of number of triples stored (and also performance).  Not sure what the limits are these days…. but IIRC about 7 years ago we tried storing about 7-800 million triples in RDF4j’s native store, and it would start returning wrong query results… iirc the developers said they’d tested it up to about 3 or 400 million.

The commercial stores tend to do better with large quantities of data… e.g. stardog.</z><z id="t1652522015" t="Bart Kleijngeld To read input RDF files and query the graph this yields, we currently use RDF4j (in Kotlin) and its SPARQL support. Slowly but surely my colleagues are being sold on Clojure, so I&apos;m trying to have some code done to demonstrate the capabilities. Using RDF4j this extensively in Clojure is going to be a massive amount of Java interop, which is far from ideal (that&apos;s what we want to move away from the in the first place). I was thinking of using only its Rio module for reading RDF files. I can then map the triples in the model to hash-maps easily, moving into Clojure territory. What I would then end up needing is some in-memory RDF graph DB that I can use to query the information for our purposes. I&apos;ve only just realised this doesn&apos;t have to support SPARQL (this is new territory for me), and could very well be some other query language. I&apos;ve been looking around and: • I feel [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] ’s Asami might be a good fit. • I also saw [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] ’s Grafter which seems a nice wrapper on top of RDF4j, although I don&apos;t know how much it supports, and I noticed the RDF4j version it supports is rather old. Also, I came across Arachne&apos;s Aristotle, and some no longer maintained SPARQL query DSL named matsu. I appreciate the help! Oh, and this is going to be an open-source project, so only free (as in &quot;doesn&apos;t cost money&quot;) projects are an option."><y>#</y><d>2022-05-14</d><h>09:53</h><w>Bart Kleijngeld</w>To read input RDF files and query the graph this yields, we currently use RDF4j (in Kotlin) and its SPARQL support. Slowly but surely my colleagues are being sold on Clojure, so I&apos;m trying to have some code done to demonstrate the capabilities.

Using RDF4j this extensively in Clojure is going to be a massive amount of Java interop, which is far from ideal (that&apos;s what we want to move away from the in the first place). I was thinking of using only its Rio module for reading RDF files. I can then map the triples in the model to hash-maps easily, moving into Clojure territory.

What I would then end up needing is some in-memory RDF graph DB that I can use to query the information for our purposes. I&apos;ve only just realised this doesn&apos;t have to support SPARQL (this is new territory for me), and could very well be some other query language. I&apos;ve been looking around and:
• I feel <a>@quoll</a>’s Asami might be a good fit.
• I also saw <a>@rickmoynihan</a>’s Grafter which seems a nice wrapper on top of RDF4j, although I don&apos;t know how much it supports, and I noticed the RDF4j version it supports is rather old.
Also, I came across Arachne&apos;s Aristotle, and some no longer maintained SPARQL query DSL named matsu.

I appreciate the help! Oh, and this is going to be an open-source project, so only free (as in &quot;doesn&apos;t cost money&quot;) projects are an option.</z><z id="t1652777376" t="rickmoynihan 👋 Hey! Sorry I was away on holiday… Grafter is essentially a wrapper over a chunk RDF4j, mainly as you say to simplify the interop. You are right that the RDF4j version it supports is getting a little long in the tooth. There were some breaking changes in RDF4j which caused our tests to fail, which is why just bumping the version further doesn’t simply work… though I suspect it will be relatively easy to fix it up (just requires a little time). In terms of what grafter supports; it has essentially full support for parsing and serialising RDF in any serialisation (via Rio). The abstraction is that you basically send/receive the data via lazy sequences. One of the main thing grafter adds is a bidirectional mapping of RDF data types into clojure datatypes, rather than the native RDF4j datatypes… i.e. an xsd:long is just a java.lang.Long , not some RDF4j numeric class that doesn’t support arithmetic. Some grafter protocols then provide ways to get the datatype-uri for all these native types etc. There is support for querying various sparql repositories (remote via the sparql protocol and in memory/native-store RDF4j ones) and having the datatypes handled for you properly. We don’t have a lot of support for RDF4j’s model APIs… and instead tend to fire the triples we get into an in memory store of our own (matcha: https://github.com/Swirrl/matcha ). Matcha basically targets a similar use case to asami; essentially being a schemaless in memory triplestore for querying RDF via basic graph patterns. Unlike asami it makes no attempt to look and feel like datomic. It was written at approximately the same time as Asami and if I’d known about Asami at the time I’d probably have just used it instead."><y>#</y><d>2022-05-17</d><h>08:49</h><r>rickmoynihan</r><b>👋</b> Hey! Sorry I was away on holiday…

Grafter is essentially a wrapper over a chunk RDF4j, mainly as you say to simplify the interop.

You are right that the RDF4j version it supports is getting a little long in the tooth.  There were some breaking changes in RDF4j which caused our tests to fail, which is why just bumping the version further doesn’t simply work… though I suspect it will be relatively easy to fix it up (just requires a little time).

In terms of what grafter supports; it has essentially full support for parsing and serialising RDF in any serialisation (via Rio).  The abstraction is that you basically send/receive the data via lazy sequences.

One of the main thing grafter adds is a bidirectional mapping of RDF data types into clojure datatypes, rather than the native RDF4j datatypes… i.e. an <code>xsd:long</code> is just a <code>java.lang.Long</code>, not some RDF4j numeric class that doesn’t support arithmetic.  Some grafter protocols then provide ways to get the datatype-uri for all these native types etc.

There is support for querying various sparql repositories (remote via the sparql protocol and in memory/native-store RDF4j ones) and having the datatypes handled for you properly.

We don’t have a lot of support for RDF4j’s model APIs… and instead tend to fire the triples we get into an in memory store of our own (matcha: <a href="https://github.com/Swirrl/matcha" target="_blank">https://github.com/Swirrl/matcha</a>).

Matcha basically targets a similar use case to asami; essentially being a schemaless in memory triplestore for querying RDF via basic graph patterns.  Unlike asami it makes no attempt to look and feel like datomic.

It was written at approximately the same time as Asami and if I’d known about Asami at the time I’d probably have just used it instead.</z><z id="t1652777807" t="Bart Kleijngeld Hope you had a good holiday! Thanks for the extensive explanation, this helps a lot. I&apos;m currently trying out Asami and am very happy with it so far. However, there&apos;s some concerns from colleagues regarding the query language not being SPARQL (and being Clojure), limiting contributors to the project on the long run. I hope to convince them to convince the others that a Clojure DSL is actually preferable 😉 , but just in case I&apos;m curious to know: does Grafter support the SPARQL querying and repositories (both in-memory and others) of RDF4j? It sounds like a very nice approach to wrapping RDF4j, happy to see the project being actively maintained!"><y>#</y><d>2022-05-17</d><h>08:56</h><r>Bart Kleijngeld</r>Hope you had a good holiday!

Thanks for the extensive explanation, this helps a lot. I&apos;m currently trying out Asami and am very happy with it so far. However, there&apos;s some concerns from colleagues regarding the query language not being SPARQL (and being Clojure), limiting contributors to the project on the long run. I hope to convince them to convince the others that a Clojure DSL is actually preferable <b>😉</b>, but just in case I&apos;m curious to know: does Grafter support the SPARQL querying and repositories (both in-memory and others) of RDF4j?

It sounds like a very nice approach to wrapping RDF4j, happy to see the project being actively maintained!</z><z id="t1652781021" t="rickmoynihan Ok, there’s actually quite a lot to dig into here, mostly around what your application and its architecture are. For the work we typically do, we need a real database and use a proper SPARQL based triplestore. There’s generally far too much data to hold in memory. So the pattern we tend to follow is to query the database with SPARQL CONSTRUCT statements, and load the triples into an in memory triplestore, and then query that again locally with many queries to construct the view. There are a number of reasons to do this, but the main one is because remodelling the data as clojure maps / trees from backend database queries involves remapping terms; and because of normalisation/dryness issues etc you end up building a mini database in your datastructures e.g. {:article-db {:article/1 {:title &quot;blah&quot; ,,,}} :articles [:article/1 :article/2] ,,,} Once you realise this is essentially an intermediate database to assemble a view, you realise it’s much more direct to just use an in memory database. Most of these in memory database / models aren’t actually SPARQL anyway; so if that’s your usecase you’re not really losing much… e.g. in Jena you’d use a Model for this and query it with BGP’s… same in RDF4j; or with arbitrary code. Asami and Matcha etc can be put to this purpose — but you can still have SPARQL as your main database. The reason to do this sort of design is because it avoids the network overhead of lots of small queries. Also you can make trade offs e.g. often it can be quicker to slightly over select data from the database, but have a simpler query, and then filter it out later client side."><y>#</y><d>2022-05-17</d><h>09:50</h><r>rickmoynihan</r>Ok, there’s actually quite a lot to dig into here, mostly around what your application and its architecture are.

For the work we typically do, we need a real database and use a proper SPARQL based triplestore.  There’s generally far too much data to hold in memory.

So the pattern we tend to follow is to query the database with SPARQL <code>CONSTRUCT</code> statements, and load the triples into an in memory triplestore, and then query that again locally with many queries to construct the view.

There are a number of reasons to do this, but the main one is because remodelling the data as clojure maps / trees from backend database queries involves remapping terms; and because of normalisation/dryness issues etc you end up building a mini database in your datastructures e.g.

<pre>{:article-db {:article/1 {:title &quot;blah&quot; ,,,}}
 :articles [:article/1 :article/2]
,,,}</pre>
Once you realise this is essentially an intermediate database to assemble a view, you realise it’s much more direct to just use an in memory database.

Most of these in memory database / models aren’t actually SPARQL anyway; so if that’s your usecase you’re not really losing much… e.g. in Jena you’d use a <code>Model</code> for this and query it with BGP’s… same in RDF4j; or with arbitrary code.

Asami and Matcha etc can be put to this purpose — but you can still have SPARQL as your main database.

The reason to do this sort of design is because it avoids the network overhead of lots of small queries.  Also you can make trade offs e.g. often it can be quicker to slightly over select data from the database, but have a simpler query, and then filter it out later client side.</z><z id="t1652781715" t="rickmoynihan You could also look at using flint to generate SPARQL queries on an in memory RDF4j sparql repo. IMHO flint is brilliant, but not yet perfectly suited to scenarios where you want to compose multiple queries. I filed an issue about this here https://github.com/yetanalytics/flint/issues/24 which also shows how to integrate this sort of thing with grafter (you could easily substitute (repo/sparql-repo ,,,) for an in memory (repo/sail-repo)"><y>#</y><d>2022-05-17</d><h>10:01</h><r>rickmoynihan</r>You could also look at using flint to generate SPARQL queries on an in memory RDF4j sparql repo.

IMHO flint is brilliant, but not yet perfectly suited to scenarios where you want to compose multiple queries.  I filed an issue about this here

<a href="https://github.com/yetanalytics/flint/issues/24" target="_blank">https://github.com/yetanalytics/flint/issues/24</a>

which also shows how to integrate this sort of thing with grafter (you could easily substitute <code>(repo/sparql-repo ,,,)</code> for an in memory <code>(repo/sail-repo)</code></z><z id="t1652784893" t="rickmoynihan To explicitly answer your question though; yes grafter does support querying remote sparql repositories and in memory ones. It also has some support for SPARQL variable / binding substitution, i.e. you can write a static SPARQL query in a file and then provide one or more bindings for the variable. In my experience this pattern means you can avoid a lot of cases where you might want to generate queries dynamically… which typically makes for maintainable code with a clearer performance profile. There are cases where you need to generate queries though; in particular SELECT queries if you want to be dynamic in the columns (you can avoid this by using CONSTRUCT s though. If you need to do query generation though something like flint is far preferable to string munging."><y>#</y><d>2022-05-17</d><h>10:54</h><r>rickmoynihan</r>To explicitly answer your question though; yes grafter does support querying remote sparql repositories and in memory ones.

It also has some support for SPARQL variable / binding substitution, i.e. you can write a static SPARQL query in a file and then provide one or more bindings for the variable.  In my experience this pattern means you can avoid a lot of cases where you might want to generate queries dynamically… which typically makes for maintainable code with a clearer performance profile.

There are cases where you need to generate queries though; in particular <code>SELECT</code> queries if you want to be dynamic in the columns (you can avoid this by using <code>CONSTRUCT</code>s though.  If you need to do query generation though something like flint is far preferable to string munging.</z><z id="t1652787198" t="quoll Funnily enough, I have started a SPARQL parser for Asami, but I have a way to go on it. I’m parsing the SPARQL (via Instaparse), but the transforms to Asami’s query language is a lot. It&apos;s probably a reasonably portable library, if you&apos;re interested [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] ? 🙂"><y>#</y><d>2022-05-17</d><h>11:33</h><r>quoll</r>Funnily enough, I have started a SPARQL parser for Asami, but I have a way to go on it. I’m parsing the SPARQL (via Instaparse), but the transforms to Asami’s query language is a lot.
It&apos;s probably a reasonably portable library, if you&apos;re interested <a>@rickmoynihan</a>? <b>🙂</b></z><z id="t1652787241" t="quoll The fact that Asami’s features focus on SPARQL semantics helps there 😊"><y>#</y><d>2022-05-17</d><h>11:34</h><r>quoll</r>The fact that Asami’s features focus on SPARQL semantics helps there <b>😊</b></z><z id="t1652787525" t="rickmoynihan So it converts SPARQL strings -&gt; asami edn queries?"><y>#</y><d>2022-05-17</d><h>11:38</h><r>rickmoynihan</r>So it converts SPARQL strings -&gt; asami  edn queries?</z><z id="t1652787853" t="rickmoynihan FWIW I already have an instaparse bnf that converts SPARQL into an intermediate EDN based AST… though it’s not currently part of an open sourced lib"><y>#</y><d>2022-05-17</d><h>11:44</h><r>rickmoynihan</r>FWIW I already have an instaparse bnf that converts SPARQL into an intermediate EDN based AST… though it’s not currently part of an open sourced lib</z><z id="t1652787926" t="rickmoynihan It’s basically lossless; only stripping whitespace and comments"><y>#</y><d>2022-05-17</d><h>11:45</h><r>rickmoynihan</r>It’s basically lossless; only stripping whitespace and comments</z><z id="t1652788592" t="quoll The Instaparse bit was quick. It&apos;s the transforms that are taking me time 🙂"><y>#</y><d>2022-05-17</d><h>11:56</h><r>quoll</r>The Instaparse bit was quick. It&apos;s the transforms that are taking me time <b>🙂</b></z><z id="t1652789044" t="Bart Kleijngeld Great info [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] , thanks! That gives me a far better picture of what&apos;s possible. I&apos;m definitely going to check out flint"><y>#</y><d>2022-05-17</d><h>12:04</h><r>Bart Kleijngeld</r>Great info <a>@rickmoynihan</a>, thanks! That gives me a far better picture of what&apos;s possible. I&apos;m definitely going to check out flint</z><z id="t1652789584" t="rickmoynihan &gt; The Instaparse bit was quick. It’s the transforms that are taking me time 🙂 Indeed. 🙂"><y>#</y><d>2022-05-17</d><h>12:13</h><r>rickmoynihan</r>&gt; The Instaparse bit was quick. It’s the transforms that are taking me time <b>🙂</b>
Indeed. <b>🙂</b></z><z id="t1652529566" t="quoll I’m trying to write RDF/SPARQL code for Asami in pure Clojure, but I’ve had extremely limited time lately. (Mostly due to travel for work and conferences). But for now Asami could only work with RDF if you use external tools for things like Turtle parsing"><y>#</y><d>2022-05-14</d><h>11:59</h><w>quoll</w>I’m trying to write RDF/SPARQL code for Asami in pure Clojure, but I’ve had extremely limited time lately. (Mostly due to travel for work and conferences). But for now Asami could only work with RDF if you use external tools for things like Turtle parsing</z><z id="t1652531140" t="abdullahibra Asami could only use Datomic as backend for not in-memory option?"><y>#</y><d>2022-05-14</d><h>12:25</h><r>abdullahibra</r>Asami could only use Datomic as backend for not in-memory option?</z><z id="t1652531191" t="abdullahibra honestly, Asami looks very interesting."><y>#</y><d>2022-05-14</d><h>12:26</h><r>abdullahibra</r>honestly, Asami looks very interesting.</z><z id="t1652532158" t="quoll Asami has been designed to be like Datomic, with many backend options. But for now, a local file option is the only one that&apos;s implemented."><y>#</y><d>2022-05-14</d><h>12:42</h><r>quoll</r>Asami has been designed to be like Datomic, with many backend options. But for now, a local file option is the only one that&apos;s implemented.</z><z id="t1652532200" t="quoll You wouldn&apos;t use Datomic as a backend. But you could use any one of Datomic’s backend options."><y>#</y><d>2022-05-14</d><h>12:43</h><r>quoll</r>You wouldn&apos;t use Datomic as a backend. But you could use any one of Datomic’s backend options.</z><z id="t1652532218" t="quoll The first 2 I’d like to do are Redis and Postgres"><y>#</y><d>2022-05-14</d><h>12:43</h><r>quoll</r>The first 2 I’d like to do are Redis and Postgres</z><z id="t1652532821" t="Bart Kleijngeld Parsing the RDF Turtle (and other serializations) files to triples in Clojure is a breeze using RDF4j. So from there I think I can just load up those triples in Asami and go from there 🙂 . I checked out your Strange Loop talk: Asami looks great! From what I can tell the query language is also already really powerful. Transitive properties and AND/OR assertions will really help me out. I don&apos;t think there&apos;s an equivalent to SPARQL&apos;s property path though, right? Where I would say something like ?x (p*/q)+ ?y ?"><y>#</y><d>2022-05-14</d><h>12:53</h><r>Bart Kleijngeld</r>Parsing the RDF Turtle (and other serializations) files to triples in Clojure is a breeze using RDF4j. So from there I think I can just load up those triples in Asami and go from there <b>🙂</b>.

I checked out your Strange Loop talk: Asami looks great! From what I can tell the query language is also already really powerful. Transitive properties and AND/OR assertions will really help me out.

I don&apos;t think there&apos;s an equivalent to SPARQL&apos;s property path though, right? Where I would say something like <code>?x (p*/q)+ ?y</code>?</z><z id="t1652530435" t="quoll If you like Jena (and Andy put in a lot of work in to make storage scale better) then you could consider writing a wrapper library around it?"><y>#</y><d>2022-05-14</d><h>12:13</h><w>quoll</w>If you like Jena (and Andy put in a lot of work in to make storage scale better) then you could consider writing a wrapper library around it?</z><z id="t1652531081" t="abdullahibra Yes. maybe"><y>#</y><d>2022-05-14</d><h>12:24</h><r>abdullahibra</r>Yes. maybe</z><z id="t1652777624" t="rickmoynihan FWIW I’ve been wanting to add a Jena backend to grafter for a long time. The protocols and namespaces are essentially already abstracted to support this addition. https://github.com/Swirrl/grafter/blob/master/src/grafter_2/rdf/protocols.cljc"><y>#</y><d>2022-05-17</d><h>08:53</h><r>rickmoynihan</r>FWIW I’ve been wanting to add a Jena backend to grafter for a long time.

The protocols and namespaces are essentially already abstracted to support this addition.

<a href="https://github.com/Swirrl/grafter/blob/master/src/grafter_2/rdf/protocols.cljc" target="_blank">https://github.com/Swirrl/grafter/blob/master/src/grafter_2/rdf/protocols.cljc</a></z><z id="t1652530794" t="quoll I https://github.com/quoll/stardog-clj in 2014. I literally only spent one night on it, and https://github.com/stardog-union/stardog-clj/blob/9476063ff09d8f012260bdaa034fe112d80ff458/src/stardog/core.clj#L2 the next day. Last week I learned that it&apos;s now in active deployment at NASA :rolling_on_the_floor_laughing:"><y>#</y><d>2022-05-14</d><h>12:19</h><w>quoll</w>I <a href="https://github.com/quoll/stardog-clj" target="_blank">https://github.com/quoll/stardog-clj</a> in 2014. I literally only spent one night on it, and <a href="https://github.com/stardog-union/stardog-clj/blob/9476063ff09d8f012260bdaa034fe112d80ff458/src/stardog/core.clj#L2" target="_blank">https://github.com/stardog-union/stardog-clj/blob/9476063ff09d8f012260bdaa034fe112d80ff458/src/stardog/core.clj#L2</a> the next day. Last week I learned that it&apos;s now in active deployment at NASA <b>:rolling_on_the_floor_laughing:</b></z><z id="t1652531103" t="abdullahibra Amazing"><y>#</y><d>2022-05-14</d><h>12:25</h><r>abdullahibra</r>Amazing</z><z id="t1652532296" t="quoll I wasn&apos;t trying to say something to amaze, but rather that it doesn&apos;t take a lot, and that&apos;s still enough to go a long way!"><y>#</y><d>2022-05-14</d><h>12:44</h><r>quoll</r>I wasn&apos;t trying to say something to amaze, but rather that it doesn&apos;t take a lot, and that&apos;s still enough to go a long way!</z><z id="t1652532596" t="abdullahibra I saw it amazing, with less effort you achieved a lot."><y>#</y><d>2022-05-14</d><h>12:49</h><r>abdullahibra</r>I saw it amazing, with less effort you achieved a lot.</z><z id="t1652533609" t="Bart Kleijngeld Very nice. That&apos;s inspiring indeed 😄 , cool story!"><y>#</y><d>2022-05-14</d><h>13:06</h><r>Bart Kleijngeld</r>Very nice. That&apos;s inspiring indeed <b>😄</b>, cool story!</z><z id="t1652532715" t="Eric Scott [:attrs {:href &quot;/_/_/users/U4P4NREBY&quot;}] has kindly compiled a set of clojure-based graph resources: https://github.com/simongray/clojure-graph-resources"><y>#</y><d>2022-05-14</d><h>12:51</h><w>Eric Scott</w><a>@simongray</a> has kindly compiled a set of clojure-based graph resources: <a href="https://github.com/simongray/clojure-graph-resources" target="_blank">https://github.com/simongray/clojure-graph-resources</a></z><z id="t1652532869" t="Bart Kleijngeld Couldn&apos;t have hoped for more. Thanks!"><y>#</y><d>2022-05-14</d><h>12:54</h><r>Bart Kleijngeld</r>Couldn&apos;t have hoped for more. Thanks!</z><z id="t1652605604" t="Bart Kleijngeld I&apos;ve succeeded in loading my RDF triples into Asami, but I can&apos;t get the entity function to work with nested? set to true . My triples (examples for demonstration: everything is a string): (take 2 rdf-data) ;; Output ([&quot;&quot; &quot;&quot; &quot;\&quot;1\&quot;^^&lt;&gt;&quot;] [&quot;&quot; &quot;&quot; &quot;&quot;]) Then I add them to the store: @(d/transact conn {:tx-triples model}) Then, I&apos;m trying to use entity to retrieve some object (nested): (d/entity (d/db conn) &quot;&quot; true) ;; Output: {&quot;&quot; &quot;&quot;, &quot;&quot; &quot;_:e0047c979d3f4f7ebe4d980b2fd52b7293&quot;, &quot;&quot; &quot;&quot;} As you can see, the blank node _:e0047c979d3f4f7ebe4d980b2fd52b7293 is not resolved. However, I know it&apos;s there: (d/entity (d/db conn) &quot;_:e0047c979d3f4f7ebe4d980b2fd52b7293&quot; true) ;; Output: {&quot;&quot; &quot;\&quot;1\&quot;^^&lt;&gt;&quot;, &quot;&quot; &quot;\&quot;0\&quot;^^&lt;&gt;&quot;, &quot;&quot; &quot;&quot;, &quot;&quot; &quot;&quot;} Am I doing something wrong or misunderstanding something? Is there something I can do to achieve this?"><y>#</y><d>2022-05-15</d><h>09:06</h><w>Bart Kleijngeld</w>I&apos;ve succeeded in loading my RDF triples into Asami, but I can&apos;t get the <code>entity</code> function to work with <code>nested?</code> set to <code>true</code>.

My triples (examples for demonstration: everything is a string):
<pre>(take 2 rdf-data)

;; Output

([&quot;&quot;
  &quot;&quot;
  &quot;\&quot;1\&quot;^^&lt;&gt;&quot;]
 [&quot;&quot;
  &quot;&quot;
  &quot;&quot;])</pre>
Then I add them to the store:
<pre>@(d/transact conn {:tx-triples model})</pre>
Then, I&apos;m trying to use <code>entity</code> to retrieve some object (nested):
<pre>(d/entity (d/db conn) &quot;&quot; true)

;; Output:

{&quot;&quot; &quot;&quot;,
 &quot;&quot; &quot;_:e0047c979d3f4f7ebe4d980b2fd52b7293&quot;,
 &quot;&quot; &quot;&quot;}</pre>
As you can see, the blank node <code>_:e0047c979d3f4f7ebe4d980b2fd52b7293</code> is not resolved. However, I know it&apos;s there:
<pre>(d/entity (d/db conn) &quot;_:e0047c979d3f4f7ebe4d980b2fd52b7293&quot; true)

;; Output:

{&quot;&quot; &quot;\&quot;1\&quot;^^&lt;&gt;&quot;,
 &quot;&quot; &quot;\&quot;0\&quot;^^&lt;&gt;&quot;,
 &quot;&quot; &quot;&quot;,
 &quot;&quot; &quot;&quot;}</pre>
Am I doing something wrong or misunderstanding something? Is there something I can do to achieve this?</z><z id="t1652620603" t="quoll I&apos;m thinking that it&apos;s because EVERYTHING is a string. I turned off type checking so that strings could be in any position in a triple, but the entity loading code was written before that. It sees a string and presumes that&apos;s a leaf."><y>#</y><d>2022-05-15</d><h>13:16</h><w>quoll</w>I&apos;m thinking that it&apos;s because EVERYTHING is a string. I turned off type checking so that strings could be in any position in a triple, but the entity loading code was written before that. It sees a string and presumes that&apos;s a leaf.</z><z id="t1652620666" t="quoll Are you able to control the datatypes you&apos;re generating? I think you need to generate a node if you want to recurse into something "><y>#</y><d>2022-05-15</d><h>13:17</h><w>quoll</w>Are you able to control the datatypes you&apos;re generating? I think you need to generate a node if you want to recurse into something </z><z id="t1652620730" t="quoll I definitely need to update the entity reader if I want to allow URIs as nodes :thinking_face:"><y>#</y><d>2022-05-15</d><h>13:18</h><w>quoll</w>I definitely need to update the entity reader if I want to allow URIs as nodes <b>:thinking_face:</b></z><z id="t1652690992" t="Bart Kleijngeld I can imagine allowing URIs as nodes is something that doesn&apos;t necessarily fit in with your vision for the project, and requires some mulling over. Is it an option then, to, in the meantime, override the default reader, as is demonstrated in the README: ;; nodes can also be read from a string, with the appropriate reader =&gt; (set! *data-readers* graph/node-reader) =&gt; (d/entity db #a/n &quot;4&quot;) Not sure if I&apos;m understanding that correctly, I&apos;m a beginner 🙂 . To respond to your other question and suggestion: I&apos;m reading RDF Turtle files locally and can transform the data to my pleasing before putting it into Asami. So I&apos;m in full control over the data types and shape. If you say generating nodes manually is an easier or better route, I&apos;m willing to take it. I&apos;m not sure exactly how I would do this though. Would you then generate a node for every IRI in the subject position of the triple, and keep a hash map of these associations so when you encounter that IRI in an object position of triples, you can replace it by a reference to the associated node?"><y>#</y><d>2022-05-16</d><h>08:49</h><r>Bart Kleijngeld</r>I can imagine allowing URIs as nodes is something that doesn&apos;t necessarily fit in with your vision for the project, and requires some mulling over.

Is it an option then, to, in the meantime, override the default reader, as is demonstrated in the README:
<pre>;; nodes can also be read from a string, with the appropriate reader
=&gt; (set! *data-readers* graph/node-reader)
=&gt; (d/entity db #a/n &quot;4&quot;)</pre>
Not sure if I&apos;m understanding that correctly, I&apos;m a beginner <b>🙂</b>.

To respond to your other question and suggestion: I&apos;m reading RDF Turtle files locally and can transform the data to my pleasing before putting it into Asami. So I&apos;m in full control over the data types and shape.

If you say generating nodes manually is an easier or better route, I&apos;m willing to take it. I&apos;m not sure exactly how I would do this though. Would you then generate a node for every IRI in the subject position of the triple, and keep a hash map of these associations so when you encounter that IRI in an object position of triples, you can replace it by a reference to the associated node?</z><z id="t1652691121" t="Bart Kleijngeld It is of course possible that Asami isn&apos;t the best fit for working with RDF data this directly. But I really love Asami and it solves a lot of issues for us (structural loop detection, recursive entity fetching, and native Clojure query DSL among other things)"><y>#</y><d>2022-05-16</d><h>08:52</h><r>Bart Kleijngeld</r>It is of course possible that Asami isn&apos;t the best fit for working with RDF data this directly. But I really love Asami and it solves a lot of issues for us (structural loop detection, recursive entity fetching, and native Clojure query DSL among other things)</z><z id="t1652698955" t="quoll Well, it was inspired by a couple of things. It&apos;s an index/store that is built for RDF, but I added the entity code based on Datomic. It shouldn&apos;t be too bad… right now the entity code asks if the node that was just loaded is an internal node or keyword. If so, it recurses. I can add a test for URIs that also appear in a subject position. That should work without issue. It just results in an extra index lookup for URIs that aren&apos;t subjects anywhere. I think that&apos;s fine"><y>#</y><d>2022-05-16</d><h>11:02</h><r>quoll</r>Well, it was inspired by a couple of things. It&apos;s an index/store that is built for RDF, but I added the entity code based on Datomic.
It shouldn&apos;t be too bad… right now the entity code asks if the node that was just loaded is an internal node or keyword. If so, it recurses. I can add a test for URIs that also appear in a subject position. That should work without issue. It just results in an extra index lookup for URIs that aren&apos;t subjects anywhere. I think that&apos;s fine</z><z id="t1652699438" t="Bart Kleijngeld And I presume those URIs can be plain strings which are being parsed to check if they are URIs? Or does that have a performance penalty and must they be Java URI objects or some other variation?"><y>#</y><d>2022-05-16</d><h>11:10</h><r>Bart Kleijngeld</r>And I presume those URIs can be plain strings which are being parsed to check if they are URIs? Or does that have a performance penalty and must they be Java <code>URI</code>  objects or some other variation?</z><z id="t1652699995" t="quoll URI objects "><y>#</y><d>2022-05-16</d><h>11:19</h><r>quoll</r>URI objects </z><z id="t1652700153" t="quoll Because otherwise: • every string needs to be checked • URIs can be flexible, and it&apos;s too easy to mess it up. eg. “name:Bart” is a valid URI"><y>#</y><d>2022-05-16</d><h>11:22</h><r>quoll</r>Because otherwise:
• every string needs to be checked
• URIs can be flexible, and it&apos;s too easy to mess it up. eg. “name:Bart” is a valid URI</z><z id="t1652701026" t="quoll Anyway... this the RDF channel. We should be talking about this in #asami (I&apos;ve been responding using my phone, so I wasn&apos;t paying attention to where we were)"><y>#</y><d>2022-05-16</d><h>11:37</h><r>quoll</r>Anyway... this the RDF channel. We should be talking about this in #asami (I&apos;ve been responding using my phone, so I wasn&apos;t paying attention to where we were)</z><z id="t1652701317" t="Bart Kleijngeld Yes, that makes sense. And I wasn&apos;t aware of that channel, I&apos;ve joined it now. You&apos;re right, let&apos;s continue over there."><y>#</y><d>2022-05-16</d><h>11:41</h><r>Bart Kleijngeld</r>Yes, that makes sense.

And I wasn&apos;t aware of that channel, I&apos;ve joined it now. You&apos;re right, let&apos;s continue over there.</z><z id="t1652702144" t="Bart Kleijngeld Should I copy/paste our conversation over there? Or what is the best way forward"><y>#</y><d>2022-05-16</d><h>11:55</h><r>Bart Kleijngeld</r>Should I copy/paste our conversation over there? Or what is the best way forward</z><z id="t1652704119" t="quoll Just keep talking over there. It&apos;s fine"><y>#</y><d>2022-05-16</d><h>12:28</h><r>quoll</r>Just keep talking over there. It&apos;s fine</z><z id="t1652620826" t="quoll I can try to spend time online today, but right now I’m about to make pancakes for my kids (and coffee for me)"><y>#</y><d>2022-05-15</d><h>13:20</h><w>quoll</w>I can try to spend time online today, but right now I’m about to make pancakes for my kids (and coffee for me)</z><z id="t1652688997" t="Bart Kleijngeld That&apos;s really nice of you, I&apos;m happy with the help I&apos;m receiving from you (and others here)! And sounds like you have your priorities sorted out well 😄 ."><y>#</y><d>2022-05-16</d><h>08:16</h><r>Bart Kleijngeld</r>That&apos;s really nice of you, I&apos;m happy with the help I&apos;m receiving from you (and others here)!

And sounds like you have your priorities sorted out well <b>😄</b>.</z><z id="t1653377824" t="simongray FYI: I spent some time yesterday evening updating the https://github.com/simongray/clojure-graph-resources that I maintain."><y>#</y><d>2022-05-24</d><h>07:37</h><w>simongray</w>FYI: I spent some time yesterday evening updating the  <a href="https://github.com/simongray/clojure-graph-resources" target="_blank">https://github.com/simongray/clojure-graph-resources</a> that I maintain.</z><z id="t1653378306" t="Bart Kleijngeld Really useful, thanks!"><y>#</y><d>2022-05-24</d><h>07:45</h><r>Bart Kleijngeld</r>Really useful, thanks!</z><z id="t1653688791" t="abdullahibra hey guys, is there anybody using asami in production?"><y>#</y><d>2022-05-27</d><h>21:59</h><w>abdullahibra</w>hey guys, is there anybody using asami in production?</z><z id="t1653693838" t="quoll Cisco was. They were shifting though, so I can&apos;t say if they are right now"><y>#</y><d>2022-05-27</d><h>23:23</h><w>quoll</w>Cisco was. They were shifting though, so I can&apos;t say if they are right now</z><z id="t1655301414" t="Kelvin So I was playing around with Apache Jena&apos;s https://jena.apache.org/documentation/javadoc/arq/org/apache/jena/query/ResultSetFormatter.html class (in order to work with the SPARQL Protocol). Each method accepts either a https://jena.apache.org/documentation/javadoc/arq/org/apache/jena/query/ResultSet.html or a boolean, and since the latter was easier to work with I was using that. And here are the results: (defmacro -&gt;format-str &quot;Macro to work with ResultSetFormatter to return strings.&quot; [f this] `(with-open [~&apos;baos (java.io.ByteArrayOutputStream.)] (~f ~&apos;baos ~this) (String. (.toByteArray ~&apos;baos)))) (-&gt;format-str ResultSetFormatter/outputAsJSON true) ; Result: &quot;{ \n \&quot;head\&quot; : { } ,\n \&quot;boolean\&quot; : true\n}\n&quot; (-&gt;format-str ResultSetFormatter/outputAsXML true) ; Result: &quot;&lt;?xml version=\&quot;1.0\&quot;?&gt;\n&lt;sparql xmlns=\&quot;\&quot;&gt;\n &lt;head&gt;\n &lt;/head&gt;\n &lt;boolean&gt;true&lt;/boolean&gt;\n&lt;/sparql&gt;\n&quot; (-&gt;format-str ResultSetFormatter/outputAsCSV true) ; Result: &quot;&quot;"><y>#</y><d>2022-06-15</d><h>13:56</h><w>Kelvin</w>So I was playing around with Apache Jena&apos;s <a href="https://jena.apache.org/documentation/javadoc/arq/org/apache/jena/query/ResultSetFormatter.html" target="_blank">https://jena.apache.org/documentation/javadoc/arq/org/apache/jena/query/ResultSetFormatter.html</a> class (in order to work with the SPARQL Protocol). Each method accepts either a <a href="https://jena.apache.org/documentation/javadoc/arq/org/apache/jena/query/ResultSet.html" target="_blank">https://jena.apache.org/documentation/javadoc/arq/org/apache/jena/query/ResultSet.html</a> or a boolean, and since the latter was easier to work with I was using that. And here are the results:
<pre>(defmacro -&gt;format-str
  &quot;Macro to work with ResultSetFormatter to return strings.&quot;
  [f this]
  `(with-open [~&apos;baos (java.io.ByteArrayOutputStream.)]
     (~f ~&apos;baos ~this)
     (String. (.toByteArray ~&apos;baos))))

(-&gt;format-str ResultSetFormatter/outputAsJSON true)

; Result: &quot;{ \n  \&quot;head\&quot; : { } ,\n  \&quot;boolean\&quot; : true\n}\n&quot;

(-&gt;format-str ResultSetFormatter/outputAsXML true)

; Result: &quot;&lt;?xml version=\&quot;1.0\&quot;?&gt;\n&lt;sparql xmlns=\&quot;\&quot;&gt;\n  &lt;head&gt;\n  &lt;/head&gt;\n  &lt;boolean&gt;true&lt;/boolean&gt;\n&lt;/sparql&gt;\n&quot;

(-&gt;format-str ResultSetFormatter/outputAsCSV true)

; Result: &quot;&quot;</pre></z><z id="t1655301511" t="Kelvin As you can see, for JSON and XML the formatted results make sense, but for CSV the result becomes an empty string. This seems &quot;wrong,&quot; and I haven&apos;t been able to find much documentation on what the result for ASK queries (which return booleans) should be"><y>#</y><d>2022-06-15</d><h>13:58</h><w>Kelvin</w>As you can see, for JSON and XML the formatted results make sense, but for CSV the result becomes an empty string. This seems &quot;wrong,&quot; and I haven&apos;t been able to find much documentation on what the result for ASK queries (which return booleans) should be</z><z id="t1655391557" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U02FU7RMG8M&quot;}] : I think I’ve come across this before, and it’s something that’s just not part of the standards… AFAIK there’s no standard that even says triple stores should support text/csv at all, let alone a standard on how that format should map to non tabular queries. I think there are essentially two options for endpoint implementers — return a 406 (not acceptable) or map the result to a non standard that looks something like: boolean true"><y>#</y><d>2022-06-16</d><h>14:59</h><w>rickmoynihan</w><a>@kelvin063</a>: I think I’ve come across this before, and it’s something that’s just not part of the standards… AFAIK there’s no standard that even says triple stores should support text/csv at all, let alone a standard on how that format should map to non tabular queries.

I think there are essentially two options for endpoint implementers — return a 406 (not acceptable) or map the result to a non standard that looks something like:

<pre>boolean
true</pre></z><z id="t1655392442" t="Kelvin &gt; there’s no standard that even says triple stores should support text/csv The standard does specify how text/csv should be formatted for SELECT response: https://www.w3.org/TR/sparql11-results-csv-tsv/ . The issue is that for ASK queries it seems to be undefined. &gt; or map the result to a non standard that looks something like My tentative solution is to just manually return a true or false string since that is (as long as you add the right EOF chars) technically valid CSV or TSV. I think for a case like this, it would be better to return some information to the user rather than give them no info in a 406."><y>#</y><d>2022-06-16</d><h>15:14</h><r>Kelvin</r>&gt; there’s no standard that even says triple stores should support text/csv
The standard does specify how text/csv should be formatted for SELECT response: <a href="https://www.w3.org/TR/sparql11-results-csv-tsv/" target="_blank">https://www.w3.org/TR/sparql11-results-csv-tsv/</a>. The issue is that for ASK queries it seems to be undefined.
&gt; or map the result to a non standard that looks something like
My tentative solution is to just manually return a <code>true</code> or <code>false</code> string since that is (as long as you add the right EOF chars) technically valid CSV or TSV. I think for a case like this, it would be better to return some information to the user rather than give them no info in a 406.</z><z id="t1655457766" t="rickmoynihan Ahh yes you’re right, my bad, I should have double checked to remind myself - it’s been 6 years since I last implemented the specs for sparql and the sparql protocol, though I’ve done it twice! Regardless I think the two options I mention are your options… 1. Return a 406 (and essentially insist they either set application/sparql-results+(json|xml) 2. Return a CSV with some boolean column in it with the true or false as the value. In my experience some endpoints do 1 (e.g. stardog - and our drafter endpoints do) whilst others e.g. fuseki allow for 2, even though it’s non standard. I wouldn’t return just true or false though, but would also return a header row to ensure the format is at least structurally the same as what is mandated by the sparql-results csv-tsv spec. The problem then is that the choice of column name is not standardised…. We fudge this on our SPARQL query UI to look like tables (even though we implement option 1 underneath) and call the column boolean , whilst fuseki calls the column _askResult . Personally I think 1 is the most correct option and it encourages portability and correct handling of sparql query types and content types in client code. i.e. clients wont risk ever falling outside of the standard as might if you offered this extension. It’s probably also worth mentioning that a lot of the behaviour rests on RFC2616; and that providing endpoints implement content negotiation properly that one option which means clients can avoid having to detect/parse the query type is to NOT support text/csv for ASK queries but to allow clients to declare they can accept multiple Accept headers. i.e. a client could send: Accept: text/csv,application/sparql-results+json,text/turtle And then it can determine the query type by attempting to parse the response as JSON, turtle and finally csv (though actually why bother with CSV at all here). Personally I prefer option 1, as it encourages users and clients to be standards compliant… though if clients were written against fuseki they may accidentally be depending on _askResult as a column in their CSV. So 🤷 YMMV"><y>#</y><d>2022-06-17</d><h>09:22</h><r>rickmoynihan</r>Ahh yes you’re right, my bad, I should have double checked to remind myself - it’s been 6 years since I last implemented the specs for sparql and the sparql protocol, though I’ve done it twice!

Regardless I think the two options I mention are your options…

1. Return a 406 (and essentially insist they either set <code>application/sparql-results+(json|xml)</code>
2. Return a CSV with some <code>boolean</code> column in it with the <code>true</code> or <code>false</code> as the value.
In my experience some endpoints do 1 (e.g. stardog - and our drafter endpoints do) whilst others e.g. fuseki allow for 2, even though it’s non standard.

I wouldn’t return just <code>true</code> or <code>false</code> though, but would also return a header row to ensure the format is at least structurally the same as what is mandated by the sparql-results csv-tsv spec.  The problem then is that the choice of column name is not standardised…. We fudge this on our SPARQL query UI to look like tables (even though we implement option 1 underneath) and call the column <code>boolean</code>, whilst fuseki calls the column <code>_askResult</code>.

Personally I think 1 is the most correct option and it encourages portability and correct handling of sparql query types and content types in client code.  i.e. clients wont risk ever falling outside of the standard as might if you offered this extension.

It’s probably also worth mentioning that a lot of the behaviour rests on RFC2616; and that providing endpoints implement content negotiation properly that one option which means clients can avoid having to detect/parse the query type is to NOT support <code>text/csv</code> for <code>ASK</code> queries but to allow clients to declare they can accept multiple <code>Accept</code> headers.

i.e. a client could send:

<code>Accept: text/csv,application/sparql-results+json,text/turtle</code>

And then it can determine the query type by attempting to parse the response as JSON, turtle and finally csv (though actually why bother with CSV at all here).

Personally I prefer option 1, as it encourages users and clients to be standards compliant… though if clients were written against fuseki they may accidentally be depending on <code>_askResult</code> as a column in their CSV.

So <b>🤷</b> YMMV</z><z id="t1655457865" t="rickmoynihan Incidentally I seem to vaguely recall some endpoints supporting text/plain for ASK queries; which would just return true or false , but again it’s non standard."><y>#</y><d>2022-06-17</d><h>09:24</h><r>rickmoynihan</r>Incidentally I seem to vaguely recall some endpoints supporting <code>text/plain</code> for <code>ASK</code> queries; which would just return <code>true</code> or <code>false</code>, but again it’s non standard.</z><z id="t1655476104" t="Kelvin Hmm digging into https://w3c.github.io/rdf-tests/sparql11/data-sparql11/protocol/index.html#http://www.w3.org/2009/sparql/docs/tests/data-sparql11/protocol/manifest#query_content_type_construct , you can clearly see that it&apos;s not expected that ASK queries support CSV or TSV responses. So you&apos;re definitely onto something."><y>#</y><d>2022-06-17</d><h>14:28</h><r>Kelvin</r>Hmm digging into <a href="https://w3c.github.io/rdf-tests/sparql11/data-sparql11/protocol/index.html#http://www.w3.org/2009/sparql/docs/tests/data-sparql11/protocol/manifest#query_content_type_construct" target="_blank">https://w3c.github.io/rdf-tests/sparql11/data-sparql11/protocol/index.html#http://www.w3.org/2009/sparql/docs/tests/data-sparql11/protocol/manifest#query_content_type_construct</a>, you can clearly see that it&apos;s not expected that ASK queries support CSV or TSV responses. So you&apos;re definitely onto something.</z><z id="t1655391657" t="rickmoynihan From the code you pasted though it looks like you’re not interacting with the sparql protocol at all; your accessing internal classes. Also I suspect the issue may just be introduced by building a string from an empty byte array… i.e. perhaps Jena is just returning null in this case, and its abstraction (the polymorphism on outputAsCSV) may just be leaky"><y>#</y><d>2022-06-16</d><h>15:00</h><w>rickmoynihan</w>From the code you pasted though it looks like you’re not interacting with the sparql protocol at all; your accessing internal classes.

Also I suspect the issue may just be introduced by building a string from an empty byte array… i.e. perhaps Jena is just returning <code>null</code> in this case, and its abstraction (the polymorphism on outputAsCSV) may just be leaky</z><z id="t1655394448" t="Kelvin Yeah I&apos;m building my own /sparql endpoint here"><y>#</y><d>2022-06-16</d><h>15:47</h><r>Kelvin</r>Yeah I&apos;m building my own <code>/sparql</code> endpoint here</z><z id="t1655393106" t="quoll I need to look, but I thought that TSV files were part of the standard? I know that they were discussed a LOT in the SPARQL committee"><y>#</y><d>2022-06-16</d><h>15:25</h><w>quoll</w>I need to look, but I thought that TSV files were part of the standard? I know that they were discussed a LOT in the SPARQL committee</z><z id="t1655455116" t="rickmoynihan yes you’re right — I’d forgotten and should have double checked!"><y>#</y><d>2022-06-17</d><h>08:38</h><r>rickmoynihan</r>yes you’re right — I’d forgotten and should have double checked!</z><z id="t1655394404" t="Kelvin &gt; I need to look, but I thought that TSV files were part of the standard? I know that they were discussed a LOT in the SPARQL committee TSV files for SELECT queries, yes"><y>#</y><d>2022-06-16</d><h>15:46</h><w>Kelvin</w>&gt; I need to look, but I thought that TSV files were part of the standard? I know that they were discussed a LOT in the SPARQL committee
TSV files for SELECT queries, yes</z><z id="t1655394412" t="Kelvin TSV files for ASK queries, not so much apparently"><y>#</y><d>2022-06-16</d><h>15:46</h><w>Kelvin</w>TSV files for ASK queries, not so much apparently</z><z id="t1655396125" t="quoll Makes sense. It doesn’t really have a lot of use (much more structure than required for a true/false response)"><y>#</y><d>2022-06-16</d><h>16:15</h><w>quoll</w>Makes sense. It doesn’t really have a lot of use (much more structure than required for a true/false response)</z><z id="t1655396686" t="Kelvin Sometimes an existence check is all I need in a query, so I find ASK queries useful"><y>#</y><d>2022-06-16</d><h>16:24</h><w>Kelvin</w>Sometimes an existence check is all I need in a query, so I find ASK queries useful</z><z id="t1655396731" t="Kelvin Hence my motivation to cover all the bases"><y>#</y><d>2022-06-16</d><h>16:25</h><w>Kelvin</w>Hence my motivation to cover all the bases</z><z id="t1655396796" t="quoll Oh, for sure. That’s why ASK was created. But returning the result in TSV/CSV imposes structure that is beyond what is necessary"><y>#</y><d>2022-06-16</d><h>16:26</h><w>quoll</w>Oh, for sure. That’s why ASK was created. But returning the result in TSV/CSV imposes structure that is beyond what is necessary</z><z id="t1655488282" t="Kelvin Another thing: alot of the content types I&apos;ve been seeing in the SPARQL specs, test cases, etc. are not listed in the IANA list of official MIME types: https://www.iana.org/assignments/media-types/media-types.xhtml"><y>#</y><d>2022-06-17</d><h>17:51</h><w>Kelvin</w>Another thing: alot of the content types I&apos;ve been seeing in the SPARQL specs, test cases, etc. are not listed in the IANA list of official MIME types: <a href="https://www.iana.org/assignments/media-types/media-types.xhtml" target="_blank">https://www.iana.org/assignments/media-types/media-types.xhtml</a></z><z id="t1655488299" t="Kelvin The two most common examples being application/sparql-results+json and application/rdf+json"><y>#</y><d>2022-06-17</d><h>17:51</h><w>Kelvin</w>The two most common examples being <code>application/sparql-results+json</code> and <code>application/rdf+json</code></z><z id="t1655712947" t="rickmoynihan ha good find"><y>#</y><d>2022-06-20</d><h>08:15</h><w>rickmoynihan</w>ha good find</z><z id="t1655759079" t="Al Baker ask returns a boolean, CSV/TSV is mentioned in the spec for SELECT queries: https://www.w3.org/TR/sparql11-protocol/#conneg"><y>#</y><d>2022-06-20</d><h>21:04</h><w>Al Baker</w>ask returns a boolean, CSV/TSV is mentioned in the spec for SELECT queries: <a href="https://www.w3.org/TR/sparql11-protocol/#conneg" target="_blank">https://www.w3.org/TR/sparql11-protocol/#conneg</a></z><z id="t1655759236" t="Al Baker but really vendors should follow the advise of the spec which is to properly support content negotiation and then add these representations as appropriate (obviously with the xml/json sparql serialization responses being baseline). E.g. we added the json-ld for Stardog. For a while that was somewhat unique, though I think most vendors implement it now."><y>#</y><d>2022-06-20</d><h>21:07</h><w>Al Baker</w>but really vendors should follow the advise of the spec which is to properly support content negotiation and then add these representations as appropriate (obviously with the xml/json sparql serialization responses being baseline). E.g. we added the json-ld for Stardog. For a while that was somewhat unique, though I think most vendors implement it now.</z><z id="t1655759607" t="Al Baker one of the things I want to do is given that stardog supports server managed namespace prefixes - we could just return the qname, or maybe even have a representation that just returned the local name and dropped the qualified piece altogether. Obviously it&apos;d still be a proper URI in the database, but that&apos;d open up a bunch of use cases where apps don&apos;t/can&apos;t deal with URIs, or even qnames. i.e. it&apos;s just going to be a string name of a thing elevated up and shared as this representation."><y>#</y><d>2022-06-20</d><h>21:13</h><w>Al Baker</w>one of the things I want to do is given that stardog supports server managed namespace prefixes - we could just return the qname, or maybe even have a representation that just returned the local name and dropped the qualified piece altogether. Obviously it&apos;d still be a proper URI in the database, but that&apos;d open up a bunch of use cases where apps don&apos;t/can&apos;t deal with URIs, or even qnames. i.e. it&apos;s just going to be a string name of a thing elevated up and shared as this representation.</z><z id="t1655815176" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U0DCK21EJ&quot;}] : I’ve had similar thoughts about curies/qnames — but I think for SPARQL and consumers the problem is that the edge cases where a prefix for a URI in the database doesn’t exist kill the viability/utility. i.e. I think for those scenarios you need to validate that all data in the endpoint on ingestion has a prefix associated with its URI, as you’re building a syntactic dependency on the exact shorthand names (or risking collisions)."><y>#</y><d>2022-06-21</d><h>12:39</h><w>rickmoynihan</w><a>@albaker</a>: I’ve had similar thoughts about curies/qnames — but I think for SPARQL and consumers the problem is that the edge cases where a prefix for a URI in the database doesn’t exist kill the viability/utility.

i.e. I think for those scenarios you need to validate that all data in the endpoint on ingestion has a prefix associated with its URI, as you’re building a syntactic dependency on the exact shorthand names (or risking collisions).</z><z id="t1655815466" t="rickmoynihan This kind of thing would also make mapping to graphql schemas a lot easier."><y>#</y><d>2022-06-21</d><h>12:44</h><w>rickmoynihan</w>This kind of thing would also make mapping to graphql schemas a lot easier.</z><z id="t1655815467" t="rickmoynihan I like JSON-LD but it really opened the floodgates on URIs no longer just being opaque semantic identifiers, but there also being a syntactic representation that consumers can depend on. Another example of this trend is the CSVW vocabulary; built on JSON-LD it isn’t just interpretable as an RDF vocabulary, but also as a syntactic/structural JSON vocabulary. This meant that then the standard for CSVW isn’t really JSON-LD; it’s just a subset of it as it necessarily imposes limitations on the use of JSON-LD contexts etc, and also had to standardise interpretation of the format by essentially inlining chunks of the JSON-LD spec in the CSVW, so it could impose those dialect restrictions."><y>#</y><d>2022-06-21</d><h>12:44</h><w>rickmoynihan</w>I like JSON-LD but it really opened the floodgates on URIs no longer just being opaque semantic identifiers, but there also being a syntactic representation that consumers can depend on.

Another example of this trend is the CSVW vocabulary; built on JSON-LD it isn’t just interpretable as an RDF vocabulary, but also as a syntactic/structural JSON vocabulary.  This meant that then the standard for CSVW isn’t really JSON-LD; it’s just a subset of it as it necessarily imposes limitations on the use of JSON-LD contexts etc, and also had to standardise interpretation of the format by essentially inlining chunks of the JSON-LD spec in the CSVW, so it could impose those dialect restrictions.</z><z id="t1655986249" t="Al Baker [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] it wouldn&apos;t be a silver bullet for sure -- but take like Stardog&apos;s stored query = REST service approach. Having this response format option would make it look like vanilla JSON service... this URL just happens to run this SPARQL query, and you get your response out as something easy to deal with. We do that similar type of translation stuff for graphql in Stardog already, and it&apos;s alright - plenty of folks are using it, even in combo w/ stuff like Apollo. But you still wind up with URI or prefixes in the mix"><y>#</y><d>2022-06-23</d><h>12:10</h><w>Al Baker</w><a>@rickmoynihan</a> it wouldn&apos;t be a silver bullet for sure -- but take like Stardog&apos;s stored query = REST service approach. Having this response format option would make it look like vanilla JSON service... this URL just happens to run this SPARQL query, and you get your response out as something easy to deal with.  We do that similar type of translation stuff for graphql in Stardog already, and it&apos;s alright - plenty of folks are using it, even in combo w/ stuff like Apollo. But you still wind up with URI or prefixes in the mix</z><z id="t1655986952" t="rickmoynihan Yeah that’s quite neat… My only real problem with prefixes in these contexts for API’s is handling them in a non-breaking manner with data from an open world. i.e. a new URI term appears, and consumers adopt it by it’s full URI; but it stands out like a sore thumb when other identifiers have prefixes registered… so you add a prefix for it, but then break existing consumers."><y>#</y><d>2022-06-23</d><h>12:22</h><r>rickmoynihan</r>Yeah that’s quite neat…

My only real problem with prefixes in these contexts for API’s is handling them in a non-breaking manner with data from an open world.

i.e. a new URI term appears, and consumers adopt it by it’s full URI; but it stands out like a sore thumb when other identifiers have prefixes registered… so you add a prefix for it, but then break existing consumers.</z><z id="t1655987074" t="rickmoynihan So then you have to ask what is the point. You might as well just use the URIs; though there are more dimensions to this decision for sure."><y>#</y><d>2022-06-23</d><h>12:24</h><r>rickmoynihan</r>So then you have to ask what is the point.  You might as well just use the URIs; though there are more dimensions to this decision for sure.</z><z id="t1655986321" t="Al Baker my original use case for this was I think the early verions of Gephi didn&apos;t deal with URIs nicely, so you&apos;d load it up but then you&apos;d get URLs w/ poor UX on truncating, so the whole thing was accurate on shape/colors, but you had to double click to see what was really going on, since every label was effectively the same (the first 16 characters of the URI, then truncated with a few dots or whatever it was)."><y>#</y><d>2022-06-23</d><h>12:12</h><w>Al Baker</w>my original use case for this was I think the early verions of Gephi didn&apos;t deal with URIs nicely, so you&apos;d load it up but then you&apos;d get URLs w/ poor UX on truncating, so the whole thing was accurate on shape/colors, but you had to double click to see what was really going on, since every label was effectively the same (the first 16 characters of the URI, then truncated with a few dots or whatever it was).</z><z id="t1655986370" t="Al Baker revisiting Gephi is on my list... I&apos;ve been wanting one of those larger scale views of the graph, that wasn&apos;t necessarily readable, but would give you maybe edge/class colorization and you could get a visual indication on how much of what was present"><y>#</y><d>2022-06-23</d><h>12:12</h><w>Al Baker</w>revisiting Gephi is on my list... I&apos;ve been wanting one of those larger scale views of the graph, that wasn&apos;t necessarily readable, but would give you maybe edge/class colorization and you could get a visual indication on how much of what was present</z><z id="t1656678445" t="simongray https://github.com/arachne-framework/aristotle/pull/12"><y>#</y><d>2022-07-01</d><h>12:27</h><w>simongray</w><a href="https://github.com/arachne-framework/aristotle/pull/12" target="_blank">https://github.com/arachne-framework/aristotle/pull/12</a></z><z id="t1656678455" t="simongray FYI"><y>#</y><d>2022-07-01</d><h>12:27</h><w>simongray</w>FYI</z><z id="t1656682067" t="quoll I don’t know how many people may have heard of my old project “Naga”. It’s a production rules engine for graph databases that creates new data based on existing data and rules. It works on Datomic and Asami. I’d always planned on building an adapter to allow it to talk to SPARQL, but hadn’t got around to it before now. Well, it turns out that I may need this for real. Unfortunately, for the rules I need I either have to expand the rules vocabulary, or parse SPARQL so I can do a SPIN-style system. That’s a lot of work, so I figured I’d ask… are there any general SPARQL production rules engines already? I guess SPIN would work, but I’m thinking that only works on Jena, or am I wrong about that?"><y>#</y><d>2022-07-01</d><h>13:27</h><w>quoll</w>I don’t know how many people may have heard of my old project “Naga”. It’s a production rules engine for graph databases that creates new data based on existing data and rules. It works on Datomic and Asami. I’d always planned on building an adapter to allow it to talk to SPARQL, but hadn’t got around to it before now.
Well, it turns out that I may need this for real. Unfortunately, for the rules I need I either have to expand the rules vocabulary, or parse SPARQL so I can do a SPIN-style system. That’s a lot of work, so I figured I’d ask… are there any general SPARQL production rules engines already? I guess SPIN would work, but I’m thinking that only works on Jena, or am I wrong about that?</z><z id="t1656690817" t="mlad.vladimir [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] there (deprecated) RDF4J SPIN implementation https://rdf4j.org/documentation/programming/spin/ as well. SHACL rules are often suggested as substitution for SPIN."><y>#</y><d>2022-07-01</d><h>15:53</h><w>mlad.vladimir</w><a>@quoll</a> there (deprecated) RDF4J SPIN implementation <a href="https://rdf4j.org/documentation/programming/spin/" target="_blank">https://rdf4j.org/documentation/programming/spin/</a> as well.
SHACL rules are often suggested as substitution for SPIN.</z><z id="t1656702729" t="quoll Yeah, SHACL is a [:attrs nil] short of what I need"><y>#</y><d>2022-07-01</d><h>19:12</h><w>quoll</w>Yeah, SHACL is a <b>long way</b> short of what I need</z><z id="t1656702903" t="quoll Incidentally, you can implement SHACL with SPIN. You can’t implement SPIN functionality with SHACL"><y>#</y><d>2022-07-01</d><h>19:15</h><r>quoll</r>Incidentally, you can implement SHACL with SPIN. You can’t implement SPIN functionality with SHACL</z><z id="t1656702745" t="quoll Besides, we already have SHACL"><y>#</y><d>2022-07-01</d><h>19:12</h><w>quoll</w>Besides, we already have SHACL</z><z id="t1656702868" t="quoll I did wonder about trying to update the RDF4J implementation of SPIN to talk to SPARQL. I haven’t done Sesame in a long, long time 🙂"><y>#</y><d>2022-07-01</d><h>19:14</h><w>quoll</w>I did wonder about trying to update the RDF4J implementation of SPIN to talk to SPARQL. I haven’t done Sesame in a long, long time <b>🙂</b></z><z id="t1656703819" t="quoll (When I said “Jena” I was thinking about TopQuadrant. I don’t know if SPIN actually works on public Jena)"><y>#</y><d>2022-07-01</d><h>19:30</h><w>quoll</w>(When I said “Jena” I was thinking about TopQuadrant. I don’t know if SPIN actually works on public Jena)</z><z id="t1659361369" t="Kelvin Question about SPARQL"><y>#</y><d>2022-08-01</d><h>13:42</h><w>Kelvin</w>Question about SPARQL</z><z id="t1659361416" t="Kelvin So I’m revisiting the https://github.com/yetanalytics/flint library and one of the changes I’m proposing is to prevent aggregate expressions from being nested inside other aggregates."><y>#</y><d>2022-08-01</d><h>13:43</h><w>Kelvin</w>So I’m revisiting the <a href="https://github.com/yetanalytics/flint" target="_blank">https://github.com/yetanalytics/flint</a> library and one of the changes I’m proposing is to prevent aggregate expressions from being nested inside other aggregates.</z><z id="t1659361477" t="Kelvin That is because that is not allowed in Apache Jena (which I’m using as my reference implementation); you’d get this exception: ; Execution error (QueryParseException) at org.apache.jena.sparql.lang.QueryParserBase/throwParseException (QueryParserBase.java:578). ; Line [x], column [y]: Nested aggregate in expression not legal"><y>#</y><d>2022-08-01</d><h>13:44</h><w>Kelvin</w>That is because that is not allowed in Apache Jena (which I’m using as my reference implementation); you’d get this exception:
<pre>; Execution error (QueryParseException) at org.apache.jena.sparql.lang.QueryParserBase/throwParseException (QueryParserBase.java:578).
; Line [x], column [y]: Nested aggregate in expression not legal</pre></z><z id="t1659371591" t="Eric Scott Can you give an example of such a nested aggregate?"><y>#</y><d>2022-08-01</d><h>16:33</h><r>Eric Scott</r>Can you give an example of such a nested aggregate?</z><z id="t1659374678" t="Kelvin SELECT (MAX(AVG(?x)) AS ?maxAvg) WHERE { ?x ?y ?z . } GROUP BY ?z"><y>#</y><d>2022-08-01</d><h>17:24</h><r>Kelvin</r><pre>SELECT (MAX(AVG(?x)) AS ?maxAvg)
WHERE {
    ?x ?y ?z .
}
GROUP BY ?z</pre></z><z id="t1659374695" t="Kelvin Does it make semantic sense? Not really (which is probably why Jena has that restriction)"><y>#</y><d>2022-08-01</d><h>17:24</h><r>Kelvin</r>Does it make semantic sense? Not really (which is probably why Jena has that restriction)</z><z id="t1659374765" t="Kelvin For the record this is totally legal: SELECT (AVG(?x) AS ?avg) (MAX(?avg) AS ?maxAvg) WHERE { ?x ?y ?z . } GROUP BY ?z"><y>#</y><d>2022-08-01</d><h>17:26</h><r>Kelvin</r>For the record this is totally legal:
<pre>SELECT (AVG(?x) AS ?avg) (MAX(?avg) AS ?maxAvg)
WHERE {
  ?x ?y ?z .
}
GROUP BY ?z</pre></z><z id="t1659377839" t="Eric Scott I agree that doesn&apos;t make logical sense to take the aggregate of an aggregate."><y>#</y><d>2022-08-01</d><h>18:17</h><r>Eric Scott</r>I agree that doesn&apos;t make logical sense to take the aggregate of an aggregate.</z><z id="t1659378019" t="Kelvin Which means even if it’s technically allowed in the syntax, I’ll likely add that restriction because a) it doesn’t make semantic sense and b) we’d don’t really want syntax that’s not supported by certain impls like Jena, right?"><y>#</y><d>2022-08-01</d><h>18:20</h><r>Kelvin</r>Which means even if it’s technically allowed in the syntax, I’ll likely add that restriction because a) it doesn’t make semantic sense and b) we’d don’t really want syntax that’s not supported by certain impls like Jena, right?</z><z id="t1659386655" t="Eric Scott Yeah"><y>#</y><d>2022-08-01</d><h>20:44</h><r>Eric Scott</r>Yeah</z><z id="t1659429373" t="rickmoynihan In my mind adding a restriction like this puts you on the slope of implementing a poor mans type checker; or at least you’d be mixing concerns of static analysis with parsing. As you’ve noted it looks like this is visible in Jena’s implementation. They’re denying the first form as syntactically invalid (when it’s not), but can’t detect it when the functions aren’t composed directly, and an intermediate variable is introduced. By way of analogy; this is a perfectly valid clojure form: (inc &quot;foo&quot;) but it’s a semantically incorrect program; and will result in an evaluation error when run. The error isn’t detected in the parser however. I think static analysis features are super useful; but if implemented that they should occur in a separate layer / concern; for example a linter like clj-kondo or type checker."><y>#</y><d>2022-08-02</d><h>08:36</h><r>rickmoynihan</r>In my mind adding a restriction like this puts you on the slope of implementing a poor mans type checker; or at least you’d be mixing concerns of static analysis with parsing.

As you’ve noted it looks like this is visible in Jena’s implementation.  They’re denying the first form as syntactically invalid (when it’s not), but can’t detect it when the functions aren’t composed directly, and an intermediate variable is introduced.

By way of analogy; this is a perfectly valid clojure form:  <code>(inc &quot;foo&quot;)</code> but it’s a  semantically incorrect program; and will result in an evaluation error when run.

The error isn’t detected in the parser however.

I think static analysis features are super useful; but if implemented that they should occur in a separate layer / concern; for example a linter like <code>clj-kondo</code> or type checker.</z><z id="t1659429489" t="rickmoynihan Out of interest what does Jena return when you run this? https://clojurians.slack.com/archives/C09GHBXRC/p1659374765264119?thread_ts=1659361477.275269&amp;amp;cid=C09GHBXRC"><y>#</y><d>2022-08-02</d><h>08:38</h><r>rickmoynihan</r>Out of interest what does Jena return when you run this?

<a href="https://clojurians.slack.com/archives/C09GHBXRC/p1659374765264119?thread_ts=1659361477.275269&amp;amp;cid=C09GHBXRC" target="_blank">https://clojurians.slack.com/archives/C09GHBXRC/p1659374765264119?thread_ts=1659361477.275269&amp;amp;cid=C09GHBXRC</a></z><z id="t1659462396" t="Kelvin I did some playing around in the https://sparql-playground.sib.swiss/ . The following: SELECT (COUNT(?s) AS ?countS) (MAX(?countS) AS ?maxCountS) WHERE { ?s ?p ?o } LIMIT 10 returns 14 . If you remove the MAX expression binding then ?countS is also 14 ."><y>#</y><d>2022-08-02</d><h>17:46</h><r>Kelvin</r>I did some playing around in the <a href="https://sparql-playground.sib.swiss/" target="_blank">https://sparql-playground.sib.swiss/</a>. The following:
<pre>SELECT (COUNT(?s) AS ?countS) (MAX(?countS) AS ?maxCountS) WHERE {
  ?s ?p ?o
}
LIMIT 10</pre>
returns <code>14</code>. If you remove the <code>MAX</code> expression binding then <code>?countS</code> is also <code>14</code>.</z><z id="t1659462422" t="Kelvin So MAX at least returns the max of a singleton collection, which actually makes sense"><y>#</y><d>2022-08-02</d><h>17:47</h><r>Kelvin</r>So <code>MAX</code> at least returns the max of a singleton collection, which actually makes sense</z><z id="t1659463005" t="Kelvin WAIT OOPS I misread the results"><y>#</y><d>2022-08-02</d><h>17:56</h><r>Kelvin</r>WAIT OOPS I misread the results</z><z id="t1659463025" t="Kelvin ?countS is 14 but ?maxCountS returns nothing"><y>#</y><d>2022-08-02</d><h>17:57</h><r>Kelvin</r><code>?countS</code> is <code>14</code> but <code>?maxCountS</code> returns nothing</z><z id="t1659464136" t="Kelvin And of course I did the nested aggregates:"><y>#</y><d>2022-08-02</d><h>18:15</h><r>Kelvin</r>And of course I did the nested aggregates:</z><z id="t1659464139" t="Kelvin SELECT (MAX(COUNT(?s)) AS ?maxCountS) WHERE { ?s ?p ?o } LIMIT 10"><y>#</y><d>2022-08-02</d><h>18:15</h><r>Kelvin</r><pre>SELECT (MAX(COUNT(?s)) AS ?maxCountS) WHERE {
  ?s ?p ?o
}
LIMIT 10</pre></z><z id="t1659464174" t="Kelvin Given that the query actually went through without error it seems that it is indeed a Jena-specific detail"><y>#</y><d>2022-08-02</d><h>18:16</h><r>Kelvin</r>Given that the query actually went through without error it seems that it is indeed a Jena-specific detail</z><z id="t1659478689" t="rickmoynihan Yes, this is the behaviour I’d expect to be honest. The query should run and remove the bindings for the solution when it errors during evaluation. The spec describes this here: https://www.w3.org/TR/sparql11-query/#aggregateExample2 Also note the definition of MAX in the spec,and the description for Set functions here where they’re described as receiving a a value of type MultiSet: https://www.w3.org/TR/sparql11-query/#setFunctions The above is interesting though because it implies to me that the query below should also fail, even though the grammar permits it, as 10 is a literal integer not a MultiSet: SELECT (MAX(10) AS ?maxCountS) WHERE { } I’ve tried this and your variants on the sparql playground (sesame), stardog and jena and all return &quot;10&quot; . Which makes sense but it feels like it should be an error according to the spec; though I’ve not looked thoroughly. I suspect all these implementations coerce the literals in this case into multisets through some implicit coercion or something; though it seems peculiar with regards to your discovery."><y>#</y><d>2022-08-02</d><h>22:18</h><r>rickmoynihan</r>Yes, this is the behaviour I’d expect to be honest.  The query should run and remove the bindings for the solution when it errors during evaluation.  The spec describes this here:

<a href="https://www.w3.org/TR/sparql11-query/#aggregateExample2" target="_blank">https://www.w3.org/TR/sparql11-query/#aggregateExample2</a>

Also note the definition of MAX in the spec,and the description for Set functions here where they’re described as receiving a  a value of type MultiSet:

<a href="https://www.w3.org/TR/sparql11-query/#setFunctions" target="_blank">https://www.w3.org/TR/sparql11-query/#setFunctions</a>

The above is interesting though because it implies to me that the query below should also fail, even though the grammar permits it, as <code>10</code> is a literal integer not a MultiSet:

<pre>SELECT (MAX(10) AS ?maxCountS) WHERE {
}</pre>
I’ve tried this and your variants on the sparql playground (sesame), stardog and jena and all return <code>&quot;10&quot;</code>.  Which makes sense but it feels like it should be an error according to the spec; though I’ve not looked thoroughly.

I suspect all these implementations coerce the literals in this case into multisets through some implicit coercion or something; though it seems peculiar with regards to your discovery.</z><z id="t1659361523" t="Kelvin Thing is, I can’t find such a restriction in the https://www.w3.org/TR/sparql11-query/ , so I’d like to know if this is a general restriction or a Jena implementation detail."><y>#</y><d>2022-08-01</d><h>13:45</h><w>Kelvin</w>Thing is, I can’t find such a restriction in the <a href="https://www.w3.org/TR/sparql11-query/" target="_blank">https://www.w3.org/TR/sparql11-query/</a>, so I’d like to know if this is a general restriction or a Jena implementation detail.</z><z id="t1659428827" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U02FU7RMG8M&quot;}] : I’m pretty sure this is a Jena implementation detail, and for what it’s worth I’d advise against copying it. It looks to me that this isn’t a syntactic concern it’s a semantics concern; and flint should stick to just syntax. I’m not arguing that Jena shouldn’t error in these cases; but IMHO they shouldn’t error in the parser; they should error in the evaluator. &gt; we’d don’t really want syntax that’s not supported by certain impls like Jena, right? I think this is a slippery slope and if you defer to implementations as your oracle you’ll be cutting the expressivity of the language to whatever common subset they all support; which will be less than the union of their collective grammar/features. So if anything I’d argue that you should err on the side of the opposite direction. For example many triple stores implement extensions to the SPARQL grammar, and you probably don’t want to rule out ever supporting these extensions. In my mind this would benefit from a clear policy of potentially growing the grammar (or at least fully supporting it) rather than artificially restricting it."><y>#</y><d>2022-08-02</d><h>08:27</h><r>rickmoynihan</r><a>@U02FU7RMG8M</a>: I’m pretty sure this is a Jena implementation detail, and for what it’s worth I’d advise against copying it.

It looks to me that this isn’t a syntactic concern it’s a semantics concern; and flint should stick to just syntax.

I’m not arguing that Jena shouldn’t error in these cases; but IMHO they shouldn’t error in the parser; they should error in the evaluator.

&gt; we’d don’t really want syntax that’s not supported by certain impls like Jena, right?
I think this is a slippery slope and if you defer to implementations as your oracle you’ll be cutting the expressivity of the language to whatever common subset they all support; which will be less than the union of their collective grammar/features.

So if anything I’d argue that you should err on the side of the opposite direction.  For example many triple stores implement extensions to the SPARQL grammar, and you probably don’t want to rule out ever supporting these extensions.

In my mind this would benefit from a clear policy of potentially growing the grammar (or at least fully supporting it) rather than  artificially restricting it.</z><z id="t1659457614" t="Kelvin &gt; you’ll be cutting the expressivity of the language to whatever common subset they all support; which will be less than the union of their collective grammar/features. I actually thought that only implementing the common subset would be desirable; that way, you can just plug Flint (or whatever lib) into whatever implementation and you won’t get error messages from said implementation."><y>#</y><d>2022-08-02</d><h>16:26</h><r>Kelvin</r>&gt; you’ll be cutting the expressivity of the language to whatever common subset they all support; which will be less than the union of their collective grammar/features.
I actually thought that only implementing the common subset would be desirable; that way, you can just plug Flint (or whatever lib) into whatever implementation and you won’t get error messages from said implementation.</z><z id="t1659457636" t="Kelvin But it seems like I may be mistaken on that premise?"><y>#</y><d>2022-08-02</d><h>16:27</h><r>Kelvin</r>But it seems like I may be mistaken on that premise?</z><z id="t1659480633" t="rickmoynihan In the general case, and as a general principle to follow beyond this specific example, I think applications (and flint) can rely on the implementations erroring; and I don’t see why flint needs to protect me from the implementation erroring. Yes the error can happen sooner if you can detect it syntactically; but you’re on a slippery slope, and I think it’s more principled to stay out of static analysis or type checking etc. Yes those could be useful things; but I think they should be built on top of something like flint; or it’s intermediate representation, and not baked into the parsing itself. In this specific case; I think (though I’m not 100% sure) we’ve discovered that Jena is returning the wrong result in this case. It should be removing the binding from that solution during evaluation; not erroring during parsing. Is it a big deal? Not really; because why would you write query. My point on common subset is really that the spec should be the common subset you implement; and that you should focus on correctness and the grammar/parsing. Jena is I think strictly speaking doing less than the spec requires here, so flint doing that would mean flint is targeting the lowest common denominator of sparql; which will be less than sparql itself specifies. Then once you have that subset you could possibly choose to increase the size of grammar beyond the spec by supporting various syntax extensions some implementations may provide. I’m not suggesting this by the way; just that you could."><y>#</y><d>2022-08-02</d><h>22:50</h><r>rickmoynihan</r>In the general case, and as a general principle to follow beyond this specific example, I think applications (and flint) can rely on the implementations erroring; and I don’t see why flint needs to protect me from the implementation erroring.

Yes the error can happen sooner if you can detect it syntactically; but you’re on a slippery slope, and I think it’s more principled to stay out of static analysis or type checking etc.  Yes those could be useful things; but I think they should be built on top of something like flint; or it’s intermediate representation, and not baked into the parsing itself.

In this specific case; I think (though I’m not 100% sure) we’ve discovered that Jena is returning the wrong result in this case.  It should be removing the binding from that solution during evaluation; not erroring during parsing.  Is it a big deal?  Not really; because why would you write query.

My point on common subset is really that the spec should be the common subset you implement; and that you should focus on correctness and the grammar/parsing.  Jena is I think strictly speaking doing less than the spec requires here, so flint doing that would mean flint is targeting the lowest common denominator of sparql; which will be less than sparql itself specifies.

Then once you have that subset you could possibly choose to increase the size of grammar beyond the spec by supporting various syntax extensions some implementations may provide.  I’m not suggesting this by the way; just that you could.</z><z id="t1659362722" t="Kelvin Also another question: according to the https://www.w3.org/TR/sparql11-query/#sparqlGrammar a GROUP BY clause is defined as: GroupClause ::= &apos;GROUP&apos; &apos;BY&apos; GroupCondition+ GroupCondition ::= BuiltInCall | FunctionCall | &apos;(&apos; Expression ( &apos;AS&apos; Var )? &apos;)&apos; | Var"><y>#</y><d>2022-08-01</d><h>14:05</h><w>Kelvin</w>Also another question: according to the <a href="https://www.w3.org/TR/sparql11-query/#sparqlGrammar" target="_blank">https://www.w3.org/TR/sparql11-query/#sparqlGrammar</a> a <code>GROUP BY</code> clause is defined as:
<pre>GroupClause    ::= &apos;GROUP&apos; &apos;BY&apos; GroupCondition+
GroupCondition ::= BuiltInCall | FunctionCall | &apos;(&apos; Expression ( &apos;AS&apos; Var )? &apos;)&apos; | Var</pre></z><z id="t1659362752" t="Kelvin Where BuiltInCall and FunctionCall exclude pure arithmetic expressions like (?x + ?x)"><y>#</y><d>2022-08-01</d><h>14:05</h><w>Kelvin</w>Where <code>BuiltInCall</code> and <code>FunctionCall</code> exclude pure arithmetic expressions like <code>(?x + ?x)</code></z><z id="t1659362792" t="Kelvin Yet this seems to be legal in Jena? SELECT (SUM(?z) AS ?sum) WHERE { ?x ?y ?z . } GROUP BY (?x + ?x) ; not a function of the form FNAME(args...)"><y>#</y><d>2022-08-01</d><h>14:06</h><w>Kelvin</w>Yet this seems to be legal in Jena?
<pre>SELECT (SUM(?z) AS ?sum)
WHERE {
  ?x ?y ?z .
}
GROUP BY (?x + ?x) ; not a function of the form FNAME(args...)</pre></z><z id="t1659454571" t="quoll This is legal because (?x + ?x) is an expression (the 3rd GroupCondition)"><y>#</y><d>2022-08-02</d><h>15:36</h><r>quoll</r>This is legal because <code>(?x + ?x)</code> is an expression (the 3rd GroupCondition)</z><z id="t1659454781" t="Kelvin Yep [:attrs {:href &quot;/_/_/users/UB3R8UYA1&quot;}] pointed that out to me yesterday!"><y>#</y><d>2022-08-02</d><h>15:39</h><r>Kelvin</r>Yep <a>@UB3R8UYA1</a> pointed that out to me yesterday!</z><z id="t1659455772" t="quoll I read further down and saw that"><y>#</y><d>2022-08-02</d><h>15:56</h><r>quoll</r>I read further down and saw that</z><z id="t1659455827" t="quoll The horrible part about that BNF is writing code that implements it 😖"><y>#</y><d>2022-08-02</d><h>15:57</h><r>quoll</r>The horrible part about that BNF is writing code that implements it <b>😖</b></z><z id="t1659455856" t="quoll You can have code that performs all of the operations, but actually connecting it to the parsed grammar is a pain"><y>#</y><d>2022-08-02</d><h>15:57</h><r>quoll</r>You can have code that performs all of the operations, but actually connecting it to the parsed grammar is a pain</z><z id="t1659366827" t="Eric Scott As I read this, ?y would have to be a URI and not a numeric literal. Can you SUM over URIs?"><y>#</y><d>2022-08-01</d><h>15:13</h><w>Eric Scott</w>As I read this, ?y would have to be a URI and not a numeric literal.  Can you SUM over URIs?</z><z id="t1659454592" t="quoll Not according to the spec, no"><y>#</y><d>2022-08-02</d><h>15:36</h><r>quoll</r>Not according to the spec, no</z><z id="t1659367609" t="Kelvin Oh these were just junk bindings that I cooked up to demonstrate the example"><y>#</y><d>2022-08-01</d><h>15:26</h><w>Kelvin</w>Oh these were just junk bindings that I cooked up to demonstrate the example</z><z id="t1659367632" t="Kelvin But I’ll change it to ?z to make it more correct"><y>#</y><d>2022-08-01</d><h>15:27</h><w>Kelvin</w>But I’ll change it to <code>?z</code> to make it more correct</z><z id="t1659368074" t="Eric Scott So the problem is that GROUP BY (?literal1 + ?literal2) fails but BIND(?literal1 + ?literal2 AS ?literal3) / GROUP BY (?literal3) does work?"><y>#</y><d>2022-08-01</d><h>15:34</h><w>Eric Scott</w>So the problem is that GROUP BY (?literal1 + ?literal2) fails but BIND(?literal1 + ?literal2 AS ?literal3) / GROUP BY (?literal3) does work?</z><z id="t1659368175" t="Kelvin The problem is that GROUP BY (?literal1 + ?literal2) SHOULD fail according to the grammar but it doesn’t"><y>#</y><d>2022-08-01</d><h>15:36</h><w>Kelvin</w>The problem is that <code>GROUP BY (?literal1 + ?literal2)</code> SHOULD fail according to the grammar but it doesn’t</z><z id="t1659368489" t="Eric Scott So the critical part is here, then, I think: &apos;(&apos; Expression ( &apos;AS&apos; Var )? &apos;)&apos;"><y>#</y><d>2022-08-01</d><h>15:41</h><w>Eric Scott</w>So the critical part is here, then, I think: <code> &apos;(&apos; Expression ( &apos;AS&apos; Var )? &apos;)&apos;</code></z><z id="t1659368549" t="Eric Scott I think the ? after the (&apos;AS Var)&apos; makes that optional, so I think you&apos;re within your rights to just put an Expression"><y>#</y><d>2022-08-01</d><h>15:42</h><w>Eric Scott</w>I think the <code>?</code> after the (&apos;AS Var)&apos; makes that optional, so I think you&apos;re within your rights to just put an <code>Expression</code></z><z id="t1659368837" t="Kelvin Ooooh"><y>#</y><d>2022-08-01</d><h>15:47</h><w>Kelvin</w>Ooooh</z><z id="t1659368857" t="Eric Scott Though I can&apos;t think of how you&apos;d reference that downstream."><y>#</y><d>2022-08-01</d><h>15:47</h><w>Eric Scott</w>Though I can&apos;t think of how you&apos;d reference that downstream.</z><z id="t1659368878" t="Kelvin Well that’s a confusing way to write BrackettedExpression | &apos;(&apos; Expression &apos;AS&apos; Var &apos;)&apos;"><y>#</y><d>2022-08-01</d><h>15:47</h><w>Kelvin</w>Well that’s a confusing way to write <code>BrackettedExpression | &apos;(&apos; Expression &apos;AS&apos; Var &apos;)&apos;</code></z><z id="t1659368967" t="Kelvin &gt; Though I can’t think of how you’d reference that downstream. It’s all part of covering my bases when it comes to syntax, even if semantically it doesn’t make sense"><y>#</y><d>2022-08-01</d><h>15:49</h><w>Kelvin</w>&gt; Though I can’t think of how you’d reference that downstream.
It’s all part of covering my bases when it comes to syntax, even if semantically it doesn’t make sense</z><z id="t1659430673" t="rickmoynihan ☝️ I think you’ve hit the nail on the head here and that this is the right intuition! i.e. focus on grammar/syntax in flint and covering your bases. Don’t shrink the grammar because some statements within the grammar are nonsensical. It’s always the case in essentially language/grammar of sufficient complexity that there are syntactically valid statements which are nonsensical. The best known ways to short cut this are type-checkers; and it’s proven that even the most advanced type checkers can’t detect all possible errors."><y>#</y><d>2022-08-02</d><h>08:57</h><r>rickmoynihan</r><b>☝️</b> I think you’ve hit the nail on the head here and that this is the right intuition!  i.e. focus on grammar/syntax in flint and covering your bases.  Don’t shrink the grammar because some statements within the grammar are nonsensical.

It’s always the case in essentially language/grammar of sufficient complexity that there are syntactically valid statements which are nonsensical.  The best known ways to short cut this are type-checkers; and it’s proven that even the most advanced type checkers can’t detect all possible errors.</z><z id="t1659454686" t="Kelvin Indeed, I don’t want Flint to become a type checker - notice how expressions are completely untyped. I think one issue is that the line between pure syntax checking and type checking can get quite blurred, especially when you have a contract system like Spec."><y>#</y><d>2022-08-02</d><h>15:38</h><r>Kelvin</r>Indeed, I don’t want Flint to become a type checker - notice how expressions are completely untyped. I think one issue is that the line between pure syntax checking and type checking can get quite blurred, especially when you have a contract system like Spec.</z><z id="t1659481337" t="rickmoynihan &gt; I think one issue is that the line between pure syntax checking and type checking can get quite blurred Yes it can. I think the SPARQL BNF may illustrate this point too. &gt; especially when you have a contract system like Spec Yes, spec does blur these lines a little too. I think part of the problem there is that spec complects parsing with property-testing/generation. Can you point to any specific examples in flint where the boundary is blurred?"><y>#</y><d>2022-08-02</d><h>23:02</h><r>rickmoynihan</r>&gt;  I think one issue is that the line between pure syntax checking and type checking can get quite blurred
Yes it can.  I think the SPARQL BNF may illustrate this point too.

&gt; especially when you have a contract system like Spec
Yes, spec does blur these lines a little too.  I think part of the problem there is that spec complects parsing with property-testing/generation.

Can you point to any specific examples in flint where  the boundary is blurred?</z><z id="t1659535428" t="Kelvin &gt; Can you point to any specific examples in flint where the boundary is blurred? The simplest example I could think of would be with LIMIT clauses which requires not only integers, but positive, nonzero integers (see https://github.com/yetanalytics/flint/blob/main/src/main/com/yetanalytics/flint/spec/modifier.cljc#L32 ). Since that is a case where not only the axiom has to be an integer syntactically, but it also has to conform to the aforementioned properties."><y>#</y><d>2022-08-03</d><h>14:03</h><r>Kelvin</r>&gt; Can you point to any specific examples in flint where the boundary is blurred?
The simplest example I could think of would be with <code>LIMIT</code> clauses which requires not only integers, but positive, nonzero integers (see <a href="https://github.com/yetanalytics/flint/blob/main/src/main/com/yetanalytics/flint/spec/modifier.cljc#L32" target="_blank">https://github.com/yetanalytics/flint/blob/main/src/main/com/yetanalytics/flint/spec/modifier.cljc#L32</a>). Since that is a case where not only the axiom has to be an integer syntactically, but it also has to conform to the aforementioned properties.</z><z id="t1659958797" t="rickmoynihan I think the SPARQL grammar mandates an unsigned / non negative integer in those places too though; so I think it’s reasonable for flint to do the same there too."><y>#</y><d>2022-08-08</d><h>11:39</h><r>rickmoynihan</r>I think the SPARQL grammar mandates an unsigned / non negative integer in those places too though; so I think it’s reasonable for flint to do the same there too.</z><z id="t1659369365" t="Eric Scott Yeah I agree that&apos;s ugly."><y>#</y><d>2022-08-01</d><h>15:56</h><w>Eric Scott</w>Yeah I agree that&apos;s ugly.</z><z id="t1659370827" t="Kelvin The whole SPARQL BNF grammar is rather ugly"><y>#</y><d>2022-08-01</d><h>16:20</h><w>Kelvin</w>The whole SPARQL BNF grammar is rather ugly</z><z id="t1659431957" t="rickmoynihan Yes. I think it may have been cleaner if they’d defined a generic function syntax; and then defined elsewhere the minimal set of functions and their names that should exist in the core language. Instead they made the built in functions all be keywords in the language grammar itself. I guess it’s all trade offs; the complexity needs to go somewhere; they just chose to move a lot of it into the grammar. Presumably to try and simplify the wider specification effort."><y>#</y><d>2022-08-02</d><h>09:19</h><r>rickmoynihan</r>Yes.

I think it may have been cleaner if they’d defined a generic function syntax; and then defined elsewhere the minimal set of functions and their names that should exist in the core language.

Instead they made the built in functions all be keywords in the language grammar itself.

I guess it’s all trade offs; the complexity needs to go somewhere; they just chose to move a lot of it into the grammar.  Presumably to try and simplify the wider specification effort.</z><z id="t1659454697" t="quoll I don’t know who did the grammar to start with, but I got the impression that it came from Jena. Andy always maintained it, and I presumed that he’d written the original, but I don’t know"><y>#</y><d>2022-08-02</d><h>15:38</h><r>quoll</r>I don’t know who did the grammar to start with, but I got the impression that it came from Jena. Andy always maintained it, and I presumed that he’d written the original, but I don’t know</z><z id="t1659454761" t="Kelvin By “he” do you mean Andy Seaborne (who’s listed as one of the query language editors and is a current maintainer of Jena)?"><y>#</y><d>2022-08-02</d><h>15:39</h><r>Kelvin</r>By “he” do you mean Andy Seaborne (who’s listed as one of the query language editors and is a current maintainer of Jena)?</z><z id="t1659455074" t="quoll Yes, sorry. Andy Seabourne"><y>#</y><d>2022-08-02</d><h>15:44</h><r>quoll</r>Yes, sorry. Andy Seabourne</z><z id="t1659370860" t="Kelvin But yeah thanks for the help on this; I wonder if you have any insight on my earlier question though: https://clojurians.slack.com/archives/C09GHBXRC/p1659361416947169"><y>#</y><d>2022-08-01</d><h>16:21</h><w>Kelvin</w>But yeah thanks for the help on this; I wonder if you have any insight on my earlier question though: <a href="https://clojurians.slack.com/archives/C09GHBXRC/p1659361416947169" target="_blank">https://clojurians.slack.com/archives/C09GHBXRC/p1659361416947169</a></z><z id="t1659371637" t="Eric Scott I&apos;m still waiting to see my first pretty BNF spec. :-)"><y>#</y><d>2022-08-01</d><h>16:33</h><w>Eric Scott</w>I&apos;m still waiting to see my first pretty BNF spec. :-)</z><z id="t1659476927" t="Kelvin [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] I finally got around to making a PR to implement protocols for axioms, in order to support the datatype coercion that you’ve been asking for"><y>#</y><d>2022-08-02</d><h>21:48</h><w>Kelvin</w><a>@rickmoynihan</a> I finally got around to making a PR to implement protocols for axioms, in order to support the datatype coercion that you’ve been asking for</z><z id="t1659476963" t="Kelvin https://github.com/yetanalytics/flint/pull/27"><y>#</y><d>2022-08-02</d><h>21:49</h><r>Kelvin</r><a href="https://github.com/yetanalytics/flint/pull/27" target="_blank">https://github.com/yetanalytics/flint/pull/27</a></z><z id="t1659477030" t="Kelvin (If only there was an easy way to add people from outside one’s GitHub org as reviewers; otherwise I’d add you as a reviewer directly.)"><y>#</y><d>2022-08-02</d><h>21:50</h><r>Kelvin</r>(If only there was an easy way to add people from outside one’s GitHub org as reviewers; otherwise I’d add you as a reviewer directly.)</z><z id="t1659482223" t="rickmoynihan OMG!!! This looks fantastic! Thanks 🙇 I’ve skimmed over the PR briefly and made some small comments, but I’ve not had time to look in more depth."><y>#</y><d>2022-08-02</d><h>23:17</h><r>rickmoynihan</r>OMG!!! This looks fantastic!  Thanks <b>🙇</b>

I’ve skimmed over the PR briefly and made some small comments, but I’ve not had time to look in more depth.</z><z id="t1659535307" t="Kelvin Thanks for taking a look! I didn’t see any comments that you left though?"><y>#</y><d>2022-08-03</d><h>14:01</h><r>Kelvin</r>Thanks for taking a look! I didn’t see any comments that you left though?</z><z id="t1659620829" t="rickmoynihan ahh sorry I’d added them one by one in a review; and forgot to submit it."><y>#</y><d>2022-08-04</d><h>13:47</h><r>rickmoynihan</r>ahh sorry I’d added them one by one in a review; and forgot to submit it.</z><z id="t1659620885" t="rickmoynihan As I said though these are just from skimming over the diffs; and I’d like to find some more time to dig into it. Though It’ll likely be next week before I get to look at it properly."><y>#</y><d>2022-08-04</d><h>13:48</h><r>rickmoynihan</r>As I said though these are just from skimming over the diffs; and I’d like to find some more time to dig into it.  Though It’ll likely be next week before I get to look at it properly.</z><z id="t1659620913" t="rickmoynihan generally though I think this looks great! 🙇"><y>#</y><d>2022-08-04</d><h>13:48</h><r>rickmoynihan</r>generally though I think this looks great!  <b>🙇</b></z><z id="t1659994616" t="Kelvin I responded to the comments that you left. The only one I disagree with was the one about java.util.Date for reasons I explain in my followup, but otherwise I took into account your suggestions."><y>#</y><d>2022-08-08</d><h>21:36</h><r>Kelvin</r>I responded to the comments that you left. The only one I disagree with was the one about <code>java.util.Date</code> for reasons I explain in my followup, but otherwise I took into account your suggestions.</z><z id="t1660151467" t="Kelvin Just added support for OffsetDateTime , ZonedDateTime , LocalDateTime , and their date- and time-only equivalents"><y>#</y><d>2022-08-10</d><h>17:11</h><r>Kelvin</r>Just added support for <code>OffsetDateTime</code>, <code>ZonedDateTime</code>, <code>LocalDateTime</code>, and their date- and time-only equivalents</z><z id="t1660575018" t="Bart Kleijngeld I&apos;m generating an Avro schema from a SHACL model, and an interesting question has come up: do I choose null or [] to represent absence?"><y>#</y><d>2022-08-15</d><h>14:50</h><w>Bart Kleijngeld</w>I&apos;m generating an Avro schema from a SHACL model, and an interesting question has come up: do I choose <code>null</code> or <code>[]</code> to represent absence?</z><z id="t1660575066" t="Bart Kleijngeld To get to the point: say I have a resource Sensor , which can have zero or more measurement s. I see two ways to encode &quot;there are no measurements&quot;: 1. I make its type optional and use null , or 2. I use the empty array [] to do that. (Note that I&apos;ve assumed a closed world here. Under the open world assumption there would&apos;ve been a clear distinction: [] encodes the statement &quot;there are no measurements&quot;, and null encodes &quot;i don&apos;t know of any measurements&quot;.)"><y>#</y><d>2022-08-15</d><h>14:51</h><r>Bart Kleijngeld</r>To get to the point: say I have a resource <code>Sensor</code>, which can have zero or more <code>measurement</code>s. I see two ways to encode &quot;there are no measurements&quot;: 1. I make its type optional and use <code>null</code>, or 2. I use the empty array <code>[]</code> to do that.

(Note that I&apos;ve assumed a closed world here. Under the open world assumption there would&apos;ve been a clear distinction: <code>[]</code> encodes the statement &quot;there are no measurements&quot;, and <code>null</code> encodes &quot;i don&apos;t know of any measurements&quot;.)</z><z id="t1660575071" t="Bart Kleijngeld I was wondering if others have input here: am I correct in seeing semantic equivalence there under the closed world assumption? And if so, what reasons would I have to choose either option? Thanks!"><y>#</y><d>2022-08-15</d><h>14:51</h><r>Bart Kleijngeld</r>I was wondering if others have input here: am I correct in seeing semantic equivalence there under the closed world assumption? And if so, what reasons would I have to choose either option?

Thanks!</z><z id="t1660575889" t="quoll Yes, I believe you have semantic equivalence. To do it in an open world you’d need a collection, or an explicit “not exists” value to assert. To do it in a closed world, then you can just make it optional, which is what an Avro null does. The empty collection is OK, since being explicit like that means the same thing under both assumptions"><y>#</y><d>2022-08-15</d><h>15:04</h><r>quoll</r>Yes, I believe you have semantic equivalence.
To do it in an open world you’d need a collection, or an explicit “not exists” value to assert.
To do it in a closed world, then you can just make it optional, which is what an Avro <code>null</code> does. The empty collection is OK, since being explicit like that means the same thing under both assumptions</z><z id="t1660575961" t="quoll Though, I should check in to make sure… you said that you want to “represent absence”. Are you representing that the value is unset, or are you representing that the value is unknown? I’m presuming the former, but I shouldn’t make assumptions 🙂"><y>#</y><d>2022-08-15</d><h>15:06</h><r>quoll</r>Though, I should check in to make sure… you said that you want to “represent absence”. Are you representing that the value is unset, or are you representing that the value is unknown? I’m presuming the former, but I shouldn’t make assumptions <b>🙂</b></z><z id="t1660576252" t="Bart Kleijngeld Okay, glad to see my reasoning verified at least 🙂 ."><y>#</y><d>2022-08-15</d><h>15:10</h><r>Bart Kleijngeld</r>Okay, glad to see my reasoning verified at least <b>🙂</b>.</z><z id="t1660576533" t="Bart Kleijngeld Regarding your question: good to check that, particularly since choosing the words &quot;represent absence&quot; was the hardest part of phrasing the question 😄 . Anyways, I&apos;m not sure I understand the difference between the two interpretations you suggest (within a closed world that is). Could you elaborate? To add a little context that might help: the code I&apos;m writing aims to be a general generator of Avro schemas from a given SHACL model. That means I&apos;m trying to map SHACL concepts onto Avro ones in a meaningful way (SHACL is of course more expressive), but also a practical way (the Avro schemas are going to be actually used, so things like schema evolution matter to the point where I might sacrifice some &quot;transformational purity&quot; if that makes sense)"><y>#</y><d>2022-08-15</d><h>15:15</h><r>Bart Kleijngeld</r>Regarding your question: good to check that, particularly since choosing the words &quot;represent absence&quot; was the hardest part of phrasing the question <b>😄</b>. Anyways, I&apos;m not sure I understand the difference between the two interpretations you suggest (within a closed world that is). Could you elaborate?

To add a little context that might help: the code I&apos;m writing aims to be a general generator of Avro schemas from a given SHACL model. That means I&apos;m trying to map SHACL concepts onto Avro ones in a meaningful way (SHACL is of course more expressive), but also a practical way (the Avro schemas are going to be actually used, so things like schema evolution matter to the point where I might sacrifice some &quot;transformational purity&quot; if that makes sense)</z><z id="t1660576852" t="Bart Kleijngeld My point is: all I get from a SHACL model is cardinality constraints, and all I have in Avro are the possibility of making a type nullable or not. To that degree of generality I have to make a choice whether I map SHACL&apos;s minCardinality = 0; maxCardinality &gt; 1 to a nullable array or not"><y>#</y><d>2022-08-15</d><h>15:20</h><r>Bart Kleijngeld</r>My point is: all I get from a SHACL model is cardinality constraints, and all I have in Avro are the possibility of making a type nullable or not. To that degree of generality I have to make a choice whether I map SHACL&apos;s <code>minCardinality = 0; maxCardinality &gt; 1</code> to a nullable array or not</z><z id="t1660576870" t="quoll What I meant was basically 3 types of value: • a value, such as a number. • a null, to indicate that there is no value. • an “unknown” to indicate that the value is not known. We have a surprising number of these with medical data. For instance, if a thumb is broken, then the “laterality” property will be either left, or right, or unknown, but it can’t be null, since one of the 2 thumbs was broken. But the location of a neoplasm can have no laterality, if it is, for instance, in the center of the chest."><y>#</y><d>2022-08-15</d><h>15:21</h><r>quoll</r>What I meant was basically 3 types of value:
• a value, such as a number.
• a null, to indicate that there is no value.
• an “unknown” to indicate that the value is not known.
We have a surprising number of these with medical data. For instance, if a thumb is broken, then the “laterality” property will be either left, or right, or unknown, but it can’t be null, since one of the 2 thumbs was broken. But the location of a neoplasm can have no laterality, if it is, for instance, in the center of the chest.</z><z id="t1660577034" t="quoll if you have an array, then I wouldn’t make it nullable, but that’s a personal preference. Clojure is great for treating empty seqables as nil, but only if you wrap them in seq . But in the non-Clojure world, you often need to explicitly check for null before you’re allowed to look at a collection, which is annoying, since you need separate code for that."><y>#</y><d>2022-08-15</d><h>15:23</h><r>quoll</r>if you have an array, then I wouldn’t make it nullable, but that’s a personal preference. Clojure is great for treating empty seqables as nil, but only if you wrap them in <code>seq</code>. But in the non-Clojure world, you often need to explicitly check for null before you’re allowed to look at a collection, which is annoying, since you need separate code for that.</z><z id="t1660577051" t="quoll The other thing is that you have 2 things with the same meaning: null and []"><y>#</y><d>2022-08-15</d><h>15:24</h><r>quoll</r>The other thing is that you have 2 things with the same meaning: <code>null</code> and <code>[]</code></z><z id="t1660577055" t="Bart Kleijngeld I love that example"><y>#</y><d>2022-08-15</d><h>15:24</h><r>Bart Kleijngeld</r>I love that example</z><z id="t1660577093" t="Bart Kleijngeld Those kind of practical consequences are the ones I&apos;m looking for. That&apos;s a good argument for not making it nullable indeed"><y>#</y><d>2022-08-15</d><h>15:24</h><r>Bart Kleijngeld</r>Those kind of practical consequences are the ones I&apos;m looking for. That&apos;s a good argument for not making it nullable indeed</z><z id="t1660577105" t="quoll Well, some people will say that the model was incomplete and should have include “center” as an option. But in the real world ALL models are incomplete 🙂"><y>#</y><d>2022-08-15</d><h>15:25</h><r>quoll</r>Well, some people will say that the model was incomplete and should have include “center” as an option. But in the real world ALL models are incomplete <b>🙂</b></z><z id="t1660577157" t="Bart Kleijngeld Agree. That wouldn&apos;t be an argument against the &quot;unknown&quot; value imo"><y>#</y><d>2022-08-15</d><h>15:25</h><r>Bart Kleijngeld</r>Agree. That wouldn&apos;t be an argument against the &quot;unknown&quot; value imo</z><z id="t1660577160" t="quoll I’ve always marveled at how when I try to describe modeling to people via a simple example from the real world, someone will ALWAYS find exceptions to the model"><y>#</y><d>2022-08-15</d><h>15:26</h><r>quoll</r>I’ve always marveled at how when I try to describe modeling to people via a simple example from the real world, someone will ALWAYS find exceptions to the model</z><z id="t1660577198" t="Bart Kleijngeld Haha yes that sounds like a familiar wall to bump into"><y>#</y><d>2022-08-15</d><h>15:26</h><r>Bart Kleijngeld</r>Haha yes that sounds like a familiar wall to bump into</z><z id="t1660596703" t="Bart Kleijngeld I let it sink in and yes, [] indeed seems the more practical choice here :). An extra reason I came up with is: there&apos;s only one value to express a cardinality of zero then. I like having fewer choices, keeps things simple ;). Thanks for the input"><y>#</y><d>2022-08-15</d><h>20:51</h><r>Bart Kleijngeld</r>I let it sink in and yes, [] indeed seems the more practical choice here :). An extra reason I came up with is: there&apos;s only one value to express a cardinality of zero then. I like having fewer choices, keeps things simple ;).

Thanks for the input</z><z id="t1660827062" t="simongray I would like to link to a prior version of an attribute as defined in the prior version of my schema. There is owl:priorVersion which is used in lexinfo in this manner, however that usage seems to be incorrect as the domain and range of owl:priorVersion are both owl:Ontology . Is there some other well-known attribute which accomplishes this?"><y>#</y><d>2022-08-18</d><h>12:51</h><w>simongray</w>I would like to link to a prior version of an attribute as defined in the prior version of my schema. There is <code>owl:priorVersion</code>  which is used in lexinfo in this manner, however that usage seems to be incorrect as the domain and range of <code>owl:priorVersion</code>  are both <code>owl:Ontology</code> . Is there some other well-known attribute which accomplishes this?</z><z id="t1660827905" t="Bart Kleijngeld The way I read the OWL 2 recommendation, that&apos;s indeed not a suited choice. Section 5.5 describes all annotation properties, and I think you&apos;re right there&apos;s nothing suitable there. I&apos;m interested in your case though. Could you elaborate why you want to do this?"><y>#</y><d>2022-08-18</d><h>13:05</h><r>Bart Kleijngeld</r>The way I read the OWL 2 recommendation, that&apos;s indeed not a suited choice. Section 5.5 describes all annotation properties, and I think you&apos;re right there&apos;s nothing suitable there.

I&apos;m interested in your case though. Could you elaborate why you want to do this?</z><z id="t1660829093" t="simongray I basically just wanted to document equivalence in a less strong way"><y>#</y><d>2022-08-18</d><h>13:24</h><r>simongray</r>I basically just wanted to document equivalence in a less strong way</z><z id="t1660843598" t="Eric Scott Would any of the relationships discussed here make sense?"><y>#</y><d>2022-08-18</d><h>17:26</h><r>Eric Scott</r>Would any of the relationships discussed here make sense?</z><z id="t1660843603" t="Eric Scott https://www.w3.org/TR/vocab-dcat/#Property:resource_qualified_relation"><y>#</y><d>2022-08-18</d><h>17:26</h><r>Eric Scott</r><a href="https://www.w3.org/TR/vocab-dcat/#Property:resource_qualified_relation" target="_blank">https://www.w3.org/TR/vocab-dcat/#Property:resource_qualified_relation</a></z><z id="t1660843690" t="Eric Scott Also: https://www.w3.org/TR/vocab-dcat/#Class:Dataset"><y>#</y><d>2022-08-18</d><h>17:28</h><r>Eric Scott</r>Also: <a href="https://www.w3.org/TR/vocab-dcat/#Class:Dataset" target="_blank">https://www.w3.org/TR/vocab-dcat/#Class:Dataset</a></z><z id="t1660892877" t="simongray Thank you, [:attrs {:href &quot;/_/_/users/UB3R8UYA1&quot;}] . I think DCAT is probably the way to go."><y>#</y><d>2022-08-19</d><h>07:07</h><r>simongray</r>Thank you, <a>@UB3R8UYA1</a>. I think DCAT is probably the way to go.</z><z id="t1661243604" t="rickmoynihan I’m not sure the dcat property is quite right :thinking_face: I think rdfs:seeAlso might be a little better and also carries less entailment baggage than the dcat property; though arguably the intention is for ?objects to provide more information about ?subjects ; than for ?additional_properties to provide more information about them; which I think is closer to the intent you want… I’d be tempted to find something in prov-o: https://www.w3.org/TR/2013/REC-prov-o-20130430/"><y>#</y><d>2022-08-23</d><h>08:33</h><r>rickmoynihan</r>I’m not sure the dcat property is quite right <b>:thinking_face:</b>

I think <code>rdfs:seeAlso</code> might be a little better and also carries less entailment baggage than the dcat property; though arguably the intention is for <code>?objects</code> to provide more information about <code>?subjects</code>; than for <code>?additional_properties</code> to provide more information about them; which I think is closer to the intent you want…

I’d be tempted to find something in prov-o:

<a href="https://www.w3.org/TR/2013/REC-prov-o-20130430/" target="_blank">https://www.w3.org/TR/2013/REC-prov-o-20130430/</a></z><z id="t1660829642" t="simongray Another question. Say I want to define my (non-ontology) RDF dataset as an rdf:type which one should I pick? I need something to attach metadata to."><y>#</y><d>2022-08-18</d><h>13:34</h><w>simongray</w>Another question. Say I want to define my (non-ontology) RDF dataset as an <code>rdf:type</code> which one should I pick? I need something to attach metadata to.</z><z id="t1660898948" t="simongray [:attrs {:href &quot;/_/_/users/UB3R8UYA1&quot;}] Do you also know if there is a standard way of marking a Resource as being part of a Dataset? e.g. &lt;some-resource&gt; prefix:isPartOf &lt;some-dataset&gt; ."><y>#</y><d>2022-08-19</d><h>08:49</h><w>simongray</w><a>@eric.d.scott</a> Do you also know if there is a standard way of marking a Resource as being part of a Dataset? e.g. <code>&lt;some-resource&gt; prefix:isPartOf &lt;some-dataset&gt; .</code></z><z id="t1660925999" t="Eric Scott would &lt;some dataset&gt; dct:hasPart &lt;some resource&gt; work (from the Dublin Core)"><y>#</y><d>2022-08-19</d><h>16:19</h><r>Eric Scott</r>would &lt;some dataset&gt; dct:hasPart &lt;some resource&gt; work (from the Dublin Core)</z><z id="t1660926003" t="Eric Scott https://www.dublincore.org/specifications/dublin-core/dcmi-terms/#hasPart"><y>#</y><d>2022-08-19</d><h>16:20</h><r>Eric Scott</r><a href="https://www.dublincore.org/specifications/dublin-core/dcmi-terms/#hasPart" target="_blank">https://www.dublincore.org/specifications/dublin-core/dcmi-terms/#hasPart</a></z><z id="t1660926119" t="Eric Scott Oh, there&apos;s an inverse:"><y>#</y><d>2022-08-19</d><h>16:21</h><r>Eric Scott</r>Oh, there&apos;s an inverse:</z><z id="t1660926124" t="Eric Scott https://www.dublincore.org/specifications/dublin-core/dcmi-terms/#http://purl.org/dc/terms/isPartOf"><y>#</y><d>2022-08-19</d><h>16:22</h><r>Eric Scott</r><a href="https://www.dublincore.org/specifications/dublin-core/dcmi-terms/#http://purl.org/dc/terms/isPartOf" target="_blank">https://www.dublincore.org/specifications/dublin-core/dcmi-terms/#http://purl.org/dc/terms/isPartOf</a></z><z id="t1661159054" t="simongray Those seem perfect! Thanks a lot, Eric. 🙂"><y>#</y><d>2022-08-22</d><h>09:04</h><r>simongray</r>Those seem perfect! Thanks a lot, Eric. <b>🙂</b></z><z id="t1660898984" t="simongray Basically, I have bunch of data that I want to partition into separate datasets."><y>#</y><d>2022-08-19</d><h>08:49</h><w>simongray</w>Basically, I have bunch of data that I want to partition into separate datasets.</z><z id="t1661050576" t="Rowland Watkins [:attrs {:href &quot;/_/_/users/U4P4NREBY&quot;}] depending on your persistence approach, you could also consider named graphs for separating datasets"><y>#</y><d>2022-08-21</d><h>02:56</h><w>Rowland Watkins</w><a>@simongray</a> depending on your persistence approach, you could also consider named graphs for separating datasets</z><z id="t1661158640" t="simongray [:attrs {:href &quot;/_/_/users/U03FKR4EU5S&quot;}] Thanks, I actually didn’t think of that. However, I wonder what kind of complexities I could run into with named graphs since I also make heavy use of inferencing in Jena. Maybe it won’t be an issue at all. I guess this is the best way to prepare data for export. In any case, I still want the metadata relations in place since the dataset and all relevant schema are meant to be browsed:"><y>#</y><d>2022-08-22</d><h>08:57</h><w>simongray</w><a>@rowland.watkins</a> Thanks, I actually didn’t think of that. However, I wonder what kind of complexities I could run into with named graphs since I also make heavy use of inferencing in Jena. Maybe it won’t be an issue at all. I guess this is the best way to prepare data for export.

In any case, I still want the metadata relations in place since the dataset and all relevant schema are meant to be browsed:</z><z id="t1661184847" t="Rowland Watkins While my knowledge of the tooling is definitely behind (I haven’t used Jena in over a decade), I used the NG4J (Named Graphs 4 Jena) extension lib to do a lot of the heavy work. If memory serves, you can specify the Model Jena outputs entailments to, in this case a NamesGraphDataSet. Jena has clearly grown over the years!"><y>#</y><d>2022-08-22</d><h>16:14</h><w>Rowland Watkins</w>While my knowledge of the tooling is definitely behind (I haven’t used Jena in over a decade), I used the NG4J (Named Graphs 4 Jena) extension lib to do a lot of the heavy work. If memory serves, you can specify the Model Jena outputs entailments to, in this case a NamesGraphDataSet. Jena has clearly grown over the years!</z><z id="t1661184889" t="Rowland Watkins Throwing entailments into a names graph was very useful to me since I was able to garbage collect the forward reasoner output between runs"><y>#</y><d>2022-08-22</d><h>16:14</h><w>Rowland Watkins</w>Throwing entailments into a names graph was very useful to me since I was able to garbage collect the forward reasoner output between runs</z><z id="t1661185017" t="Rowland Watkins [:attrs {:href &quot;/_/_/users/U4P4NREBY&quot;}] so in your case there could be an option to throw entailments into a named graph and optionally merge them with the main dataset if they are useful"><y>#</y><d>2022-08-22</d><h>16:16</h><w>Rowland Watkins</w><a>@simongray</a> so in your case there could be an option to throw entailments into a named graph and optionally merge them with the main dataset  if they are useful</z><z id="t1665564349" t="simongray Hey [:attrs {:href &quot;/_/_/users/U03FKR4EU5S&quot;}] I actually did end up partitioning a Jena Dataset into separate named models which of course each have their own underlying RDF graph. While it did require a bit of refactoring, it helped me immensely to have these things properly separated. Exporting as separate files is very easy now! Thank you for the pointer."><y>#</y><d>2022-10-12</d><h>08:45</h><r>simongray</r>Hey <a>@rowland.watkins</a> I actually did end up partitioning a Jena Dataset into separate named models which of course each have their own underlying RDF graph. While it did require a bit of refactoring, it helped me immensely to have these things properly separated. Exporting as separate files is very easy now! Thank you for the pointer.</z><z id="t1665564349" t="simongray Hey [:attrs {:href &quot;/_/_/users/U03FKR4EU5S&quot;}] I actually did end up partitioning a Jena Dataset into separate named models which of course each have their own underlying RDF graph. While it did require a bit of refactoring, it helped me immensely to have these things properly separated. Exporting as separate files is very easy now! Thank you for the pointer."><y>#</y><d>2022-10-12</d><h>08:45</h><w>simongray</w>Hey <a>@rowland.watkins</a> I actually did end up partitioning a Jena Dataset into separate named models which of course each have their own underlying RDF graph. While it did require a bit of refactoring, it helped me immensely to have these things properly separated. Exporting as separate files is very easy now! Thank you for the pointer.</z><z id="t1661240736" t="simongray I will look into it! Thanks once again."><y>#</y><d>2022-08-23</d><h>07:45</h><w>simongray</w>I will look into it! Thanks once again.</z><z id="t1661241973" t="simongray &gt; [:attrs {:class &quot;username&quot;}] yeah, people keep telling me it’s slow and I shouldn’t use it, but it’s been fine so far and has a lot of functionality compared to the alternatives."><y>#</y><d>2022-08-23</d><h>08:06</h><w>simongray</w>&gt; <span>{:tag :a, :attrs {:href &quot;/_/_/users/U03FKR4EU5S&quot;}, :content (&quot;@rowland.watkins&quot;)}</span>
yeah, people keep telling me it’s slow and I shouldn’t use it, but it’s been fine so far and has a lot of functionality compared to the alternatives.</z><z id="t1661249350" t="quoll It totally depends on what you&apos;re doing, especially the scale at which you&apos;re doing it. If you&apos;re only working with 10s of millions of triples then it&apos;s totally fine. TopQuadrant even built their systems over Jena"><y>#</y><d>2022-08-23</d><h>10:09</h><w>quoll</w>It totally depends on what you&apos;re doing, especially the scale at which you&apos;re doing it. If you&apos;re only working with 10s of millions of triples then it&apos;s totally fine. TopQuadrant even built their systems over Jena</z><z id="t1661253290" t="Rowland Watkins Yeah, exactly this"><y>#</y><d>2022-08-23</d><h>11:14</h><w>Rowland Watkins</w>Yeah, exactly this</z><z id="t1661253339" t="Rowland Watkins I also might add that the perception of Jena being slow might be a combination of being JVM-bound and the fact the forward (possibly backward) reasoners are memory intensive"><y>#</y><d>2022-08-23</d><h>11:15</h><w>Rowland Watkins</w>I also might add that the perception of Jena being slow might be a combination of being JVM-bound and the fact the forward (possibly backward) reasoners are memory intensive</z><z id="t1661253408" t="Rowland Watkins Most other libs I knew of (Redland, RDFLib, 3store, 4store) don’t have any general purpose reasoners (out of the box at least)"><y>#</y><d>2022-08-23</d><h>11:16</h><w>Rowland Watkins</w>Most other libs I knew of (Redland, RDFLib, 3store, 4store) don’t have any general purpose reasoners (out of the box at least)</z><z id="t1661263382" t="quoll JVM isn’t really a reason to be slow though"><y>#</y><d>2022-08-23</d><h>14:03</h><w>quoll</w>JVM isn’t really a reason to be slow though</z><z id="t1661263435" t="quoll e.g. Stardog is very fast. That said, the Stardog VP Engineering said that it would be nice to have done it in Rust instead, due to performance, but it’s simply not worth it at this stage"><y>#</y><d>2022-08-23</d><h>14:03</h><w>quoll</w>e.g. Stardog is very fast.
That said, the Stardog VP Engineering said that it would be nice to have done it in Rust instead, due to performance, but it’s simply not worth it at this stage</z><z id="t1661267592" t="Rowland Watkins You’re quite right, the JVM isn’t a reason to be slow. "><y>#</y><d>2022-08-23</d><h>15:13</h><w>Rowland Watkins</w>You’re quite right, the JVM isn’t a reason to be slow. </z><z id="t1661267669" t="Rowland Watkins Never taken a look at Stardog, I should get up to speed on these newer platforms - [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] thanks for the info"><y>#</y><d>2022-08-23</d><h>15:14</h><w>Rowland Watkins</w>Never taken a look at Stardog, I should get up to speed on these newer platforms - <a>@quoll</a> thanks for the info</z><z id="t1661270074" t="quoll There’s also a Clojure lib for Stardog. I started it, and they picked it up to maintain it. Then they told me just recently that it’s currently in use at NASA… which I thought was cool 🙂"><y>#</y><d>2022-08-23</d><h>15:54</h><r>quoll</r>There’s also a Clojure lib for Stardog. I started it, and they picked it up to maintain it. Then they told me just recently that it’s currently in use at NASA… which I thought was cool <b>🙂</b></z><z id="t1662371378" t="Bart Kleijngeld I would like to be able to state SKOS concepts as deprecated (for older terms no longer in use). Does someone know a vocabulary that can help me with this? Or perhaps something like assigning a status (i.e. &quot;in use&quot;, &quot;pending&quot; or &quot;deprecated&quot;)?"><y>#</y><d>2022-09-05</d><h>09:49</h><w>Bart Kleijngeld</w>I would like to be able to state SKOS concepts as deprecated (for older terms no longer in use). Does someone know a vocabulary that can help me with this? Or perhaps something like assigning a status (i.e. &quot;in use&quot;, &quot;pending&quot; or &quot;deprecated&quot;)?</z><z id="t1662409486" t="quoll It depends... I presume that these are your concepts? i.e. bart:MyConcept rdf:type skos:Concept ."><y>#</y><d>2022-09-05</d><h>20:24</h><r>quoll</r>It depends... I presume that these are your concepts? i.e.
<code>bart:MyConcept rdf:type skos:Concept .</code></z><z id="t1662409752" t="quoll There&apos;s always owl:deprecated : bart:MyConcept owl:deprecated &quot;true&quot;^^xsd:boolean ."><y>#</y><d>2022-09-05</d><h>20:29</h><r>quoll</r>There&apos;s always <code>owl:deprecated</code>:
<code>bart:MyConcept owl:deprecated &quot;true&quot;^^xsd:boolean .</code></z><z id="t1662411082" t="quoll Also, Dublin core has dc:valid for a period when something is valid (if you have that info), and dc:isReplacedBy to indicate a more recent term"><y>#</y><d>2022-09-05</d><h>20:51</h><r>quoll</r>Also, Dublin core has <code>dc:valid</code> for a period when something is valid (if you have that info), and <code>dc:isReplacedBy</code> to indicate a more recent term</z><z id="t1662411112" t="quoll That may not express quite what you want though. It depends on what you&apos;re trying to do 🙂"><y>#</y><d>2022-09-05</d><h>20:51</h><r>quoll</r>That may not express quite what you want though. It depends on what you&apos;re trying to do <b>🙂</b></z><z id="t1662411119" t="quoll I recommend using the OWL term"><y>#</y><d>2022-09-05</d><h>20:51</h><r>quoll</r>I recommend using the OWL term</z><z id="t1662663595" t="Bart Kleijngeld Sorry for the late response. It&apos;s for our data governance, so I think those DC properties look really promising. Any reason why you specifically recommend the OWL term? Thanks Paula!"><y>#</y><d>2022-09-08</d><h>18:59</h><r>Bart Kleijngeld</r>Sorry for the late response.

It&apos;s for our data governance, so I think those DC properties look really promising. Any reason why you specifically recommend the OWL term? Thanks Paula!</z><z id="t1662668221" t="quoll Because its definition is specifically that a resource is deprecated. The dc terms do this as well, but require extra information with them."><y>#</y><d>2022-09-08</d><h>20:17</h><r>quoll</r>Because its definition is specifically that a resource is deprecated. The <code>dc</code> terms do this as well, but require extra information with them.</z><z id="t1662417921" t="quoll I was just looking at a library I wrote earlier this year, and was really surprised at what I made. Apparently I decided to parse CSV files into records with a basic schema, and then emit those records as Turtle. I had no idea. Cool! 🙂"><y>#</y><d>2022-09-05</d><h>22:45</h><w>quoll</w>I was just looking at a library I wrote earlier this year, and was really surprised at what I made. Apparently I decided to parse CSV files into records with a basic schema, and then emit those records as Turtle. I had no idea. Cool! <b>🙂</b></z><z id="t1662714344" t="Bart Kleijngeld The bliss of being human and possessing imperfect memory"><y>#</y><d>2022-09-09</d><h>09:05</h><r>Bart Kleijngeld</r>The bliss of being human and possessing imperfect memory</z><z id="t1663854000" t="Bart Kleijngeld I&apos;m trying to compile a GraalVM native image for our application, but the RDF4j dependency causes an reflection related issue: Exception: #error { :cause Class org.eclipse.rdf4j.model.IRI[] is instantiated reflectively but was never registered.Register the class by adding &quot;unsafeAllocated&quot; for the class in reflect-config.json. :via We&apos;re trying out https://www.graalvm.org/22.2/reference-manual/native-image/dynamic-features/Reflection/#manual-configuration from the GraalVM docs, but to no avail yet. It is remarkable that IRI seems to be understood to be a class, where really it&apos;s an interface. Maybe that&apos;s (part of) the cause of this issue? Anyways, before delving into more detail, I was mostly hoping someone (perhaps [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] ?) has already gotten this to work at some point."><y>#</y><d>2022-09-22</d><h>13:40</h><w>Bart Kleijngeld</w>I&apos;m trying to compile a GraalVM native image for our application, but the RDF4j dependency causes an reflection related issue:
<pre>Exception: #error {
 :cause Class org.eclipse.rdf4j.model.IRI[] is instantiated reflectively but was never registered.Register the class by adding &quot;unsafeAllocated&quot; for the class in reflect-config.json.
 :via</pre>
We&apos;re trying out <a href="https://www.graalvm.org/22.2/reference-manual/native-image/dynamic-features/Reflection/#manual-configuration" target="_blank">https://www.graalvm.org/22.2/reference-manual/native-image/dynamic-features/Reflection/#manual-configuration</a> from the GraalVM docs, but to no avail yet. It is remarkable that <code>IRI</code> seems to be understood to be a class, where really it&apos;s an interface. Maybe that&apos;s (part of) the cause of this issue?

Anyways, before delving into more detail, I was mostly hoping someone (perhaps <a>@rickmoynihan</a>?) has already gotten this to work at some point.</z><z id="t1664011304" t="rickmoynihan Hi, Yeah, that does look a bit strange… I suspect the array of IRI’s is due to a variadic method in the RDF4j API. Yes, I’ve managed to get code to work under graalvm, with RDF4j. However I don’t recall seeing this specific issue. Though on at least one of the occasions I got graalvm to generate the config for me…. On that project I do see { &quot;name&quot;:&quot;org.eclipse.rdf4j.model.IRI&quot; }, in my reflect-config.json . It looks to me like you’re trying to instantiate an array of IRI’s… I suspect there are more clues in the stack trace you removed from the paste… in particular the call site of the reflective call. Beyond further interogating the stack trace; I’d suggest you get graal to generate a config via instrumentation and see if it works… then you can work to trim that config down."><y>#</y><d>2022-09-24</d><h>09:21</h><r>rickmoynihan</r>Hi,

Yeah, that does look a bit strange… I suspect the array of IRI’s is due to a variadic method in the RDF4j API.

Yes, I’ve managed to get code to work under graalvm, with RDF4j.

However I don’t recall seeing this specific issue.  Though on at least one of the occasions I got graalvm to generate the config for me…. On that project I do see

<pre>{
  &quot;name&quot;:&quot;org.eclipse.rdf4j.model.IRI&quot;
},</pre>
in my <code>reflect-config.json</code>.

It looks to me like you’re trying to instantiate an array of IRI’s… I suspect there are more clues in the stack trace you removed from the paste… in particular the call site of the reflective call.

Beyond further interogating the stack trace; I’d suggest you get graal to generate a config via instrumentation and see if it works… then you can work to trim that config down.</z><z id="t1664022739" t="Bart Kleijngeld Alright, good to know it should be possible. Is there any way for me to get a look at that reflect-config.json for inspiration? I did generate one with the tracing agent, and also built one myself, but this specific issue I haven&apos;t yet been able to surpass"><y>#</y><d>2022-09-24</d><h>12:32</h><r>Bart Kleijngeld</r>Alright, good to know it should be possible. Is there any way for me to get a look at that <code>reflect-config.json</code>  for inspiration? I did generate one with the tracing agent, and also built one myself, but this specific issue I haven&apos;t yet been able to surpass</z><z id="t1664023711" t="Bart Kleijngeld Running different scenarios to cover more code paths with the tracing agent seemed to have helped discovering more cases of reflection. The generated config seems to have alleviated the issue 🙂 . [:attrs nil] It works! Thanks"><y>#</y><d>2022-09-24</d><h>12:48</h><r>Bart Kleijngeld</r>Running different scenarios to cover more code paths with the tracing agent seemed to have helped discovering more cases of reflection. The generated config seems to have alleviated the issue <b>🙂</b>. <del>I do get another error, but I&apos;ll see if I can get that fixed.</del> It works!

Thanks</z><z id="t1666166750" t="rickmoynihan 👍 sorry for the late reply — I was on paternity leave."><y>#</y><d>2022-10-19</d><h>08:05</h><r>rickmoynihan</r><b>👍</b> sorry for the late reply — I was on paternity leave.</z><z id="t1666166757" t="rickmoynihan but glad you got it working"><y>#</y><d>2022-10-19</d><h>08:05</h><r>rickmoynihan</r>but glad you got it working</z><z id="t1663855894" t="quoll It’s an interface, but it’s being instantiated reflectively?"><y>#</y><d>2022-09-22</d><h>14:11</h><w>quoll</w>It’s an interface, but it’s being instantiated reflectively?</z><z id="t1663855902" t="quoll Something seems off with that"><y>#</y><d>2022-09-22</d><h>14:11</h><w>quoll</w>Something seems off with that</z><z id="t1663856046" t="Bart Kleijngeld Yes, I thought so too. I&apos;m ready to read up and delve in, but before I do I just hope someone here already has gone through the experience of getting RDF4j to work in a GraalVM native image at least."><y>#</y><d>2022-09-22</d><h>14:14</h><r>Bart Kleijngeld</r>Yes, I thought so too. I&apos;m ready to read up and delve in, but before I do I just hope someone here already has gone through the experience of getting RDF4j to work in a GraalVM native image at least.</z><z id="t1664529559" t="simongray My PR got merged, so now Aristotle is on Jena 4.5: https://github.com/arachne-framework/aristotle"><y>#</y><d>2022-09-30</d><h>09:19</h><w>simongray</w>My PR got merged, so now Aristotle is on Jena 4.5: <a href="https://github.com/arachne-framework/aristotle" target="_blank">https://github.com/arachne-framework/aristotle</a></z><z id="t1665564420" t="simongray Next up: figuring out CSVW exports too 🙂 [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}]"><y>#</y><d>2022-10-12</d><h>08:47</h><w>simongray</w>Next up: figuring out CSVW exports too <b>🙂</b> <a>@rickmoynihan</a></z><z id="t1665711061" t="Rowland Watkins [:attrs {:href &quot;/_/_/users/U4P4NREBY&quot;}] this is awesome to hear, nicely done! I had a similar use case, needing to keep graphs separate and then reference within other graphs (digital signatures in RDF). Hope the rest of your work continues smoothly!"><y>#</y><d>2022-10-14</d><h>01:31</h><w>Rowland Watkins</w><a>@simongray</a> this is awesome to hear, nicely done! I had a similar use case, needing to keep graphs separate and then reference within other graphs (digital signatures in RDF). Hope the rest of your work continues smoothly!</z><z id="t1665740161" t="simongray Hey SPARQl experts… is there any way to make such a query performant? I have a bunch many-to-many relationships between RDF resources of a specific rdf:type. I want to capture all of these triples. My naïve query (using Aristotle’s query syntax) is the following: &apos;[:bgp [?s1 :rdf/type :ontolex/LexicalConcept] [?s1 ?rel ?s2] [?s2 :rdf/type :ontolex/LexicalConcept]] and this basically is so incredibly slow that I do not when it returns so I have given up on it. Is there any way to construct a query that can capture the relations between :ontolex/LexicalConcept resources without stalling? As an alternative, I am currently attempting to simply get each ?s1 resource out of Jena (this takes like 1-2 seconds on my machine) and then querying the graph for each of their relations separately in some kind of loop construct, since I will at least have some sense of progress when doing that…."><y>#</y><d>2022-10-14</d><h>09:36</h><w>simongray</w>Hey SPARQl experts… is there any way to make such a query performant?

I have a bunch many-to-many relationships between RDF resources of a specific rdf:type. I want to capture all of these triples. My naïve query (using Aristotle’s query syntax) is the following:

<pre>&apos;[:bgp
  [?s1 :rdf/type :ontolex/LexicalConcept]
  [?s1 ?rel ?s2]
  [?s2 :rdf/type :ontolex/LexicalConcept]]</pre>
and this basically is so incredibly slow that I do not when it returns so I have given up on it.

Is there any way to construct a query that can capture the relations between <code>:ontolex/LexicalConcept</code> resources without stalling?

As an alternative, I am currently attempting to  simply get each <code>?s1</code> resource out of Jena (this takes like 1-2 seconds on my machine) and then querying the graph for each of their relations separately in some kind of loop construct, since I will at least have some sense of progress when doing that….</z><z id="t1666036166" t="Eric Scott Do you know in advance what properties pertain between Lexical Concepts? If so, and if Aristotle supports property paths, would it make sense to rephrase as below?"><y>#</y><d>2022-10-17</d><h>19:49</h><r>Eric Scott</r>Do you know in advance what properties pertain between Lexical Concepts? If so, and if Aristotle supports property paths, would it make sense to rephrase as below?</z><z id="t1666036213" t="Eric Scott ?s1 (prop:one|prop:two|...prop:n) ?s2. ?s1 a ontolex:LexicalConcept; ?s2 a ontolex:LexicalConcept.}"><y>#</y><d>2022-10-17</d><h>19:50</h><r>Eric Scott</r><pre>?s1 (prop:one|prop:two|...prop:n) ?s2. ?s1 a ontolex:LexicalConcept; ?s2 a ontolex:LexicalConcept.}</pre></z><z id="t1666036333" t="Eric Scott Or if you need to capture the relation, then maybe use a VALUES clause."><y>#</y><d>2022-10-17</d><h>19:52</h><r>Eric Scott</r>Or if you need to capture the relation, then maybe use a VALUES clause.</z></g><g id="s5"><z id="t1666072850" t="simongray I know for sure the possible relations, but they are optional in every case. Would this make a difference?"><y>#</y><d>2022-10-18</d><h>06:00</h><r>simongray</r>I know for sure the possible relations, but they are optional in every case. Would this make a difference?</z><z id="t1666072908" t="simongray Right now I simple fetch a set of all LexicalConcepts and mapcat them using the following function: (defn synset-rel-table &quot;A performant way to fetch synset-&gt;synset relations for `synset` in `model`. The function basically exists because I wasn&apos;t able to perform a similar query in a performant way, e.g. doing this for all synsets would take ~45 minutes.&quot; [^Model model synset] (-&gt;&gt; (.listProperties (.getResource model (voc/uri-for synset))) (iterator-seq) (keep (fn [^Statement statement] (let [prefix &quot;&quot; obj (str (.getObject statement))] (when (str/starts-with? obj prefix) [synset (str (.getPredicate statement)) (voc/keyword-for obj)])))))) this works fine."><y>#</y><d>2022-10-18</d><h>06:01</h><r>simongray</r>Right now I simple fetch a set of all LexicalConcepts and <code>mapcat</code>  them using the following function:

<pre>(defn synset-rel-table
  &quot;A performant way to fetch synset-&gt;synset relations for `synset` in `model`.

  The function basically exists because I wasn&apos;t able to perform a similar query
  in a performant way, e.g. doing this for all synsets would take ~45 minutes.&quot;
  [^Model model synset]
  (-&gt;&gt; (.listProperties (.getResource model (voc/uri-for synset)))
       (iterator-seq)
       (keep (fn [^Statement statement]
               (let [prefix &quot;&quot;
                     obj    (str (.getObject statement))]
                 (when (str/starts-with? obj prefix)
                   [synset
                    (str (.getPredicate statement))
                    (voc/keyword-for obj)]))))))</pre>
this works fine.</z><z id="t1666091445" t="Eric Scott I don&apos;t think it would make a difference, and I&apos;m not even sure if it would solve your performance problem. Looks like you have a solution."><y>#</y><d>2022-10-18</d><h>11:10</h><r>Eric Scott</r>I don&apos;t think it would make a difference, and I&apos;m not even sure if it would solve your performance problem. Looks like you have a solution.</z><z id="t1666168630" t="rickmoynihan How big is the set of :ontolex/LexicalConcept s? I’m assuming both sets are large; and the problem is that ?s1 ?rel ?s2` is ungrounded. What storage engine are you using? How is it configured? Is it tdb2? And what query optimizer are you using? Is it any faster if you reorder the clauses, so that ?s1 and ?s2 are resolved first; and ?s1 ?rel ?s2 last? If reordering doesn’t help my instinct at optimising would be as Eric scott says to find the distinct set of ?rel ’s used in lexical concepts and bind that with a values clause or a property path."><y>#</y><d>2022-10-19</d><h>08:37</h><r>rickmoynihan</r>How big is the set of <code>:ontolex/LexicalConcept</code> s?

I’m assuming both sets are large; and the problem is that <code>?s1 ?rel </code>?s2` is ungrounded.

What storage engine are you using?  How is it configured?  Is it tdb2?  And what query optimizer are you using?

Is it any faster if you reorder the clauses, so that <code>?s1</code> and <code>?s2</code> are resolved first; and <code>?s1 ?rel ?s2</code> last?

If reordering doesn’t help my instinct at optimising would be as Eric scott says to find the distinct set of <code>?rel</code>’s used in lexical concepts and bind that with a values clause or a property path.</z><z id="t1666188516" t="simongray [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] This was for an in-memory graph with no reasoner attached, so it should be the absolute fastest Jena can possibly get. Gonna try your suggestions when I have my work computer available. Thanks!"><y>#</y><d>2022-10-19</d><h>14:08</h><r>simongray</r><a>@U06HHF230</a> This was for an in-memory graph with no reasoner attached, so it should be the absolute fastest Jena can possibly get. Gonna try your suggestions when I have my work computer available. Thanks!</z><z id="t1666188554" t="simongray (I obviously don&apos;t need this now, but it&apos;s always nice to gain a deeper understanding of the issue)"><y>#</y><d>2022-10-19</d><h>14:09</h><r>simongray</r>(I obviously don&apos;t need this now, but it&apos;s always nice to gain a deeper understanding of the issue)</z><z id="t1666190768" t="rickmoynihan I wouldn’t for a second assume an in memory graph is fastest."><y>#</y><d>2022-10-19</d><h>14:46</h><r>rickmoynihan</r>I wouldn’t for a second assume an in memory graph is fastest.</z><z id="t1666190814" t="rickmoynihan I’m guessing it’s not actually bottoming out to sparql at all then; but is just using basic graph pattern joins"><y>#</y><d>2022-10-19</d><h>14:46</h><r>rickmoynihan</r>I’m guessing it’s not actually bottoming out to sparql at all then; but is just using basic graph pattern joins</z><z id="t1666190928" t="rickmoynihan Typically the graph API stuff doesn’t do query planning etc; and isn’t clever about the order of joins"><y>#</y><d>2022-10-19</d><h>14:48</h><r>rickmoynihan</r>Typically the graph API stuff doesn’t do query planning etc; and isn’t clever about the order of joins</z><z id="t1666191507" t="rickmoynihan As [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] said; when you know how triple stores are implemented the performance profile is pretty predictable — though it can be in an unpredictable way :rolling_on_the_floor_laughing: — that is if the distribution of data isn’t known… or there are additional optimisations which stop applying. Many triple stores evaluate queries in bgp order… it’s the easy thing to do; as you don’t need to do a query optimiser. As I understand it the low hanging fruit for query planning; is to just shuffle the order of BGP’s in the query around based on what you know about the cardinality of indexes. So you’re evaluating the most restrictive (smallest) sets first for joins. If there’s no optimiser (as is usually the case with in memory stuff) ordering the BGP’s into expected smallest first is on you (the developer)."><y>#</y><d>2022-10-19</d><h>14:58</h><r>rickmoynihan</r>As <a>@U051N6TTC</a> said; when you know how triple stores are implemented the performance profile is pretty predictable — though it can be in an unpredictable way <b>:rolling_on_the_floor_laughing:</b> — that is if the distribution of data isn’t known… or there are additional optimisations which stop applying.

Many triple stores evaluate queries in bgp order… it’s the easy thing to do; as you don’t need to do a query optimiser.

As I understand it the low hanging fruit for query planning; is to just shuffle the order of BGP’s in the query around based on what you know about the cardinality of indexes.  So you’re evaluating the most restrictive (smallest) sets first for joins.

If there’s no optimiser (as is usually the case with in memory stuff) ordering the BGP’s into expected smallest first is on you (the developer).</z><z id="t1666191676" t="simongray I see... hm..."><y>#</y><d>2022-10-19</d><h>15:01</h><r>simongray</r>I see... hm...</z><z id="t1666191867" t="simongray This is messing with my head."><y>#</y><d>2022-10-19</d><h>15:04</h><r>simongray</r>This is messing with my head.</z><z id="t1665747501" t="simongray … I managed to make it performant by doing some interop with the Java objects. Now it takes a few seconds to generate the output I need."><y>#</y><d>2022-10-14</d><h>11:38</h><w>simongray</w>…
I managed to make it performant by doing some interop with the Java objects. Now it takes a few seconds to generate the output I need.</z><z id="t1665794260" t="quoll Ummm… how many triples do you have?"><y>#</y><d>2022-10-15</d><h>00:37</h><w>quoll</w>Ummm… how many triples do you have?</z><z id="t1665794299" t="quoll Because that sounds ridiculously slow."><y>#</y><d>2022-10-15</d><h>00:38</h><w>quoll</w>Because that sounds ridiculously slow.</z><z id="t1665794377" t="quoll Jena used to be bad, but I thought it was better "><y>#</y><d>2022-10-15</d><h>00:39</h><w>quoll</w>Jena used to be bad, but I thought it was better </z><z id="t1665833986" t="simongray It might be Aristotle being funky."><y>#</y><d>2022-10-15</d><h>11:39</h><w>simongray</w>It might be Aristotle being funky.</z><z id="t1665835602" t="quoll Hopefully!"><y>#</y><d>2022-10-15</d><h>12:06</h><r>quoll</r>Hopefully!</z><z id="t1665835823" t="quoll I have been getting some bad behavior on my notebook by Stardog lately. Perform a simple query, over and over, adding just a single bgp at a time, with each step taking ~300ms. Then add one more, and suddenly the query won’t return. Drop the database, recreate it, reload the data, and try the entire query again… 300ms."><y>#</y><d>2022-10-15</d><h>12:10</h><r>quoll</r>I have been getting some bad behavior on my notebook by Stardog lately. Perform a simple query, over and over, adding just a single bgp at a time, with each step taking ~300ms. Then add one more, and suddenly the query won’t return.
Drop the database, recreate it, reload the data, and try the entire query again… 300ms.</z><z id="t1665835864" t="quoll Happened last week. Happened again yesterday. And that’s a commercial system "><y>#</y><d>2022-10-15</d><h>12:11</h><r>quoll</r>Happened last week. Happened again yesterday. And that’s a commercial system </z><z id="t1665997291" t="simongray If I have one criticism of triplestores it would be that the computational complexity of certain queries can be a bit opaque... But then again, I&apos;ve also had stupid things happening in SQL when I forgot to create an index in advance or some other things related to the stateful nature of SQL databases."><y>#</y><d>2022-10-17</d><h>09:01</h><r>simongray</r>If I have one criticism of triplestores it would be that the computational complexity of certain queries can be a bit opaque... But then again, I&apos;ve also had stupid things happening in SQL when I forgot to create an index in advance or some other things related to the stateful nature of SQL databases.</z><z id="t1665997428" t="simongray I do sometimes wish there were a way to explicitly prepare special indices for certain queries, though, like in SQL."><y>#</y><d>2022-10-17</d><h>09:03</h><r>simongray</r>I do sometimes wish there were a way to explicitly prepare special indices for certain queries, though, like in SQL.</z><z id="t1666011533" t="quoll The benefit of RDF is that everything is already indexed (if you’re not Jena). The one exception to that is string indexing, but that’s why several databases use Lucene (Mulgara did this back in 2002. Stardog does it now. I presume others do too)"><y>#</y><d>2022-10-17</d><h>12:58</h><r>quoll</r>The benefit of RDF is that everything is already indexed (if you’re not Jena). The one exception to that is string indexing, but that’s why several databases use Lucene (Mulgara did this back in 2002. Stardog does it now. I presume others do too)</z><z id="t1666011602" t="quoll As for the computational complexity… I am protected from that since I’ve implemented these query engines more than once, so it seems clear to me 😊"><y>#</y><d>2022-10-17</d><h>13:00</h><r>quoll</r>As for the computational complexity… I am protected from that since I’ve implemented these query engines more than once, so it seems clear to me <b>😊</b></z><z id="t1666017075" t="Kelvin &gt; everything is already indexed (if you’re not Jena) Wouldn’t that depend on the backend implementation? Though I guess you’re talking about Jena’s built-in backends like Fuseki (for our app we’re using AWS Neptune which definitely has indexing, but we’re using Jena in the application layer)."><y>#</y><d>2022-10-17</d><h>14:31</h><r>Kelvin</r>&gt; everything is already indexed (if you’re not Jena)
Wouldn’t that depend on the backend implementation? Though I guess you’re talking about Jena’s built-in backends like Fuseki (for our app we’re using AWS Neptune which definitely has indexing, but we’re using Jena in the application layer).</z><z id="t1666027783" t="Eric Scott I had the impression that TDB fully indexes all triples. Am I mistaken?"><y>#</y><d>2022-10-17</d><h>17:29</h><r>Eric Scott</r>I had the impression that TDB fully indexes all triples. Am I mistaken?</z><z id="t1666027854" t="Kelvin https://jena.apache.org/documentation/tdb/architecture.html#triple-and-quad-indexes"><y>#</y><d>2022-10-17</d><h>17:30</h><r>Kelvin</r><a href="https://jena.apache.org/documentation/tdb/architecture.html#triple-and-quad-indexes" target="_blank">https://jena.apache.org/documentation/tdb/architecture.html#triple-and-quad-indexes</a></z><z id="t1666027855" t="quoll It should do, yes. Waaaaaay back, Jena had no indexing. I think that is still possible."><y>#</y><d>2022-10-17</d><h>17:30</h><r>quoll</r>It should do, yes. Waaaaaay back, Jena had no indexing. I think that is still possible.</z><z id="t1666027870" t="quoll (like… depending on the storage you use)"><y>#</y><d>2022-10-17</d><h>17:31</h><r>quoll</r>(like… depending on the storage you use)</z><z id="t1666027900" t="quoll I have some idea of how Andy was implementing TDB, as he was asking me a lot of questions about it (this was 2006/2007)"><y>#</y><d>2022-10-17</d><h>17:31</h><r>quoll</r>I have some idea of how Andy was implementing TDB, as he was asking me a lot of questions about it (this was 2006/2007)</z><z id="t1666027975" t="quoll The main point is that RDF is usually indexed every way. It’s more about the join operations, and the order they’re done in"><y>#</y><d>2022-10-17</d><h>17:32</h><r>quoll</r>The main point is that RDF is usually indexed every way. It’s more about the join operations, and the order they’re done in</z><z id="t1666074173" t="simongray [:attrs {:href &quot;/_/_/users/U02FU7RMG8M&quot;}] Isn&apos;t Fuseki a web application frontend for Jena?"><y>#</y><d>2022-10-18</d><h>06:22</h><r>simongray</r><a>@U02FU7RMG8M</a> Isn&apos;t Fuseki a web application frontend for Jena?</z><z id="t1666074316" t="simongray [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] note that there is both TDB and TDB2 now, so things may have changed since then."><y>#</y><d>2022-10-18</d><h>06:25</h><r>simongray</r><a>@U051N6TTC</a> note that there is both TDB and TDB2 now, so things may have changed since then.</z><z id="t1666087901" t="quoll I doubt things would get worse in the process! 🙂"><y>#</y><d>2022-10-18</d><h>10:11</h><r>quoll</r>I doubt things would get worse in the process! <b>🙂</b></z><z id="t1666099637" t="simongray Unless worse is better!"><y>#</y><d>2022-10-18</d><h>13:27</h><r>simongray</r>Unless worse is better!</z><z id="t1666103063" t="Kelvin [:attrs {:href &quot;/_/_/users/U4P4NREBY&quot;}] Fuseki is a SPARQL server: https://jena.apache.org/documentation/fuseki2/ - the “webapp” is just one form, the other being a more traditional RDF server (the latter which we use for app dev)"><y>#</y><d>2022-10-18</d><h>14:24</h><r>Kelvin</r><a>@U4P4NREBY</a> Fuseki is a SPARQL server: <a href="https://jena.apache.org/documentation/fuseki2/" target="_blank">https://jena.apache.org/documentation/fuseki2/</a> - the “webapp” is just one form, the other being a more traditional RDF server (the latter which we use for app dev)</z><z id="t1666170017" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] &gt; I have been getting some bad behavior on my notebook by Stardog lately. Perform a simple query, over and over, adding just a single bgp at a time, with each step taking ~300ms. Then add one more, and suddenly the query won’t return. Drop the database, recreate it, reload the data, and try the entire query again… 300ms. We see this sort of thing too, quite frequently. You might want to try running stardog optimize rather than dropping and replacing the whole thing. The query plans can be sensitive to data changes. Optimize in stardog does 3 things. 1. It recalculates stats 2. It clears any tombstoned records on deletions that may have occurred 3. It compacts the indexes https://docs.stardog.com/operating-stardog/database-administration/storage-optimize#database-optimization One problem we have is that afaik you can’t force stardog to do 2 and 3 inside an update transaction. Though you can force it to do 1 by setting index.statistics.update.blocking.ratio . At least this was true of stardog 6; afaik much of it still stands on 7 and 8; though the performance profile of those has changed a little since they switched from their homegrown indexing to using rocksdb."><y>#</y><d>2022-10-19</d><h>09:00</h><r>rickmoynihan</r><a>@U051N6TTC</a>

&gt; I have been getting some bad behavior on my notebook by Stardog lately. Perform a simple query, over and over, adding just a single bgp at a time, with each step taking ~300ms. Then add one more, and suddenly the query won’t return.
Drop the database, recreate it, reload the data, and try the entire query again… 300ms.

We see this sort of thing too, quite frequently.

You might want to try running <code>stardog optimize</code> rather than dropping and replacing the whole thing.

The query plans can be sensitive to data changes.

Optimize in stardog does 3 things.

1. It recalculates stats
2. It clears any tombstoned records on deletions that may have occurred
3. It compacts the indexes
<a href="https://docs.stardog.com/operating-stardog/database-administration/storage-optimize#database-optimization" target="_blank">https://docs.stardog.com/operating-stardog/database-administration/storage-optimize#database-optimization</a>

One problem we have is that afaik you can’t force stardog to do 2 and 3 inside an update transaction.  Though you can force it to do 1 by setting <code>index.statistics.update.blocking.ratio</code>.

At least this was true of stardog 6; afaik much of it still stands on 7 and 8; though the performance profile of those has changed a little since they switched from their homegrown indexing to using rocksdb.</z><z id="t1666005401" t="simongray [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] I have watched the presentation, read what&apos;s available on http://csvw.org , and read the CSVW primer document on the w3c website. I think it&apos;s really cool how the metadata is totally separate from the data, allowing you to basically annotate external CSV files too in addition to the ones you control yourself. I will definitely attempt to create metadata files for each of my CSV exports. However, while it is obviously underpinned by RDF (disguised as JSON-LD), it does seem like it may not be quite as flexible as I had hoped with regard to support for qnames—and supporting RDF resources use in general... Or at the very least this facet isn&apos;t as well-described in the documentation I&apos;ve consumed so far. For example, if I have a column where the values are qnames which might use several different (known) namespaces, I see no way of reconstructing these into different RDF resource IRIs using the semantics of the metadata JSON file. :thinking_face: So I guess my only choice is representing them as fully realised IRIs in that particular column or having a separate column for each namespace, so that I can use the aboutURL string interpolation method described in the primer. I wish there were a way to provide a prefix mapping in the metadata file, so that I can just use qnames directly in my columns."><y>#</y><d>2022-10-17</d><h>11:16</h><w>simongray</w><a>@rickmoynihan</a> I have watched the presentation, read what&apos;s available on <a href="http://csvw.org" target="_blank">http://csvw.org</a>, and read the CSVW primer document on the w3c website. I think it&apos;s really cool how the metadata is totally separate from the data, allowing you to basically annotate external CSV files too in addition to the ones you control yourself. I will definitely attempt to create metadata files for each of my CSV exports.

However, while it is obviously underpinned by RDF (disguised as JSON-LD), it does seem like it may not be quite as flexible as I had hoped with regard to support for qnames—and supporting RDF resources use in general... Or at the very least this facet isn&apos;t as well-described in the documentation I&apos;ve consumed so far.

For example, if I have a column where the values are qnames which might use several different (known) namespaces, I see no way of reconstructing these into different RDF resource IRIs using the semantics of the metadata JSON file. <b>:thinking_face:</b> So I guess my only choice is representing them as fully realised IRIs in that particular column or having a separate column for each namespace, so that I can use the <code>aboutURL</code> string interpolation method described in the primer.

I wish there were a way to provide a prefix mapping in the metadata file, so that I can just use qnames directly in my columns.</z><z id="t1666005700" t="simongray e.g. what I want is some CSV output that can work like this dn:someResource, dns:someRelation, dn:otherResource dn:anotherResource, wn:otherRelation, dn:yetAnotherResource which doesn&apos;t seem possible at all, unfortunately."><y>#</y><d>2022-10-17</d><h>11:21</h><w>simongray</w>e.g. what I want is some CSV output that can work like this
<pre>dn:someResource, dns:someRelation, dn:otherResource
dn:anotherResource, wn:otherRelation, dn:yetAnotherResource</pre>
which doesn&apos;t seem possible at all, unfortunately.</z><z id="t1666167771" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U4P4NREBY&quot;}] yes that’s all true. We’ve often wanted for the same feature."><y>#</y><d>2022-10-19</d><h>08:22</h><w>rickmoynihan</w><a>@simongray</a> yes that’s all true.  We’ve often wanted for the same feature.</z><z id="t1666188363" t="simongray Welp, at least I was correct in my assumptions. Thanks for letting me know."><y>#</y><d>2022-10-19</d><h>14:06</h><r>simongray</r>Welp, at least I was correct in my assumptions. Thanks for letting me know.</z><z id="t1666189902" t="quoll Does anyone else use reasoning, and if so, have they seen owl:irreflexiveProperty being treated correctly?"><y>#</y><d>2022-10-19</d><h>14:31</h><w>quoll</w>Does anyone else use reasoning, and if so, have they seen <code>owl:irreflexiveProperty</code> being treated correctly?</z><z id="t1666191815" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] : I’ve not come across that one before; is that just in OWL Full? Does irreflexiveProperty mean that :p can never be used to do :foo :p :foo ??"><y>#</y><d>2022-10-19</d><h>15:03</h><w>rickmoynihan</w><a>@quoll</a>: I’ve not come across that one before; is that just in OWL Full?

Does irreflexiveProperty mean that <code>:p</code> can never be used to do <code>:foo :p :foo</code> ??</z><z id="t1666192011" t="quoll That’s right"><y>#</y><d>2022-10-19</d><h>15:06</h><w>quoll</w>That’s right</z><z id="t1666192022" t="quoll But I’m finding that it’s being ignored by Pellet"><y>#</y><d>2022-10-19</d><h>15:07</h><w>quoll</w>But I’m finding that it’s being ignored by Pellet</z><z id="t1666257342" t="rickmoynihan what OWL profile are you running? It looks like it should be included in EL QL and RL"><y>#</y><d>2022-10-20</d><h>09:15</h><r>rickmoynihan</r>what OWL profile are you running?

It looks like it should be included in <code>EL</code> <code>QL</code> and <code>RL</code></z><z id="t1666299597" t="quoll I tried in both SL and DL"><y>#</y><d>2022-10-20</d><h>20:59</h><r>quoll</r>I tried in both SL and DL</z><z id="t1666299655" t="quoll and Pellet is DL (i.e. when you select “DL” then you get Pellet)"><y>#</y><d>2022-10-20</d><h>21:00</h><r>quoll</r>and Pellet is DL (i.e. when you select “DL” then you get Pellet)</z><z id="t1666349347" t="rickmoynihan It’s always confused me a little how the OWL 2 profiles relate to OWL 2 DL… My understanding is that OWL 2 DL is essentially the set of computable OWL; drawn from OWL 2 Full with syntax restrictions. Then the profiles are all in OWL 2 DL; but are subsets with known computational guarantees. I don’t know though why OWL 2 DL doesn’t provide any (though OWL 1 DL does); is it because it’s unknown?? i.e. why isn’t OWL 2 DL listed in Table 10 here? https://www.w3.org/TR/2012/REC-owl2-profiles-20121211/#Computational_Properties"><y>#</y><d>2022-10-21</d><h>10:49</h><r>rickmoynihan</r>It’s always confused me a little how the OWL 2 profiles relate to OWL 2 DL…

My understanding is that OWL 2 DL is essentially the set of computable OWL; drawn from OWL 2 Full with syntax restrictions.

Then the profiles are all in OWL 2 DL; but are subsets with known computational guarantees.  I don’t know though why OWL 2 DL doesn’t provide any (though OWL 1 DL does); is it because it’s unknown??  i.e. why isn’t OWL 2 DL listed in Table 10 here?

<a href="https://www.w3.org/TR/2012/REC-owl2-profiles-20121211/#Computational_Properties" target="_blank">https://www.w3.org/TR/2012/REC-owl2-profiles-20121211/#Computational_Properties</a></z><z id="t1666349747" t="rickmoynihan Hmm I guess it’s because OWL 2 Direct Semantics is OWL 2 DL??"><y>#</y><d>2022-10-21</d><h>10:55</h><r>rickmoynihan</r>Hmm I guess it’s because OWL 2 Direct Semantics is OWL 2 DL??</z><z id="t1666349768" t="rickmoynihan https://www.w3.org/TR/2012/REC-owl2-direct-semantics-20121211/#Introduction"><y>#</y><d>2022-10-21</d><h>10:56</h><r>rickmoynihan</r><a href="https://www.w3.org/TR/2012/REC-owl2-direct-semantics-20121211/#Introduction" target="_blank">https://www.w3.org/TR/2012/REC-owl2-direct-semantics-20121211/#Introduction</a></z><z id="t1666349778" t="rickmoynihan OWL can be such a rats nest :rolling_on_the_floor_laughing:"><y>#</y><d>2022-10-21</d><h>10:56</h><r>rickmoynihan</r>OWL can be such a rats nest <b>:rolling_on_the_floor_laughing:</b></z><z id="t1666364266" t="quoll Sorry I wasn’t responding… Yes. When I first started working with OWL 2 I was trying to figure out “which level is DL?” And that’s what I came up with too. It makes sense, but not when you compare it to OWL 1"><y>#</y><d>2022-10-21</d><h>14:57</h><r>quoll</r>Sorry I wasn’t responding…
Yes. When I first started working with OWL 2 I was trying to figure out “which level is DL?” And that’s what I came up with too. It makes sense, but not when you compare it to OWL 1</z><z id="t1666192038" t="quoll and all the other reasoning levels in Stardog"><y>#</y><d>2022-10-19</d><h>15:07</h><w>quoll</w>and all the other reasoning levels in Stardog</z><z id="t1666194725" t="quoll For instance: :sibling a owl:IrreflexiveProperty, owl:SymmetricProperty; owl:propertyChainAxiom (:parent :child). Given all the :parent and :child relationships, this should find siblings, but it always infers that people are their own sibling"><y>#</y><d>2022-10-19</d><h>15:52</h><r>quoll</r>For instance:
<pre>:sibling a owl:IrreflexiveProperty, owl:SymmetricProperty;
         owl:propertyChainAxiom (:parent :child).</pre>
Given all the <code>:parent</code> and <code>:child</code> relationships, this should find siblings, but it always infers that people are their own sibling</z><z id="t1666256984" t="rickmoynihan ah interesting; I was wondering how it was useful as it didn’t appear to make any entailments… so it’s main use is in essentially filtering out reflexive inferences you don’t want; makes sense."><y>#</y><d>2022-10-20</d><h>09:09</h><r>rickmoynihan</r>ah interesting; I was wondering how it was useful as it didn’t appear to make any entailments… so it’s main use is in essentially filtering out reflexive inferences you don’t want; makes sense.</z><z id="t1666299688" t="quoll Well, I figured it would make sense… if it worked :rolling_on_the_floor_laughing:"><y>#</y><d>2022-10-20</d><h>21:01</h><r>quoll</r>Well, I figured it would make sense… if it worked <b>:rolling_on_the_floor_laughing:</b></z><z id="t1666299836" t="quoll Protegé says you can’t have owl:IrreflexiveProperty on “complex” properties. Which seems to refer to any property that has any description on it"><y>#</y><d>2022-10-20</d><h>21:03</h><r>quoll</r>Protegé says you can’t have owl:IrreflexiveProperty on “complex” properties. Which seems to refer to any property that has any description on it</z><z id="t1666270364" t="simongray By chance I discovered https://patterns.dataincubator.org/ and I find it to be a really accessible resource on RDF design for anyone who know the basics. I wish I knew about it sooner. I think I have naturally picked up most of these patterns over time, but it is just so plainly stated there with useful examples."><y>#</y><d>2022-10-20</d><h>12:52</h><w>simongray</w>By chance I discovered <a href="https://patterns.dataincubator.org/" target="_blank">https://patterns.dataincubator.org/</a> and I find it to be a really accessible resource on RDF design for anyone who know the basics. I wish I knew about it sooner. I think I have naturally picked up most of these patterns over time, but it is just so plainly stated there with useful examples.</z><z id="t1666272068" t="Bart Kleijngeld That sounds pretty much exactly like what I can use right now 🙂 . Thanks!"><y>#</y><d>2022-10-20</d><h>13:21</h><r>Bart Kleijngeld</r>That sounds pretty much exactly like what I can use right now <b>🙂</b>. Thanks!</z><z id="t1666272121" t="Bart Kleijngeld I&apos;m currently reading this book, which is excellent as well (but not free, and less reference-friendly at first glance, but probably more in-depth) https://dl.acm.org/doi/book/10.1145/3382097"><y>#</y><d>2022-10-20</d><h>13:22</h><r>Bart Kleijngeld</r>I&apos;m currently reading this book, which is excellent as well (but not free, and less reference-friendly at first glance, but probably more in-depth)

<a href="https://dl.acm.org/doi/book/10.1145/3382097" target="_blank">https://dl.acm.org/doi/book/10.1145/3382097</a></z><z id="t1666273575" t="simongray Yeah, I have that book too (think I read 30-40 pages), but it seems less accessible. The one above is very concise and clear."><y>#</y><d>2022-10-20</d><h>13:46</h><r>simongray</r>Yeah, I have that book too (think I read 30-40 pages), but it seems less accessible. The one above is very concise and clear.</z><z id="t1667309159" t="Mathias Picker That&apos;s a nice resource, thanks.The best practical book about modelling I&apos;ve come across recently is http://sdmbook.panosalexopoulos.com/ by Panos Alexopoulos. It&apos;s well thought out book concentrating on ... I would say decision points in modelling, dilemmas, where you could model in more than one way, and tries to show the pros and cons of each."><y>#</y><d>2022-11-01</d><h>13:25</h><r>Mathias Picker</r>That&apos;s a nice resource, thanks.The best practical book about modelling I&apos;ve come across recently is <a href="http://sdmbook.panosalexopoulos.com/" target="_blank">http://sdmbook.panosalexopoulos.com/</a> by Panos Alexopoulos. It&apos;s well thought out book concentrating on ... I would say decision points in modelling, dilemmas, where you could model in more than one way, and tries to show the pros and cons of each.</z><z id="t1667309591" t="Mathias Picker I also like to watch semanticarts https://www.youtube.com/playlist?list=PLk2kJrehubb4dc3e5Db5Lvv9WMaOhV3V7 It&apos;s a bit hit-and-miss to find interesting videos, but some concrete modeling problems are interesting."><y>#</y><d>2022-11-01</d><h>13:33</h><r>Mathias Picker</r>I also like to watch semanticarts <a href="https://www.youtube.com/playlist?list=PLk2kJrehubb4dc3e5Db5Lvv9WMaOhV3V7" target="_blank">https://www.youtube.com/playlist?list=PLk2kJrehubb4dc3e5Db5Lvv9WMaOhV3V7</a> It&apos;s a bit hit-and-miss to find interesting videos, but some concrete modeling problems are interesting.</z><z id="t1667314479" t="simongray Thanks. Definitely gonna check that out."><y>#</y><d>2022-11-01</d><h>14:54</h><r>simongray</r>Thanks. Definitely gonna check that out.</z><z id="t1666770293" t="Mark Wardle Hi all. What would the best tooling be to load up a large number of OWL expressions (OWL functional syntax) in order to run reasoning across them? In health and care we have SNOMED CT which is an ontology. SNOMED CT contains an expression reference set that provides a number of axioms for each concept. I understand that there are a number of different reasoners available that one can, it appears, plug-in to standard OWL libraries such as ELK and Snorocket. As you can probably guess, I am very much a beginner when it comes to semantic data, so apologies for such a basic question."><y>#</y><d>2022-10-26</d><h>07:44</h><w>Mark Wardle</w>Hi all. What would the best tooling be to load up a large number of OWL expressions (OWL functional syntax) in order to run reasoning across them? In health and care we have SNOMED CT which is an ontology. SNOMED CT contains an expression reference set that provides a number of axioms for each concept. I understand that there are a number of different reasoners available that one can, it appears, plug-in to standard OWL libraries such as ELK and Snorocket. As you can probably guess, I am very much a beginner when it comes to semantic data, so apologies for such a basic question.</z><z id="t1666770328" t="Mark Wardle A related question is... do you have any recommendations on books that are relatively current, and a good introduction to RDF and OWL?"><y>#</y><d>2022-10-26</d><h>07:45</h><w>Mark Wardle</w>A related question is... do you have any recommendations on books that are relatively current, and a good introduction to RDF and OWL?</z><z id="t1666770444" t="Mark Wardle [ This is to extend https://github.com/wardle/hermes so I can do better reasoning over more complex expressions recorded in healthcare software. I can do naive reasoning using single concepts, but that gets very complicated if one tries to reason over more complex expressions. ]"><y>#</y><d>2022-10-26</d><h>07:47</h><w>Mark Wardle</w>[ This is to extend <a href="https://github.com/wardle/hermes" target="_blank">https://github.com/wardle/hermes</a> so I can do better reasoning over more complex expressions recorded in healthcare software. I can do naive reasoning using single concepts, but that gets very complicated if one tries to reason over more complex expressions. ]</z><z id="t1666770724" t="Mark Wardle I&apos;d like to implement something to support what is documented at https://confluence.ihtsdotools.org/display/DOCOWL/2.4.+Content+for+the+OWL+Axiom+Refset My expectation was that I could load each axiom into &apos;something&apos; and then run reasoning as if by magic, but that might be naive 😉"><y>#</y><d>2022-10-26</d><h>07:52</h><w>Mark Wardle</w>I&apos;d like to implement something to support what is documented at <a href="https://confluence.ihtsdotools.org/display/DOCOWL/2.4.+Content+for+the+OWL+Axiom+Refset" target="_blank">https://confluence.ihtsdotools.org/display/DOCOWL/2.4.+Content+for+the+OWL+Axiom+Refset</a>  My expectation was that I could load each axiom into &apos;something&apos; and then run reasoning as if by magic, but that might be naive <b>😉</b></z><z id="t1666779277" t="quoll Unfortunately, SNOMED is entirely defined as a TBox, which makes every reasoner I’ve run on it die a horrible death. Also unfortunate is that OWL functional syntax cannot be loaded directly by the systems I’ve tried. The latter problem is OK though, as there is a tool to extract the functional syntax into a .owl file, and then you can use a tool like Robot to convert it to .ttl "><y>#</y><d>2022-10-26</d><h>10:14</h><w>quoll</w>Unfortunately, SNOMED is entirely defined as a TBox, which makes every reasoner I’ve run on it die a horrible death.
Also unfortunate is that OWL functional syntax cannot be loaded directly by the systems I’ve tried.

The latter problem is OK though, as there is a tool to extract the functional syntax into a .owl file, and then you can use a tool like Robot to convert it to .ttl </z><z id="t1666779481" t="quoll The reasoning can be done with rules… sort of. However, all the reasoning actually just comes down to the subsumption hierarchy (rdfs:subClassOf) and that’s actually in one of the files already! Bad news is that the files are all in tab-separated-value format, and you need to extract it into rdf manually "><y>#</y><d>2022-10-26</d><h>10:18</h><w>quoll</w>The reasoning can be done with rules… sort of. However, all the reasoning actually just comes down to the subsumption hierarchy (rdfs:subClassOf) and that’s actually in one of the files already! Bad news is that the files are all in tab-separated-value format, and you need to extract it into rdf manually </z><z id="t1666782305" t="quoll The subclass relationships are all in the “Relationship” file. For instance, in the September International release, that can be found in: Snapshot/Terminology/sct2_Relationship_Snapshot_INT_20220930.txt"><y>#</y><d>2022-10-26</d><h>11:05</h><r>quoll</r>The subclass relationships are all in the “Relationship” file. For instance, in the September International release, that can be found in:
<code>Snapshot/Terminology/sct2_Relationship_Snapshot_INT_20220930.txt</code></z><z id="t1666782537" t="quoll You need to filter by the active rows, meaning that column 2 is 1 (I’m doing 0 indexing on the columns, so id is column 0)"><y>#</y><d>2022-10-26</d><h>11:08</h><r>quoll</r>You need to filter by the active rows, meaning that column 2 is <code>1</code> (I’m doing 0 indexing on the columns, so <code>id</code> is column 0)</z><z id="t1666782811" t="quoll You’re looking for any rows with a typeId (column 7) set to 116680003 . That means “IS_A” in Snomed"><y>#</y><d>2022-10-26</d><h>11:13</h><r>quoll</r>You’re looking for any rows with a <code>typeId</code> (column 7) set to <code>116680003</code>. That means “IS_A” in Snomed</z><z id="t1666782860" t="Mark Wardle Thanks [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] - that&apos;s the approach I&apos;ve used so far - but the OWL data is now also used to supply concrete values for some attributes - e.g. the dosage in numeric units of drugs in a specific product - that are only available in OWL and not in the relationships file. My tooling already handles transitive relationships, which of course includes is-a but also other relationship types such as site or pathology. I was hoping to use OWL tooling to combine these with other sources of information for inference - and to be able to do potentially more complex inference using concepts with refinements (SNOMED expressions), but perhaps it isn&apos;t worth it."><y>#</y><d>2022-10-26</d><h>11:14</h><r>Mark Wardle</r>Thanks <a>@U051N6TTC</a> - that&apos;s the approach I&apos;ve used so far - but the OWL data is now also used to supply concrete values for some attributes - e.g. the dosage in numeric units of drugs in a specific product - that are only available in OWL and not in the relationships file. My tooling already handles transitive relationships, which of course includes is-a but also other relationship types such as site or pathology. I was hoping to use OWL tooling to combine these with other sources of information for inference - and to be able to do potentially more complex inference using concepts with refinements (SNOMED expressions), but perhaps it isn&apos;t worth it.</z><z id="t1666782998" t="quoll Well, the is-a relationships actually contain all of the inferred data. All the stated data is already in the OWL file. The other reasoning (e.g. intersection between disorder, site, etc), is all precalculated and that’s what appears in the Relationship file"><y>#</y><d>2022-10-26</d><h>11:16</h><r>quoll</r>Well, the is-a relationships actually contain all of the inferred data. All the stated data is already in the OWL file. The other reasoning (e.g. intersection between disorder, site, etc), is all precalculated and that’s what appears in the Relationship file</z><z id="t1666783085" t="quoll as for more complex inferences… maybe there’s stuff in there? But they figured out a lot already. And most of the inference is done in the is-a space"><y>#</y><d>2022-10-26</d><h>11:18</h><r>quoll</r>as for more complex inferences… maybe there’s stuff in there? But they figured out a lot already. And most of the inference is done in the is-a space</z><z id="t1666783145" t="quoll they’ve misused things like “is-a” pretty badly. So finger is-a hand, and hand is-a arm"><y>#</y><d>2022-10-26</d><h>11:19</h><r>quoll</r>they’ve misused things like “is-a” pretty badly. So finger is-a hand, and hand is-a arm</z><z id="t1666783195" t="quoll (We’ve had long discussions with the doctors at work about this… some of them contribute to Snomed)"><y>#</y><d>2022-10-26</d><h>11:19</h><r>quoll</r>(We’ve had long discussions with the doctors at work about this… some of them contribute to Snomed)</z><z id="t1666783565" t="Mark Wardle Indeed - our UK drugs have the same problem."><y>#</y><d>2022-10-26</d><h>11:26</h><r>Mark Wardle</r>Indeed - our UK drugs have the same problem.</z><z id="t1666783759" t="quoll Part of the issue, IMO, is that Snomed is pure TBox, with 2 outcomes: • It’s too large to effectively reason over (you can, but it needs limited reasoning) • Most pure TBox reasoning is determining subsumption, rather than anything else: role subsumption and class subsumption."><y>#</y><d>2022-10-26</d><h>11:29</h><r>quoll</r>Part of the issue, IMO, is that Snomed is pure TBox, with 2 outcomes:
• It’s too large to effectively reason over (you can, but it needs limited reasoning)
• Most pure TBox reasoning is determining subsumption, rather than anything else: role subsumption and class subsumption.</z><z id="t1666783789" t="quoll I don’t think there’s much in the way of role-subsumption going on, but an enormous amount of class subsumption"><y>#</y><d>2022-10-26</d><h>11:29</h><r>quoll</r>I don’t think there’s much in the way of role-subsumption going on, but an enormous amount of class subsumption</z><z id="t1666783803" t="quoll (ie. all those is-a relationships)"><y>#</y><d>2022-10-26</d><h>11:30</h><r>quoll</r>(ie. all those is-a relationships)</z><z id="t1666783864" t="quoll I’ve been looking at running rules to calculate them myself, but the intersections are tricky to do efficiently."><y>#</y><d>2022-10-26</d><h>11:31</h><r>quoll</r>I’ve been looking at running rules to calculate them myself, but the intersections are tricky to do efficiently.</z><z id="t1666783896" t="Mark Wardle When processing SNOMED in context of a wider information model, or as part of an expression, one needs to normalise and then potentially run a DL classifier to test subsumption, and I was hoping I could use existing OWL tooling to do that, and I&apos;m not aware the new concrete types are available outside of the OWL reference set. But it sounds as if it isn&apos;t going to be easy to do this - which is helpful to understand in itself! I might stick with my current approach then! It makes sense why I&apos;ve not seen many implementations that do any differently to what I&apos;ve done in Hermes [ and all my other prior SNOMED implementations]."><y>#</y><d>2022-10-26</d><h>11:31</h><r>Mark Wardle</r>When processing SNOMED in context of a wider information model, or as part of an expression, one needs to normalise and then potentially run a DL classifier to test subsumption, and I was hoping I could use existing OWL tooling to do that, and I&apos;m not aware the new concrete types are available outside of the OWL reference set. But it sounds as if it isn&apos;t going to be easy to do this - which is helpful to understand in itself! I might stick with my current approach then! It makes sense why I&apos;ve not seen many implementations that do any differently to what I&apos;ve done in Hermes [ and all my other prior SNOMED implementations].</z><z id="t1666783904" t="Mark Wardle Thanks for your advice. Really helpful."><y>#</y><d>2022-10-26</d><h>11:31</h><r>Mark Wardle</r>Thanks for your advice. Really helpful.</z><z id="t1666783964" t="quoll Well, I’m more of an OWL expert than a SNOMED expert. I’m still learning the latter. But it’s frustrating for me, because of the choices they’ve made"><y>#</y><d>2022-10-26</d><h>11:32</h><r>quoll</r>Well, I’m more of an OWL expert than a SNOMED expert. I’m still learning the latter. But it’s frustrating for me, because of the choices they’ve made</z><z id="t1666783995" t="Mark Wardle And I thought I was going mad not being able to see how to easily import OWL functional syntax into tools... so hearing that you had the same issue is... re-assuring!"><y>#</y><d>2022-10-26</d><h>11:33</h><r>Mark Wardle</r>And I thought I was going mad not being able to see how to easily import OWL functional syntax into tools... so hearing that you had the same issue is... re-assuring!</z><z id="t1666784063" t="Mark Wardle Because I had tried using OWLAPI and got very confused.... 🙂"><y>#</y><d>2022-10-26</d><h>11:34</h><r>Mark Wardle</r>Because I had tried using OWLAPI and got very confused.... <b>🙂</b></z><z id="t1666784169" t="quoll The snomed-owl-toolkit libraries for working with it is easy, but I couldn’t find command line tools. I finally used https://github.com/andrewdbate/OWLSyntaxConverter that just trivially calls the library to load the owl, and then save the ttl, but that seemed wrong. Then one of my colleagues pointed me at Robot, and that seems more standard, but it doesn’t use the latest version of snomed-owl-toolkit"><y>#</y><d>2022-10-26</d><h>11:36</h><r>quoll</r>The snomed-owl-toolkit libraries for working with it is easy, but I couldn’t find command line tools. I finally used <a href="https://github.com/andrewdbate/OWLSyntaxConverter" target="_blank">https://github.com/andrewdbate/OWLSyntaxConverter</a> that just trivially calls the library to load the owl, and then save the ttl, but that seemed wrong.
Then one of my colleagues pointed me at Robot, and that seems more standard, but it doesn’t use the latest version of snomed-owl-toolkit</z><z id="t1666784244" t="quoll If you don’t know it, http://robot.obolibrary.org/ is from the OBO project (which is why I missed that it does RDF and OWL)"><y>#</y><d>2022-10-26</d><h>11:37</h><r>quoll</r>If you don’t know it, <a href="http://robot.obolibrary.org/" target="_blank">http://robot.obolibrary.org/</a> is from the OBO project (which is why I missed that it does RDF and OWL)</z><z id="t1666784271" t="Mark Wardle Thanks. I will have a look. The other issue I have found is that it is sometimes not clear whether specific tools or data are for authoring or for operational use."><y>#</y><d>2022-10-26</d><h>11:37</h><r>Mark Wardle</r>Thanks. I will have a look. The other issue I have found is that it is sometimes not clear whether specific tools or data are for authoring or for operational use.</z><z id="t1666784318" t="Mark Wardle I would definitely have skipped over that on first glance!"><y>#</y><d>2022-10-26</d><h>11:38</h><r>Mark Wardle</r>I would definitely have skipped over that on first glance!</z><z id="t1666784373" t="quoll Yes! But my colleague worked on it. I mentioned to her that there isn’t anything suggesting that I can use it to convert OWL to TTL, and she said she would pass it along"><y>#</y><d>2022-10-26</d><h>11:39</h><r>quoll</r>Yes! But my colleague worked on it. I mentioned to her that there isn’t anything suggesting that I can use it to convert OWL to TTL, and she said she would pass it along</z><z id="t1666784418" t="quoll Look at the http://robot.obolibrary.org/convert command"><y>#</y><d>2022-10-26</d><h>11:40</h><r>quoll</r>Look at the <a href="http://robot.obolibrary.org/convert" target="_blank">http://robot.obolibrary.org/convert</a> command</z><z id="t1666784434" t="quoll I’d have saved myself so much time if I’d known about that"><y>#</y><d>2022-10-26</d><h>11:40</h><r>quoll</r>I’d have saved myself so much time if I’d known about that</z><z id="t1666784509" t="quoll Oh, and in case you [:attrs nil] get a reasoning engine to work over SNOMED, you’ll need to create prototypical objects that are instances of each class, since reasoners will usually remove the TBox from query results, and of course SNOMED is entirely TBox"><y>#</y><d>2022-10-26</d><h>11:41</h><r>quoll</r>Oh, and in case you <b>can</b> get a reasoning engine to work over SNOMED, you’ll need to create prototypical objects that are instances of each class, since reasoners will usually remove the TBox from query results, and of course SNOMED is entirely TBox</z><z id="t1666784580" t="Mark Wardle Ahhh robot looks great. I could use as a java library to fly through all of the set-up and axioms and output to something else...."><y>#</y><d>2022-10-26</d><h>11:43</h><r>Mark Wardle</r>Ahhh robot looks great. I could use as a java library to fly through all of the set-up and axioms and output to something else....</z><z id="t1666784645" t="quoll So step 1 for us was creating a Turtle file full of: our-domain:_10000006 a snomed:10000006 and we can now make statements about the object our-domain:_10000006"><y>#</y><d>2022-10-26</d><h>11:44</h><r>quoll</r>So step 1 for us was creating a Turtle file full of:
<code>our-domain:_10000006 a snomed:10000006</code>
and we can now make statements about the object <code>our-domain:_10000006</code></z><z id="t1666784647" t="Mark Wardle I&apos;ve seen reports of ELK and Snorocket being used.... but now I don&apos;t know whether it will help as much as I thought it would. Super helpful though. Thank you."><y>#</y><d>2022-10-26</d><h>11:44</h><r>Mark Wardle</r>I&apos;ve seen reports of ELK and Snorocket being used.... but now I don&apos;t know whether it will help as much as I thought it would.  Super helpful though. Thank you.</z><z id="t1666784725" t="Mark Wardle Yes that&apos;s exactly what I was hoping to do - my tooling works for SNOMED but it would be nice to reason independently - that&apos;s what I was trying to state earlier."><y>#</y><d>2022-10-26</d><h>11:45</h><r>Mark Wardle</r>Yes that&apos;s exactly what I was hoping to do - my tooling works for SNOMED but it would be nice to reason independently - that&apos;s what I was trying to state earlier.</z><z id="t1666784770" t="quoll Same. But it’s just not been working out the way I’d hoped"><y>#</y><d>2022-10-26</d><h>11:46</h><r>quoll</r>Same. But it’s just not been working out the way I’d hoped</z><z id="t1666784771" t="Mark Wardle So a patient with a family history of Huntington&apos;s disease... I can reason in the context of the wider information model, but make use of the structures in SNOMED."><y>#</y><d>2022-10-26</d><h>11:46</h><r>Mark Wardle</r>So a patient with a family history of Huntington&apos;s disease... I can reason in the context of the wider information model, but make use of the structures in SNOMED.</z><z id="t1666784784" t="Mark Wardle Ah ok... that&apos;s disappointing to hear."><y>#</y><d>2022-10-26</d><h>11:46</h><r>Mark Wardle</r>Ah ok... that&apos;s disappointing to hear.</z><z id="t1666784822" t="quoll We’re mostly using pre-reasoned SNOMED to make connections for us. We just need to link to the instances that I mentioned"><y>#</y><d>2022-10-26</d><h>11:47</h><r>quoll</r>We’re mostly using pre-reasoned SNOMED to make connections for us. We just need to link to the instances that I mentioned</z><z id="t1666784834" t="quoll So then it becomes SPARQL queries to traverse the graph"><y>#</y><d>2022-10-26</d><h>11:47</h><r>quoll</r>So then it becomes SPARQL queries to traverse the graph</z><z id="t1666784928" t="quoll and I’ve pre-generated all the transitive statements with things like: insert { graph our-domain:transitive {?s rdfs:subClassOf ?sup } } using sct:900000000000207008 using our-domain:snomed-inferred where { ?s rdfs:subClassOf+ ?sup minus { ?s rdfs:subClassOf ?sup } } "><y>#</y><d>2022-10-26</d><h>11:48</h><r>quoll</r>and I’ve pre-generated all the transitive statements with things like:
<pre>insert { graph our-domain:transitive {?s rdfs:subClassOf ?sup } } using sct:900000000000207008 using our-domain:snomed-inferred where { ?s rdfs:subClassOf+ ?sup minus { ?s rdfs:subClassOf ?sup } }</pre>
</z><z id="t1666784982" t="quoll that way I can use an extra graph in the FROM clauses and I get transitive closures, with queries returning in &lt;1sec instead of 40 sec 🙂"><y>#</y><d>2022-10-26</d><h>11:49</h><r>quoll</r>that way I can use an extra graph in the FROM clauses and I get transitive closures, with queries returning in &lt;1sec instead of 40 sec <b>🙂</b></z><z id="t1666785054" t="Mark Wardle Ahh that makes sense."><y>#</y><d>2022-10-26</d><h>11:50</h><r>Mark Wardle</r>Ahh that makes sense.</z><z id="t1666795712" t="Mark Wardle I benefit from the speed of lmdb and lucene in https://github.com/wardle/hermes with hand-optimised serialisation so I didn&apos;t end up needing to cache transitive closure tables for any type of relationship, even &apos;is-a&apos;."><y>#</y><d>2022-10-26</d><h>14:48</h><r>Mark Wardle</r>I benefit from the speed of lmdb and lucene in <a href="https://github.com/wardle/hermes" target="_blank">https://github.com/wardle/hermes</a> with hand-optimised serialisation so I didn&apos;t end up needing to cache transitive closure tables for any type of relationship, even &apos;is-a&apos;.</z><z id="t1666788066" t="rickmoynihan I don’t know if this is of use to you [:attrs {:href &quot;/_/_/users/U013CFKNP2R&quot;}] but RDFox claims support for OWL 2 functional syntax. I’ve never used it myself, and I don’t do much with OWL - beyond a passing interest in it and reasoning/rule-engines and logic systems. However IIRC they have made big claims about their reasoning performance in the past."><y>#</y><d>2022-10-26</d><h>12:41</h><w>rickmoynihan</w>I don’t know if this is of use to you <a>@mark354</a> but RDFox claims support for OWL 2 functional syntax.

I’ve never used it myself, and I don’t do much with OWL - beyond a passing interest in it and reasoning/rule-engines and logic systems.  However IIRC they have made big claims about their reasoning performance in the past.</z><z id="t1666788092" t="rickmoynihan https://www.oxfordsemantic.tech/ and docs here: https://docs.oxfordsemantic.tech/introduction.html"><y>#</y><d>2022-10-26</d><h>12:41</h><w>rickmoynihan</w><a href="https://www.oxfordsemantic.tech/" target="_blank">https://www.oxfordsemantic.tech/</a>
and docs here: <a href="https://docs.oxfordsemantic.tech/introduction.html" target="_blank">https://docs.oxfordsemantic.tech/introduction.html</a></z><z id="t1666793279" t="Mark Wardle Thanks [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] I will have a look. "><y>#</y><d>2022-10-26</d><h>14:07</h><w>Mark Wardle</w>Thanks <a>@rickmoynihan</a>  I will have a look. </z><z id="t1666948453" t="simongray What&apos;s the status of using https for RDF namespaces? Would you run into any potential issues?"><y>#</y><d>2022-10-28</d><h>09:14</h><w>simongray</w>What&apos;s the status of using <code>https</code> for RDF namespaces? Would you run into any potential issues?</z><z id="t1666948975" t="quoll I have wondered this, especially in terms of compressing URIs in storage. It turns out that http is the preferred approach (which simplifies things, for which I am grateful)"><y>#</y><d>2022-10-28</d><h>09:22</h><r>quoll</r>I have wondered this, especially in terms of compressing URIs in storage. It turns out that http is the preferred approach (which simplifies things, for which I am grateful)</z><z id="t1666949029" t="quoll It&apos;s not that you can&apos;t use https, but recall that these are URIs (actually, IRIs), which means that they are identifiers, and don&apos;t necessarily indicate a retrievable resource"><y>#</y><d>2022-10-28</d><h>09:23</h><r>quoll</r>It&apos;s not that you can&apos;t use https, but recall that these are URIs (actually, IRIs), which means that they are identifiers, and don&apos;t necessarily indicate a retrievable resource</z><z id="t1666949072" t="quoll Best practice suggests that relevant data be stored at a resource&apos;s IRI, but that is not necessary, and in many cases never happens"><y>#</y><d>2022-10-28</d><h>09:24</h><r>quoll</r>Best practice suggests that relevant data be stored at a resource&apos;s IRI, but that is not necessary, and in many cases never happens</z><z id="t1666949116" t="quoll changing from http to https is changing the protocol for retrieval, but since retrieval need never happen, then why would you need to?"><y>#</y><d>2022-10-28</d><h>09:25</h><r>quoll</r>changing from http to https is changing the protocol for retrieval, but since retrieval need never happen, then why would you need to?</z><z id="t1666949321" t="quoll Another best practice is to use 303 status codes (See Other). That&apos;s because the resource that you&apos;re asking for may not be a digital one. For instance, if I have inventory information for my company, I might identify each object with IRIs like: If I were to make an http request to this address, then the digital information I get back will represent the object, but it won&apos;t [:attrs nil] the object. Ceci n&apos;est pas une pipe"><y>#</y><d>2022-10-28</d><h>09:28</h><r>quoll</r>Another best practice is to use 303 status codes (See Other). That&apos;s because the resource that you&apos;re asking for may not be a digital one. For instance, if I have inventory information for my company, I might identify each object with IRIs like:
<code></code>
If I were to make an http request to this address, then the digital information I get back will represent the object, but it won&apos;t <b>be</b> the object.
Ceci n&apos;est pas une pipe</z><z id="t1666949351" t="quoll anywhere you&apos;ve set up the service can use 303 to redirect to an https url"><y>#</y><d>2022-10-28</d><h>09:29</h><r>quoll</r>anywhere you&apos;ve set up the service can use 303 to redirect to an https url</z><z id="t1666949416" t="quoll Anyway... my point is that it&apos;s a philosophical difference and technically doesn&apos;t matter. But also that it&apos;s worthwhile sticking to http"><y>#</y><d>2022-10-28</d><h>09:30</h><r>quoll</r>Anyway... my point is that it&apos;s a philosophical difference and technically doesn&apos;t matter. But also that it&apos;s worthwhile sticking to http</z><z id="t1666954392" t="simongray Thanks Paula. 🙏"><y>#</y><d>2022-10-28</d><h>10:53</h><r>simongray</r>Thanks Paula. <b>🙏</b></z><z id="t1666954551" t="simongray I use http for my own namespaces, but in this particular case I am generating URIs for a third party&apos;s resources and 1) they don&apos;t redirect from http to https, 2) there is actually content at the URIS I am generating (they have inadvertently made something very RDF&apos;able)."><y>#</y><d>2022-10-28</d><h>10:55</h><r>simongray</r>I use http for my own namespaces, but in this particular case I am generating URIs for a third party&apos;s resources and 1) they don&apos;t redirect from http to https, 2) there is actually content at the URIS I am generating (they have inadvertently made something very RDF&apos;able).</z><z id="t1666959151" t="quoll Just to add to the point that IRIs don’t need to be http, for a little while I would publish a foaf file with a gopher:// IRI 😊"><y>#</y><d>2022-10-28</d><h>12:12</h><r>quoll</r>Just to add to the point that IRIs don’t need to be http, for a little while I would publish a foaf file with a gopher:// IRI <b>😊</b></z><z id="t1666959200" t="quoll I mean, that was the IRI that I used to refer to myself, and also the URL where you could get the document "><y>#</y><d>2022-10-28</d><h>12:13</h><r>quoll</r>I mean, that was the IRI that I used to refer to myself, and also the URL where you could get the document </z><z id="t1666959559" t="simongray makes sense"><y>#</y><d>2022-10-28</d><h>12:19</h><r>simongray</r>makes sense</z><z id="t1667223428" t="rickmoynihan I think it depends on whether you’re just doing RDF or are doing linked data. Assuming you’re doing linked data then I think these days you should try and use https for your identifiers instead of http . You can redirect from http to https , but the problem is that if you’re also offering linked data to API consumers then anyone “following their nose” will be making insecure requests, potentially susceptible to tampering/mitm etc. Anyway it entirely depends on your users and use cases and trade offs — but all things being equal I think https is the right choice; though it may add a small complexity tax to some consumers — for example if they don’t have TLS easily available to them; though I don’t think many languages will suffer that issue these days."><y>#</y><d>2022-10-31</d><h>13:37</h><r>rickmoynihan</r>I think it depends on whether you’re just doing RDF or are doing linked data.  Assuming you’re doing linked data then I think these days you should try and use <code>https</code> for your identifiers instead of <code>http</code>.

You can redirect from <code>http</code> to <code>https</code>, but the problem is that if you’re also offering linked data to API consumers then anyone “following their nose” will be making insecure requests, potentially susceptible to tampering/mitm etc.

Anyway it entirely depends on your users and use cases and trade offs — but all things being equal I think <code>https</code> is the right choice; though it may add a small complexity tax to some consumers — for example if they don’t have TLS easily available to them; though I don’t think many languages will suffer that issue these days.</z><z id="t1667226500" t="simongray I had not considered that there could different considerations concerning rdf vs. linked data—in fact I think I mostly considered them synonyms till now. I think I might go with https. There is no RDF at the other end though, just HTML pages."><y>#</y><d>2022-10-31</d><h>14:28</h><r>simongray</r>I had not considered that there could different considerations concerning rdf vs. linked data—in fact I think I mostly considered them synonyms till now. I think I might go with https. There is no RDF at the other end though, just HTML pages.</z><z id="t1667295129" t="rickmoynihan Yeah. They’re frequently considered synonyms as they often overlap; and I think a big problem with the vision statement is that it complects them all together; where as the ideas and technologies underneath are really de-complected: I tend to view the breakdown as: 1. RDF is the data model (the platonic triples). 2. Linked Data is the principle of hosting representations of your identifiers at the same location as their identifier - so they can be looked up. Strictly speaking that principle could be implemented in anything; but in practice it implies URI’s and the web — and RDF is a good fit for it because it was built from first principles with this world view. 3. The Semantic Web. This is the idea of supporting formal reasoning, and logical entailments on such an architecture or information model; but both those things are optional — and you can just take the reasoning if you want. So in my mind you can have linked data without RDF and you can have RDF without linked data. You can also have the “semantic web” without Linked Data or even RDF, as OWL also supports non RDF syntaxes. Does that make sense?"><y>#</y><d>2022-11-01</d><h>09:32</h><r>rickmoynihan</r>Yeah.  They’re frequently considered synonyms as they often overlap; and I think a big problem with the vision statement is that it complects them all together; where as the ideas and technologies underneath are really de-complected:

I tend to view the breakdown as:

1. RDF is the data model (the platonic triples).
2. Linked Data is the principle of hosting representations of your identifiers at the same location as their identifier - so they can be looked up.  Strictly speaking that principle could be implemented in anything; but in practice it implies URI’s and the web — and RDF is a good fit for it because it was built from first principles with this world view.  
3. The Semantic Web.  This is the idea of supporting formal reasoning, and logical entailments on such an architecture or information model; but both those things are optional — and you can just take the reasoning if you want.  
So in my mind you can have linked data without RDF and you can have RDF without linked data.  You can also have the “semantic web” without Linked Data or even RDF, as OWL also supports non RDF syntaxes.

Does that make sense?</z><z id="t1667298548" t="quoll Incidentally, Snomed appears to use OWL without RDF. It’s a bit annoying to extract Snomed into RDF, (though not too hard)"><y>#</y><d>2022-11-01</d><h>10:29</h><r>quoll</r>Incidentally, Snomed appears to use OWL without RDF. It’s a bit annoying to extract Snomed into RDF, (though not too hard)</z><z id="t1667306065" t="rickmoynihan yeah OWL can be pretty verbose as turtle"><y>#</y><d>2022-11-01</d><h>12:34</h><r>rickmoynihan</r>yeah OWL can be pretty verbose as turtle</z><z id="t1667306213" t="quoll Hmm, I haven’t found it so. Probably on raw character count then Manchester or functional syntaxes may be a little shorter, but not by a lot. I think that I t’s mostly white space and indentation that makes Turtle look longer. I definitely prefer to write my OWL in ttl"><y>#</y><d>2022-11-01</d><h>12:36</h><r>quoll</r>Hmm, I haven’t found it so. Probably on raw character count then Manchester or functional syntaxes may be a little shorter, but not by a lot. I think that I t’s mostly white space and indentation that makes Turtle look longer. I definitely prefer to write my OWL in ttl</z><z id="t1667306962" t="rickmoynihan I think it’s a little more than that — iirc in turtle you need to do a bit more plumbing e.g. adding a owl:Restriction; ... etc But I agree it’s not that bad; and in practice it makes the RDFization much more obvious — for example if you’re going to SPARQL for things etc… though there was TERP syntax — but nobody uses that."><y>#</y><d>2022-11-01</d><h>12:49</h><r>rickmoynihan</r>I think it’s a little more than that — iirc in turtle you need to do a bit more plumbing e.g. adding <code>a owl:Restriction; ...</code> etc

But I agree it’s not that bad; and in practice it makes the RDFization much more obvious — for example if you’re going to SPARQL for things etc… though there was TERP syntax — but nobody uses that.</z><z id="t1667307128" t="quoll I skip the restriction type declaration, because it gets inferred due to the domain of the restriction properties 😉"><y>#</y><d>2022-11-01</d><h>12:52</h><r>quoll</r>I skip the restriction type declaration, because it gets inferred due to the domain of the restriction properties <b>😉</b></z><z id="t1667319942" t="rickmoynihan … fair point 🙂"><y>#</y><d>2022-11-01</d><h>16:25</h><r>rickmoynihan</r>… fair point <b>🙂</b></z><z id="t1666948488" t="simongray i.e. is http generally expected and https a source of bugs"><y>#</y><d>2022-10-28</d><h>09:14</h><w>simongray</w>i.e. is <code>http</code> generally expected and <code>https</code> a source of bugs</z><z id="t1666961933" t="simongray I have some fairly incomprehensible grammatical description string literals for different words that I want to represent using a proper RDF relation. Not sure which relation to go for. I am using vocab like Ontolex and skos already, so anything in those...? The literals all look like similar to this: &quot; http://vb.perf.part.sg.fk &quot; - basically a bunch of abbreviated jargon."><y>#</y><d>2022-10-28</d><h>12:58</h><w>simongray</w>I have some fairly incomprehensible grammatical description string literals for different words that I want to represent using a proper RDF relation. Not sure which relation to go for. I am using vocab like Ontolex and skos already, so anything in those...? The literals all look like similar to this: &quot;<a href="http://vb.perf.part.sg.fk" target="_blank">http://vb.perf.part.sg.fk</a>&quot; - basically a bunch of abbreviated jargon.</z><z id="t1666998346" t="quoll My inclination is usually blank nodes representing the concept being represented with a rdfs:label , then skos to connect the nodes. Skos notes could be used instead of the rdfs label, as could Dublin Core properties, but I generally wouldn’t use either of those for this sort of thing."><y>#</y><d>2022-10-28</d><h>23:05</h><r>quoll</r>My inclination is usually blank nodes representing the concept being represented with a <code>rdfs:label</code>, then skos to connect the nodes. Skos notes could be used instead of the rdfs label, as could Dublin Core properties, but I generally wouldn’t use either of those for this sort of thing.</z><z id="t1666996464" t="Kelvin Announcing the release of https://github.com/yetanalytics/flint-jena : https://clojurians.slack.com/archives/C06MAR553/p1666996337075359"><y>#</y><d>2022-10-28</d><h>22:34</h><w>Kelvin</w>Announcing the release of <a href="https://github.com/yetanalytics/flint-jena" target="_blank">https://github.com/yetanalytics/flint-jena</a>:
<a href="https://clojurians.slack.com/archives/C06MAR553/p1666996337075359" target="_blank">https://clojurians.slack.com/archives/C06MAR553/p1666996337075359</a></z><z id="t1667026213" t="simongray Very nice! If I hadn&apos;t already written most of my queries, I would probably use this. The query system of Aristotle is very basic and I have had to resort to SPARQL a few times already."><y>#</y><d>2022-10-29</d><h>06:50</h><r>simongray</r>Very nice! If I hadn&apos;t already written most of my queries, I would probably use this. The query system of Aristotle is very basic and I have had to resort to SPARQL a few times already.</z><z id="t1667768816" t="simongray Loom needs a new maintainer: https://github.com/aysylu/loom/issues/135#issuecomment-1304844243"><y>#</y><d>2022-11-06</d><h>21:06</h><w>simongray</w>Loom needs a new maintainer: <a href="https://github.com/aysylu/loom/issues/135#issuecomment-1304844243" target="_blank">https://github.com/aysylu/loom/issues/135#issuecomment-1304844243</a></z><z id="t1667828484" t="quoll Hmm, I do know the codebase, and I’ve submitted patches. Maybe? Also happy to work with someone who wants to pick it up"><y>#</y><d>2022-11-07</d><h>13:41</h><r>quoll</r>Hmm, I do know the codebase, and I’ve submitted patches. Maybe?
Also happy to work with someone who wants to pick it up</z><z id="t1667830193" t="simongray I can maybe help out, but I don&apos;t really have time until 2023. This second half of of 2022 is extremely busy for me 😞"><y>#</y><d>2022-11-07</d><h>14:09</h><r>simongray</r>I can maybe help out, but I don&apos;t really have time until 2023. This second half of of 2022 is extremely busy for me <b>😞</b></z><z id="t1667830215" t="simongray I also submitted a patch or two"><y>#</y><d>2022-11-07</d><h>14:10</h><r>simongray</r>I also submitted a patch or two</z><z id="t1667934423" t="Eric Scott So how would this work, the new maintainer&apos;s account would fork the original, and the original maintainer would publicly convey their blessing?"><y>#</y><d>2022-11-08</d><h>19:07</h><r>Eric Scott</r>So how would this work, the new maintainer&apos;s account would fork the original, and the original maintainer would publicly convey their blessing?</z><z id="t1667939644" t="simongray I think an official repo transfer to clj-commons and someone signs up to be the maintainer. No forking needed."><y>#</y><d>2022-11-08</d><h>20:34</h><r>simongray</r>I think an official repo transfer to clj-commons and someone signs up to be the maintainer. No forking needed.</z><z id="t1668103648" t="Eric Scott Hey all - I think there are a couple of people who are using ont-app/vocabulary&apos;s #lstr reader tag. When I wrote it, I had overlooked this line in https://clojure.org/reference/reader#tagged_literals : Reader tags without namespace qualifiers are reserved for Clojure I&apos;m revising this module now to deprecate #lstr and use #voc/lstr instead. Is that going to ruin anybody&apos;s day?"><y>#</y><d>2022-11-10</d><h>18:07</h><w>Eric Scott</w>Hey all -

I think there are a couple of people who are using
ont-app/vocabulary&apos;s #lstr reader tag.

When I wrote it, I had overlooked this line in <a href="https://clojure.org/reference/reader#tagged_literals" target="_blank">https://clojure.org/reference/reader#tagged_literals</a>:

<pre>Reader tags without namespace qualifiers are reserved for Clojure</pre>
I&apos;m revising this module now to deprecate #lstr and use #voc/lstr instead. Is that going to ruin anybody&apos;s day?</z><z id="t1668104173" t="simongray Fine with me."><y>#</y><d>2022-11-10</d><h>18:16</h><r>simongray</r>Fine with me.</z><z id="t1668106165" t="Eric Scott BTW, this basically maps: #voc/lstr &quot;"><y>#</y><d>2022-11-10</d><h>18:49</h><w>Eric Scott</w>BTW, this basically maps:
<pre>#voc/lstr &quot;</pre></z><z id="t1669215576" t="Bart Kleijngeld Does someone have tips on good coding tools with SHACL? I&apos;m not a fan of the big GUIs such as TopBraid&apos;s Composer. I don&apos;t care what editor/IDE it is and what plugins or other stack you have experience with: if it works nicely I would love to hear!"><y>#</y><d>2022-11-23</d><h>14:59</h><w>Bart Kleijngeld</w>Does someone have tips on good coding tools with SHACL? I&apos;m not a fan of the big GUIs such as TopBraid&apos;s Composer.

I don&apos;t care what editor/IDE it is and what plugins or other stack you have experience with: if it works nicely I would love to hear!</z><z id="t1669215639" t="Bart Kleijngeld At the bare minimum I&apos;m looking at syntax highlighting and autocompletion. But a very very nice-to-have would be to have validation against a shapes graph on board as well (CLI tool would also be fine)."><y>#</y><d>2022-11-23</d><h>15:00</h><r>Bart Kleijngeld</r>At the bare minimum I&apos;m looking at syntax highlighting and autocompletion. But a very very nice-to-have would be to have validation against a shapes graph on board as well (CLI tool would also be fine).</z><z id="t1669282021" t="rickmoynihan https://github.com/BruJu/shacled-turtle"><y>#</y><d>2022-11-24</d><h>09:27</h><r>rickmoynihan</r><a href="https://github.com/BruJu/shacled-turtle" target="_blank">https://github.com/BruJu/shacled-turtle</a></z><z id="t1669282087" t="rickmoynihan though it’s arguably more of a prototype"><y>#</y><d>2022-11-24</d><h>09:28</h><r>rickmoynihan</r>though it’s arguably more of a prototype</z><z id="t1669292030" t="Bart Kleijngeld Thanks, I&apos;ll check it out"><y>#</y><d>2022-11-24</d><h>12:13</h><r>Bart Kleijngeld</r>Thanks, I&apos;ll check it out</z><z id="t1669299369" t="Bart Kleijngeld I&apos;m just really surprised I cannot find any plugin for even something like VS Code that supports auto-completion 😞"><y>#</y><d>2022-11-24</d><h>14:16</h><r>Bart Kleijngeld</r>I&apos;m just really surprised I cannot find any plugin for even something like VS Code that supports auto-completion <b>😞</b></z><z id="t1669585608" t="Takis_ do you know if knowledge graphs are useful or its too complicated and AI to be used yet?"><y>#</y><d>2022-11-27</d><h>21:46</h><w>Takis_</w>do you know if knowledge graphs are useful or its too complicated and AI to be used yet?</z><z id="t1669659830" t="quoll Yes, they are useful, but in different ways. Knowledge Graphs encode what you know about the data. AI attempts to determine what data means. They are used for different tasks. The trick is in getting them to work together, since it would be great for an AI to inform it’s decisions based on a collection of knowledge from a graph"><y>#</y><d>2022-11-28</d><h>18:23</h><r>quoll</r>Yes, they are useful, but in different ways. Knowledge Graphs encode what you know  about the data. AI attempts to determine what data means. They are used for different tasks. The trick is in getting them to work together, since it would be great for an AI to inform it’s decisions based on a collection of knowledge from a graph</z><z id="t1669585686" t="Takis_ i see so many data science jobs using statistics/probability etc but i like semantics"><y>#</y><d>2022-11-27</d><h>21:48</h><w>Takis_</w>i see so many data science jobs using statistics/probability etc but i like semantics</z><z id="t1669585875" t="Takis_ also in data engineering jobs, SQL is heavily used, like SQL data warehousing"><y>#</y><d>2022-11-27</d><h>21:51</h><w>Takis_</w>also in data engineering jobs, SQL is heavily used, like SQL data warehousing</z><z id="t1669588275" t="Takis_ also i like Mongodb and json-ld exists, i was wondering if i can use mongodb as triplestore also"><y>#</y><d>2022-11-27</d><h>22:31</h><w>Takis_</w>also i like Mongodb and json-ld exists, i was wondering if i can use mongodb as triplestore also</z><z id="t1669889355" t="rickmoynihan This sort of architecture is possible; but as quoll says the database type determines what will be efficient/easy. For example you could put JSON documents in mongo, and provide a JSONLD interpretation of them also — but you’d want to make sure that when in mongodb you only ever stored/retrieved them in the ways that mongo is sympathetic too. If you wanted to then also query more flexibly as triples, you could ingest those same documents as triples into a triple store and query there. In this sense JSON-LD can act as an integration technology; i.e. giving universal meaning to localised data; but the difficulties will then be in determining which database is the source of truth etc — and how you can keep them in sync. You may find that the need to store/query in certain ways aligns with different audiences… e.g. there is a classical divide between transaction processing and analytical processing. So you can partition the ETL/synchronisation strategies accordingly; and things like JSON-LD can give you universal ways to reference data across both systems etc. I’d also state that I believe these integration benefits can be huge; but that they can be quite hard to achieve — not least because integration project teams often don’t align with the groups in charge of the individual systems. i.e. conways law is a barrier to what you can achieve; and the right approach is then a political process to agree to integrate in this manner. If it’s greenfield and you’re designing it up front; then that is obviously a lot easier to mandate these things — but you’ll be paying the costs of the integration and modelling — before you need it. So the challenge then becomes that people don’t see any benefits of integration until the different systems are independently useful; and it can be perceived as a pointless cost on getting things done."><y>#</y><d>2022-12-01</d><h>10:09</h><r>rickmoynihan</r>This sort of architecture is possible; but as quoll says the database type determines what will be efficient/easy.

For example you could put JSON documents in mongo, and provide a JSONLD interpretation of them also — but you’d want to make sure that when in mongodb you only ever stored/retrieved them in the ways that mongo is sympathetic too.

If you wanted to then also query more flexibly as triples, you could ingest those same documents as triples into a triple store and query there.

In this sense JSON-LD can act as an integration technology; i.e. giving universal meaning to localised data; but the difficulties will then be in determining which database is the source of truth etc — and how you can keep them in sync.

You may find that the need to store/query in certain ways aligns with different audiences… e.g. there is a classical divide between transaction processing and analytical processing.

So you can partition the ETL/synchronisation strategies accordingly; and things like JSON-LD can give you universal ways to reference data across both systems etc.

I’d also state that I believe these integration benefits can be huge; but that they can be quite hard to achieve — not least because integration project teams often don’t align with the groups in charge of the individual systems.  i.e. conways law is a barrier to what you can achieve; and the right approach is then a political process to agree to integrate in this manner.

If it’s greenfield and you’re designing it up front; then that is obviously a lot easier to mandate these things — but you’ll be paying the costs of the integration and modelling —  before you need it.  So the challenge then becomes that people don’t see any benefits of integration until the different systems are independently useful; and it can be perceived as a pointless cost on getting things done.</z><z id="t1669889433" t="rickmoynihan Or rather it can be perceived as important; but not urgent — and therefore deprioritised — which leaves you with a political problem later 🙂"><y>#</y><d>2022-12-01</d><h>10:10</h><r>rickmoynihan</r>Or rather it can be perceived as important; but not urgent — and therefore deprioritised — which leaves you with a political problem later <b>🙂</b></z><z id="t1669898357" t="Takis_ thank you for infomation rick, i was thinking that i would be nice to have data in JSON in localized way, and with JSON-LD to make them part of general knowledge. i will see how it goes in the future : )"><y>#</y><d>2022-12-01</d><h>12:39</h><r>Takis_</r>thank you for infomation rick, i was thinking that i would be nice to have data in JSON in localized way, and with JSON-LD  to make them part of general knowledge. i will see how it goes in the future : )</z><z id="t1669902380" t="rickmoynihan You can also of course apply context later; behind the scenes even… i.e. convert arbitrary JSON into JSON-LD (and triples) later; and people upstream of that need never know."><y>#</y><d>2022-12-01</d><h>13:46</h><r>rickmoynihan</r>You can also of course apply context later; behind the scenes even… i.e. convert arbitrary JSON into JSON-LD (and triples) later; and people upstream of that need never know.</z><z id="t1669658192" t="quoll Any kind of data can be stored in any kind of database. The main problem comes from inefficiency in space, retrievals, or speed of operations, and the importance of these depends on how the data is used. Some of the real value in graphs comes from being able to make connections, which means doing joins. MongoDB is not good for this."><y>#</y><d>2022-11-28</d><h>17:56</h><w>quoll</w>Any kind of data can be stored in any kind of database. The main problem comes from inefficiency in space, retrievals, or speed of operations, and the importance of these depends on how the data is used.
Some of the real value in graphs comes from being able to make connections, which means doing joins. MongoDB is not good for this.</z><z id="t1669665661" t="Takis_ thank you quoll for your help, MongoDB has lookup and after MongoDB 6 works even if both collections are sharded, but i guess for fast triplestore you need graph database"><y>#</y><d>2022-11-28</d><h>20:01</h><w>Takis_</w>thank you quoll for your help, MongoDB has lookup and after MongoDB 6 works even if both collections are sharded, but i guess for fast triplestore you need graph database</z><z id="t1669741909" t="Kelvin This reminds me of a convo we had at our company over our graph DB project"><y>#</y><d>2022-11-29</d><h>17:11</h><w>Kelvin</w>This reminds me of a convo we had at our company over our graph DB project</z><z id="t1669741948" t="Kelvin It uses AWS Neptune and that was always the primary choice, but we discussed jerry-rigging a triple store with lambdas and S3 buckets"><y>#</y><d>2022-11-29</d><h>17:12</h><w>Kelvin</w>It uses AWS Neptune and that was always the primary choice, but we discussed jerry-rigging a triple store with lambdas and S3 buckets</z><z id="t1669744274" t="Eric Scott Hi all - Version 0.2.0 of https://github.com/ont-app/vocabulary has been released. It&apos;s kind of an omnibus of stuff I&apos;ve been hammering on here and there in odd moments. Finally had enough time to properly test and release. v 0.2.0 Moving from lein to deps.edn Adding shadow-cljs.edn file defining a :node-test build Custom tagged literal #lstr deprecated in favor of #voc/lstr Fix for issue 12: #voc/lstr now works in clojurescript source Fix for issue 19: Support for urns and arns. Fix for issue 20: Improvements to char-escaping behavior for keywords and URIs Fix for issue 21: vann metadata can be attached to vars as well as namespaces"><y>#</y><d>2022-11-29</d><h>17:51</h><w>Eric Scott</w>Hi all -
Version 0.2.0 of  <a href="https://github.com/ont-app/vocabulary" target="_blank">https://github.com/ont-app/vocabulary</a> has been released.

It&apos;s kind of an omnibus of stuff I&apos;ve been hammering on here and there in odd moments. Finally had enough time to properly test and release.

<pre>v 0.2.0
    Moving from lein to deps.edn
    Adding shadow-cljs.edn file defining a :node-test build
    Custom tagged literal #lstr deprecated in favor of #voc/lstr
    Fix for issue 12: #voc/lstr now works in clojurescript source
    Fix for issue 19: Support for urns and arns.
    Fix for issue 20: Improvements to char-escaping behavior for keywords and URIs
    Fix for issue 21: vann metadata can be attached to vars as well as namespaces</pre></z><z id="t1669746340" t="Bart Kleijngeld Good job! And who would&apos;ve thought my complaining would amount to something some day 😉"><y>#</y><d>2022-11-29</d><h>18:25</h><r>Bart Kleijngeld</r>Good job! And who would&apos;ve thought my complaining would amount to something some day <b>😉</b></z><z id="t1669744528" t="Eric Scott Thanks to [:attrs {:href &quot;/_/_/users/U03BYUDJLJF&quot;}] for complaining 🙂"><y>#</y><d>2022-11-29</d><h>17:55</h><w>Eric Scott</w>Thanks to <a>@bartkl</a> for complaining <b>🙂</b></z><z id="t1669904807" t="rickmoynihan A colleague asked a question about book recommendations around data modelling, it doesn’t have to be linked data specific, but she’s very interested in that side! Does anyone have any recommendations? She already mentioned the books: • Semantic Web for the working ontologist • semantic modelling for data • taxonomies • semantic software design"><y>#</y><d>2022-12-01</d><h>14:26</h><w>rickmoynihan</w>A colleague asked a question about book recommendations around data modelling, it doesn’t have to be linked data specific, but she’s very interested in that side!

Does anyone have any recommendations?

She already mentioned the books:

• Semantic Web for the working ontologist 
• semantic modelling for data
• taxonomies
• semantic software design</z><z id="t1669908179" t="respatialized I am obliged to mention Data and Reality every time this question comes up: https://buttondown.email/hillelwayne/archive/data-and-reality-2nd-edition/"><y>#</y><d>2022-12-01</d><h>15:22</h><r>respatialized</r>I am obliged to mention Data and Reality every time this question comes up:

<a href="https://buttondown.email/hillelwayne/archive/data-and-reality-2nd-edition/" target="_blank">https://buttondown.email/hillelwayne/archive/data-and-reality-2nd-edition/</a></z><z id="t1669916523" t="simongray https://patterns.dataincubator.org/"><y>#</y><d>2022-12-01</d><h>17:42</h><r>simongray</r><a href="https://patterns.dataincubator.org/" target="_blank">https://patterns.dataincubator.org/</a></z><z id="t1669982218" t="rickmoynihan Oh data and reality; great shout!"><y>#</y><d>2022-12-02</d><h>11:56</h><r>rickmoynihan</r>Oh data and reality; great shout!</z><z id="t1669946524" t="Takis_ Hello, as far as i understand RDF doesn&apos;t use object internal structure, so there is no schema,and modelling is based on vocabulary like english words. It seems natural and simple but why its not widely used? Its the many join perfomance cost? Or RDF adds complexity making modelling and quering hard?"><y>#</y><d>2022-12-02</d><h>02:02</h><w>Takis_</w>Hello, as far as i understand RDF doesn&apos;t use object internal structure, so there is no schema,and modelling is based on vocabulary like english words.
 It seems natural and simple but why its not widely used? Its the many join perfomance cost? Or RDF adds complexity making modelling and quering hard?</z><z id="t1669957843" t="quoll There&apos;s a long history, and many reasons 🙂"><y>#</y><d>2022-12-02</d><h>05:10</h><w>quoll</w>There&apos;s a long history, and many reasons <b>🙂</b></z><z id="t1669957921" t="quoll To start with, when it was first released, the only formal serialization of RDF was RDF/XML. Consequently, a lot of people thought it was just a type of XML, and a complex one at that. They&apos;d walk away from it before they understood it"><y>#</y><d>2022-12-02</d><h>05:12</h><w>quoll</w>To start with, when it was first released, the only formal serialization of RDF was RDF/XML. Consequently, a lot of people thought it was just a type of XML, and a complex one at that. They&apos;d walk away from it before they understood it</z><z id="t1669957976" t="quoll n3 was around, but it wasn&apos;t an official spec. Eventually Turtle came along, which addressed some of those problems, but by then, a lot of people were already against RDF"><y>#</y><d>2022-12-02</d><h>05:12</h><w>quoll</w>n3 was around, but it wasn&apos;t an official spec. Eventually Turtle came along, which addressed some of those problems, but by then, a lot of people were already against RDF</z><z id="t1669958054" t="quoll Since then, what I&apos;ve observed is that many people have never been exposed to graph models, and don&apos;t follow how to use graph query languages (it doesn&apos;t help that Facebook/Meta released &quot;GraphQL&quot; which is not a general purpose graph query language)"><y>#</y><d>2022-12-02</d><h>05:14</h><w>quoll</w>Since then, what I&apos;ve observed is that many people have never been exposed to graph models, and don&apos;t follow how to use graph query languages (it doesn&apos;t help that Facebook/Meta released &quot;GraphQL&quot; which is not a general purpose graph query language)</z><z id="t1669958129" t="quoll Not being exposed to it, people aren&apos;t interested in learning it. They&apos;re very resistant. I encounter a lot of this. Why? Well, it&apos;s just because they haven&apos;t heard of it, and it doesn&apos;t conform to what they know, and why invest all that time and effort into something that doesn&apos;t have a lot of market penetration?"><y>#</y><d>2022-12-02</d><h>05:15</h><w>quoll</w>Not being exposed to it, people aren&apos;t interested in learning it. They&apos;re very resistant. I encounter a lot of this. Why? Well, it&apos;s just because they haven&apos;t heard of it, and it doesn&apos;t conform to what they know, and why invest all that time and effort into something that doesn&apos;t have a lot of market penetration?</z><z id="t1669958576" t="quoll They also can&apos;t see the immediate benefits. I have been mapping my organization&apos;s RDBMS data into a graph, and after taking initial steps, I sat down with the data architect of the original system. I showed him everything I&apos;d done. His response was, &quot;What does this offer that we don&apos;t already do?&quot; I tried to explain that it was simpler, and the graph database was optimized for walking across the data, but he didn&apos;t see any benefits at all, and dismissed me. A few weeks later, I was working with a group who were talking about how they could cache transitive closure data that comes out of the database, and how best to do it. I was confused and asked them why they would not just query for it. They challenged me, &quot;Well how long would that take?&quot; I felt a bit defensive, and said that it was fast. (My experience on my notebook had been around 800ms, but if I asked for the entire dataset, then I&apos;d seen it take up to 2 seconds). &quot;How fast is fast though?&quot; &quot;Ummmm, well, it&apos;s taken up to...&quot; &quot;Is it less than an hour?&quot; &quot;A... [:attrs nil] &quot; &quot;Yes, is it less than that? The existing system takes longer.&quot; &quot;Ummm, it&apos;s usually less than a second&quot;"><y>#</y><d>2022-12-02</d><h>05:22</h><w>quoll</w>They also can&apos;t see the immediate benefits.
I have been mapping my organization&apos;s RDBMS data into a graph, and after taking initial steps, I sat down with the data architect of the original system. I showed him everything I&apos;d done. His response was, &quot;What does this offer that we don&apos;t already do?&quot; I tried to explain that it was simpler, and the graph database was optimized for walking across the data, but he didn&apos;t see any benefits at all, and dismissed me.
A few weeks later, I was working with a group who were talking about how they could cache transitive closure data that comes out of the database, and how best to do it. I was confused and asked them why they would not just query for it. They challenged me, &quot;Well how long would that take?&quot; I felt a bit defensive, and said that it was fast. (My experience on my notebook had been around 800ms, but if I asked for the entire dataset, then I&apos;d seen it take up to 2 seconds).
&quot;How fast is fast though?&quot;
&quot;Ummmm, well, it&apos;s taken up to...&quot;
&quot;Is it less than an hour?&quot;
&quot;A... <b>AN HOUR?!??</b>&quot;
&quot;Yes, is it less than that? The existing system takes longer.&quot;
&quot;Ummm, it&apos;s usually less than a second&quot;</z><z id="t1669958625" t="quoll So even though the benefit here was HUGE, people just weren&apos;t looking for it, and didn&apos;t understand it."><y>#</y><d>2022-12-02</d><h>05:23</h><w>quoll</w>So even though the benefit here was HUGE, people just weren&apos;t looking for it, and didn&apos;t understand it.</z><z id="t1669977791" t="Bart Kleijngeld I recognize a lot in this story. Our senior data architect chose RDF for modeling our conceptual information models because it&apos;s easy to merge and split distributed models across the company, and crucial to be able to rely on the globally identifier IRIs for resources. But like [:attrs {:href &quot;/_/_/users/U68Q5G1BJ&quot;}] says, it&apos;s also much more simple. When modeling data it&apos;s a breeze to be able to operate in an Open World Assumption, and to simply state what I want to model, such as subproperties, or inverse relations. Also, it&apos;s easy to append facts that extend existing standards or models without changing anything to existing models. I&apos;m completely sold, but many people only see challenges fail to see many of the benefits. Now to be fair, it can be quite an investment: finding people, changing technology stacks and processes. But I believe that the problems you&apos;ll solve will ultimately make up for that."><y>#</y><d>2022-12-02</d><h>10:43</h><w>Bart Kleijngeld</w>I recognize a lot in this story. Our senior data architect chose RDF for modeling our conceptual information models because it&apos;s easy to merge and split distributed models across the company, and crucial to be able to rely on the globally identifier IRIs for resources.

But like <a>@takis_</a> says, it&apos;s also much more simple. When modeling data it&apos;s a breeze to be able to operate in an Open World Assumption, and to simply state what I want to model, such as subproperties, or inverse relations. Also, it&apos;s easy to append facts that extend existing standards or models without changing anything to existing models.

I&apos;m completely sold, but many people only see challenges fail to see many of the benefits. Now to be fair, it can be quite an investment: finding people, changing technology stacks and processes. But I believe that the problems you&apos;ll solve will ultimately make up for that.</z><z id="t1669980606" t="Takis_ thank you for information and experience, i will try it and see, if its simple and useful sooner or later will be adopted i think"><y>#</y><d>2022-12-02</d><h>11:30</h><w>Takis_</w>thank you for information and experience, i will try it and see, if its simple and useful sooner or later will be adopted i think</z><z id="t1669983858" t="Mattias Side note - a schema and SQL is a bit like strict types, it’s a guard rail that many feel uncomfortable without. I think trying it out for a while is the only way, but many never take the time."><y>#</y><d>2022-12-02</d><h>12:24</h><w>Mattias</w>Side note - a schema and SQL is a bit like strict types, it’s a guard rail that many feel uncomfortable without. I think trying it out for a while is the only way, but many never take the time.</z><z id="t1669984840" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U68Q5G1BJ&quot;}] : I agree with the above answers; RDF/XML in particular has a lot to answer for! :rolling_on_the_floor_laughing: However, I also have another set of reasons… The main one is that I think there are significant differences between RDF, the semantic web, and linked data. All are of course highly related ideas; but they’re also separate, independent ideas, however most people, even practitioners in the field think they’re the same thing — i.e. that they’re all just synonyms for the same thing; and they’re really not. Basically in the early days, you had Timbl proselytising a vision for data on the web. Data on the web is then naturally a graph; so you have graph data, and URI’s are identifiers etc… so you basically get RDF; and if you put it on the web at the right places such that it dereferences you get linked data. There was also a lot of hand-wavy talk of agents using that data… e.g. stuff like software agents booking you a holiday… Then throw into the mix that you had the convergence of academics who had been doing research into description logics and KR coming out of the AI winter, looking for a new bandwagon to tie their interests too. I believe they also wanted a common DL that they could all use, so every research group didn’t need to invent their own one for their research projects. This effort became OWL, and they tied it and collectively sold it as the semantic web vision; and Timbl etc bought into it as it gave them some formal rigour, and a wider community etc. However that community of early adopters were heavily on the academic side; and academics are interested in papers; not practical “trivial problems” (which often turn out to be significant engineering challenges). Anyway, the Semantic Web had a lot of hype from this group; but it was heavily based on logic programming… Now logic programming is very cool; but it’s not suited to every problem; for example see the failure of Japan’s 5th generation computing project: https://en.wikipedia.org/wiki/Fifth_Generation_Computer_Systems ), the semantic web had many of these same problems, but also included distributed systems problems etc, so they made a hard problem even harder 🙂 Anyway in my view the problem was that the “Semantic Web” vision was never entirely realistic… it’s simply not suited to everyone; in the same sort of way the web is. So basically it under delivered on the promises; and much of the work was overly academic and complicated. Also in the standardisation processes back then there was a lot of design by committee without having proven parts of the value proposition. However we’re still left with lots of really good stuff — but sadly because the technologies and their pros and cons were heavily conflated, people don’t see the good bits… for example a lot of the value in RDF is in having an extensible, accretive data model. That is arguably a much bigger benefit for far more people than OWL will ever be; but very few people talk about it, so it gets ignored. IMHO there’s a huge amount of unexploited low hanging fruit in the worlds of RDF and Linked Data — it just needs packaged and marketed appropriately."><y>#</y><d>2022-12-02</d><h>12:40</h><w>rickmoynihan</w><a>@takis_</a>: I agree with the above answers; RDF/XML in particular has a lot to answer for! <b>:rolling_on_the_floor_laughing:</b>

However, I also have another set of reasons…  The main one is that I think there are significant differences between RDF, the semantic web, and linked data.

All are of course highly related ideas; but they’re also separate, independent ideas, however most people, even practitioners in the field think they’re the same thing — i.e. that they’re all just synonyms for the same thing; and they’re really not.

Basically in the early days, you had Timbl proselytising a vision for data on the web.  Data on the web is then naturally a graph; so you have graph data, and URI’s are identifiers etc… so you basically get RDF; and if you put it on the web at the right places such that it dereferences you get linked data.

There was also a lot of hand-wavy talk of agents using that data… e.g. stuff like software agents booking you a holiday… Then throw into the mix that you had the convergence of academics who had been doing research into description logics and KR coming out of the AI winter, looking for a new bandwagon to tie their interests too.

I believe they also wanted a common DL that they could all use, so every research group didn’t need to invent their own one for their research projects.  This effort became OWL, and they tied it and collectively sold it as the semantic web vision; and Timbl etc bought into it as it gave them some formal rigour, and a wider community etc.

However that community of early adopters were heavily on the academic side; and academics are interested in papers; not practical “trivial problems” (which often turn out to be significant engineering challenges).

Anyway, the Semantic Web had a lot of hype from this group; but it was heavily based on logic programming… Now logic programming is very cool; but it’s not suited to every problem; for example see the failure of Japan’s 5th generation computing project: <a href="https://en.wikipedia.org/wiki/Fifth_Generation_Computer_Systems" target="_blank">https://en.wikipedia.org/wiki/Fifth_Generation_Computer_Systems</a> ), the semantic web had many of these same problems, but also included distributed systems problems etc, so they made a hard problem even harder <b>🙂</b>

Anyway in my view the problem was that the “Semantic Web” vision was never entirely realistic… it’s simply not suited to everyone; in the same sort of way the web is.

So basically it under delivered on the promises; and much of the work was overly academic and complicated.  Also in the standardisation processes back then there was a lot of design by committee without having proven parts of the value proposition.

However we’re still left with lots of really good stuff — but sadly because the technologies and their pros and cons were heavily conflated, people don’t see the good bits… for example a lot of the value in RDF is in having an extensible, accretive data model.  That is arguably a much bigger benefit for far more people than OWL will ever be; but very few people talk about it, so it gets ignored.

IMHO there’s a huge amount of unexploited low hanging fruit in the worlds of RDF and Linked Data — it just needs packaged and marketed appropriately.</z><z id="t1669984857" t="rickmoynihan There are other dimensions to this too… for example triple stores have different performance properties to relational databases; and there are less options available, so you might not find one that fits your needs precisely, even though such a thing could exist."><y>#</y><d>2022-12-02</d><h>12:40</h><w>rickmoynihan</w>There are other dimensions to this too… for example triple stores have different performance properties to relational databases; and there are less options available, so you might not find one that fits your needs precisely, even though such a thing could exist.</z><z id="t1669990126" t="Takis_ thank you rick : ) i like all databases actually, sometime i will try graph databases also"><y>#</y><d>2022-12-02</d><h>14:08</h><w>Takis_</w>thank you rick : ) i like all databases actually, sometime i will try graph databases also</z><z id="t1669993883" t="Takis_ i think important is to see the programming way, for example tables you program in relational way, tables with each column 1 value, documents you have arrays and hash-maps so you program in functional way more, in graphs you program in logic programming way. I think deciding the programming way you like matters, instead of thinking what is the best."><y>#</y><d>2022-12-02</d><h>15:11</h><w>Takis_</w>i think important is to see the programming way, for example tables you program in relational way,
tables with each column 1 value, documents you have arrays and hash-maps so you program in functional
way more, in graphs you program in logic programming way. I think deciding the programming way you like
matters, instead of thinking what is the best.</z><z id="t1669997286" t="rickmoynihan I think there’s obviously a lot of truth to this; it’s undeniably true that the data model massively effects how you put data in and out of a system and in turn how you model it; which has wider implications for what is relevant, what is simple, what is hard etc. Though I’m not sure I’d characterise the differences in terms of programming as you have."><y>#</y><d>2022-12-02</d><h>16:08</h><r>rickmoynihan</r>I think there’s obviously a lot of truth to this; it’s undeniably true that the data model massively effects how you put data in and out of a system and in turn how you model it; which has wider implications for what is relevant, what is simple, what is hard etc.

Though I’m not sure I’d characterise the differences in terms of programming as you have.</z><z id="t1669994849" t="Mattias [:attrs {:href &quot;/_/_/users/U06HHF230&quot;}] Thanks for a fantastic history briefing! Much appreciated 😀 "><y>#</y><d>2022-12-02</d><h>15:27</h><w>Mattias</w><a>@rickmoynihan</a> Thanks for a fantastic history briefing! Much appreciated <b>😀</b> </z><z id="t1669997642" t="rickmoynihan You’re welcome. It’s a very rough approximation of the history, as I understand it. It’s certainly not entirely accurate — but I think it does offer an explanation as to the current state, and why things aren’t more widely adopted. Another perspective to view it from is as a continuation of two earlier AI communities… The neats and scruffies: https://en.wikipedia.org/wiki/Neats_and_scruffies"><y>#</y><d>2022-12-02</d><h>16:14</h><r>rickmoynihan</r>You’re welcome.  It’s a very rough approximation of the history, as I understand it.  It’s certainly not entirely accurate — but I think it does offer an explanation as to the current state, and why things aren’t more widely adopted.

Another perspective to view it from is as a continuation of two earlier AI communities…  The neats and scruffies:

<a href="https://en.wikipedia.org/wiki/Neats_and_scruffies" target="_blank">https://en.wikipedia.org/wiki/Neats_and_scruffies</a></z><z id="t1669996397" t="respatialized I think we might be nearing a comeback for KR and some of the more “old fashioned” AI stuff now that deep learning is hitting the limits of what it can accomplish without folding in other approaches, which may mean a similar resurgence in RDF, knowledge graphs, and the like. I think it’s useful to compare and contrast two projects that Meta’a AI team released recently for an illustration of why. The first is https://arstechnica.com/information-technology/2022/11/after-controversy-meta-pulls-demo-of-ai-model-that-writes-scientific-papers , a supposedly “scientific AI” that was so adept at producing pseudoscience that they took it offline the week its public demo was released. That’s because it was just another https://dl.acm.org/doi/10.1145/3442188.3445922 doing probabilistic copy-and-paste, albeit one trained exclusively on scientific literature. It had no direct representation of truth, which to me is about as basic a requirement for science as you can possibly imagine. The second is https://arstechnica.com/information-technology/2022/11/meta-researchers-create-ai-that-masters-diplomacy-tricking-human-players/amp/ , an AI able to compete with human players at Diplomacy, a strategy game that involves bluffing, subterfuge, and anticipating the plans and actions of other players - so not just modeling the truth but also what others believe to be the truth. According to Ernest Davis, the system required to make Cicero an effective player is substantially more complex than the “just throw more data and 60,000 TPUs at a specific neural network architecture” approach that has grabbed headlines recently. He https://garymarcus.substack.com/p/what-does-meta-ais-diplomacy-winning &gt; Strikingly, and in opposition to much of the Zeitgeist, Cicero relies quite heavily on hand-crafting, both in the data sets, and in the architecture; in this sense it is in many ways more reminiscent of classical “Good Old Fashioned AI” than deep learning systems that tend to be less structured, and less customized to particular problems. There is far more innateness here than we have typically seen in recent AI systems. I think graph data and RDF are much more relevant to the second approach than the first, so there may be a lot of unexplored territory in “good old fashioned AI” now that computers are substantially faster than when the AI winter took hold."><y>#</y><d>2022-12-02</d><h>15:53</h><w>respatialized</w>I think we might be nearing a comeback for KR and some of the more “old fashioned” AI stuff now that deep learning is hitting the limits of what it can accomplish without folding in other approaches, which may mean a similar resurgence in RDF, knowledge graphs, and the like.

I think it’s useful to compare and contrast two projects that Meta’a AI team released recently for an illustration of why. 

The first is <a href="https://arstechnica.com/information-technology/2022/11/after-controversy-meta-pulls-demo-of-ai-model-that-writes-scientific-papers" target="_blank">https://arstechnica.com/information-technology/2022/11/after-controversy-meta-pulls-demo-of-ai-model-that-writes-scientific-papers</a>, a supposedly “scientific AI” that was so adept at producing pseudoscience that they took it offline the week its public demo was released. That’s because it was just another <a href="https://dl.acm.org/doi/10.1145/3442188.3445922" target="_blank">https://dl.acm.org/doi/10.1145/3442188.3445922</a> doing probabilistic copy-and-paste, albeit one trained exclusively on scientific literature. It had no direct representation of truth, which to me is about as basic a requirement for science as you can possibly imagine.

The second is <a href="https://arstechnica.com/information-technology/2022/11/meta-researchers-create-ai-that-masters-diplomacy-tricking-human-players/amp/" target="_blank">https://arstechnica.com/information-technology/2022/11/meta-researchers-create-ai-that-masters-diplomacy-tricking-human-players/amp/</a>, an AI able to compete with human players at Diplomacy, a strategy game that involves bluffing, subterfuge, and anticipating the plans and actions of other players - so not just modeling the truth but also what others believe to be the truth. According to Ernest Davis, the system required to make Cicero an effective player is substantially more complex than the “just throw more data and 60,000 TPUs at a specific neural network architecture” approach that has grabbed headlines recently.

He <a href="https://garymarcus.substack.com/p/what-does-meta-ais-diplomacy-winning" target="_blank">https://garymarcus.substack.com/p/what-does-meta-ais-diplomacy-winning</a>

&gt; Strikingly, and in opposition to much of the Zeitgeist,  Cicero relies quite heavily on hand-crafting, both in the data sets, and in the architecture; in this sense it is in many ways more reminiscent of classical “Good Old Fashioned AI” than deep learning systems that tend to be less structured, and less customized to particular problems. There is far more innateness here than we have typically seen in recent AI systems.
I think graph data and RDF are much more relevant to the second approach than the first, so there may be a lot of unexplored territory in “good old fashioned AI” now that computers are substantially faster than when the AI winter took hold.</z><z id="t1669998684" t="rickmoynihan [:attrs {:href &quot;/_/_/users/UFTRLDZEW&quot;}] : I certainly think there’s a role to play for GOFAI and KR in the future of AI — and I share your concerns around purely stochastic approaches. Infact arguably there are vanishingly few purely stochastic processes; for example an often neglected fact is the role of techniques like mini max in alpha go’s design. Alpha go didn’t learn minimax, it was engineered in by people. Then even if stochastic processes do win out; it’s hard to imagine them not essentially learning things like FOPL and set theory etc and then having to use it. Cicero is interesting; thanks for sharing I’ve not seen it before. About 20 years ago I worked at a defunct startup working with multi-agent systems. So there was lots of GOFAI. We used a defeasible reasoning engine we (by which I mean our CTO) had developed, and had an agent communication language with semantics based on mutual belief… and agents could then reason about what they believed other agents believed; and yes you could also get arbitrarily meta… reasoning about if they believe I believe X, but I believe Y then Z etc. It was all super cool stuff; but something perhaps better explored in academia rather than industry 🙂"><y>#</y><d>2022-12-02</d><h>16:31</h><w>rickmoynihan</w><a>@afoltzm</a>: I certainly think there’s a role to play for GOFAI and KR in the future of AI — and I share your concerns around purely stochastic approaches.  Infact arguably there are vanishingly few purely stochastic processes; for example an often neglected fact is the role of techniques like mini max in alpha go’s design.  Alpha go didn’t learn minimax, it was engineered in by people.

Then even if stochastic processes do win out; it’s hard to imagine them not essentially learning things like FOPL and set theory etc and then having to use it.

Cicero is interesting; thanks for sharing I’ve not seen it before.  About 20 years ago I worked at a defunct startup working with multi-agent systems.  So there was lots of GOFAI.  We used a defeasible reasoning engine we (by which I mean our CTO) had developed, and had an agent communication language with semantics based on mutual belief… and agents could then reason about what they believed other agents believed; and yes you could also get arbitrarily meta… reasoning about if they believe I believe X, but I believe Y then Z etc.

It was all super cool stuff; but something perhaps better explored in academia rather than industry <b>🙂</b></z><z id="t1669999107" t="respatialized Indeed. I was also thinking about AlphaGo when writing my comment - specifically that it was designed to use Monte Carlo tree search (it certainly didn&apos;t &quot;learn&quot; search on its own!)"><y>#</y><d>2022-12-02</d><h>16:38</h><r>respatialized</r>Indeed. I was also thinking about AlphaGo when writing my comment - specifically that it was designed to use Monte Carlo tree search (it certainly didn&apos;t &quot;learn&quot; search on its own!)</z><z id="t1669999279" t="respatialized You should subscribe to Gary Marcus&apos;s Substack; he has a similar affinity for GOFAI and it&apos;s a useful way to keep up with research developments (without the breathless tech press hype) through his commentary."><y>#</y><d>2022-12-02</d><h>16:41</h><r>respatialized</r>You should subscribe to Gary Marcus&apos;s Substack; he has a similar affinity for GOFAI and it&apos;s a useful way to keep up with research developments (without the breathless tech press hype) through his commentary.</z><z id="t1670002092" t="quoll Defeasibly logic is interesting, but I think it’s too many steps ahead of where industry is. Like: • Step 1 - low hanging fruit. e.g. just representing data in a graph can make it easier to navigate, expand upon, etc. • Step 2 - description of the graphed data, in schema and ontology. This documents the structure, and can automate some of the connections. • Step 3 - reasoning. This can be rules, which are easy to implement. Or they can be based on description logics. This is the promise of OWL. • Step 4 - Advanced reasoning. Defeasible logic is here. Each step is a reasonable effort from the previous step, but from what I’ve seen, the effort is totally worth it. What does NOT work, is trying to skip ahead from where an organization is. The technology might work, but I haven’t yet seen a benefit from bringing it into production in a business"><y>#</y><d>2022-12-02</d><h>17:28</h><r>quoll</r>Defeasibly logic is interesting, but I think it’s too many steps ahead of where industry is. Like:
• Step 1 - low hanging fruit. e.g. just representing data in a graph can make it easier to navigate, expand upon, etc.
• Step 2 - description of the graphed data, in schema and ontology. This documents the structure, and can automate some of the connections.
• Step 3 - reasoning. This can be rules, which are easy to implement. Or they can be based on description logics. This is the promise of OWL.
• Step 4 - Advanced reasoning. Defeasible logic is here.
Each step is a reasonable effort from the previous step, but from what I’ve seen, the effort is totally worth it.

What does NOT work, is trying to skip ahead from where an organization is. The technology might work, but I haven’t yet seen a benefit from bringing it into production in a business</z><z id="t1670002123" t="quoll I’ve seen organizations gaining benefit all the way out at step 3"><y>#</y><d>2022-12-02</d><h>17:28</h><r>quoll</r>I’ve seen organizations gaining benefit all the way out at step 3</z><z id="t1670331610" t="rickmoynihan yeah I agree with all of that [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] . I think there’s even more value at the low hanging fruit stage though; for example property oriented thinking; using URIs (or just namespaced) global indentifiers; just having extensible data etc…"><y>#</y><d>2022-12-06</d><h>13:00</h><r>rickmoynihan</r>yeah I agree with all of that <a>@U051N6TTC</a>.

I think there’s even more value at the low hanging fruit stage though; for example property oriented thinking; using URIs (or just namespaced) global indentifiers; just having extensible data etc…</z><z id="t1670270820" t="Bart Kleijngeld In our company we have chosen an RDF stack to do our data modeling, particularly RDFS/OWL for conceptual models, and SHACL for logical models. Many of our data architects have absolutely no familiarity with it and now need to learn it. It turns out it&apos;s hard to get them excited however... &quot;What&apos;s wrong with UML?&quot;, &quot;Why not simply go for a centralized approach and store everything in a relational DB?&quot; (we are moving away from that, favoring a data mesh approach). I have plenty of reasons why I think a graph-based choice like RDF is preferable over UML/relational for this purpose (I briefly shared my view here a few days ago), but, frankly, I&apos;m actually not very experienced myself in this field, so I was hoping to learn from people here 🙂 . I will use your input, among other sources, for a presentation I&apos;m preparing on this. Why is RDF so suited to do data modeling (or isn&apos;t it?)? What are problems with UML/relational that RDF does not suffer from? What are caveats of RDF to take into consideration? Etc. Thanks!"><y>#</y><d>2022-12-05</d><h>20:07</h><w>Bart Kleijngeld</w>In our company we have chosen an RDF stack to do our data modeling, particularly RDFS/OWL for conceptual models, and SHACL for logical models. Many of our data architects have absolutely no familiarity with it and now need to learn it. It turns out it&apos;s hard to get them excited however... &quot;What&apos;s wrong with UML?&quot;, &quot;Why not simply go for a centralized approach and store everything in a relational DB?&quot; (we are moving away from that, favoring a data mesh approach).

I have plenty of reasons why I think a graph-based choice like RDF is preferable over UML/relational for this purpose (I briefly shared my view here a few days ago), but, frankly, I&apos;m actually not very experienced myself in this field, so I was hoping to learn from people here <b>🙂</b>. I will use your input, among other sources, for a presentation I&apos;m preparing on this.

Why is RDF so suited to do data modeling (or isn&apos;t it?)? What are problems with UML/relational that RDF does not suffer from? What are caveats of RDF to take into consideration? Etc.

Thanks!</z><z id="t1670272485" t="quoll UML and relational are orthogonal, IMO. Data that are regular and well defined is appropriate for relational storage. This is especially the case when the most common access mode refers to entire records at once. Data that evolve in structure, are less record-oriented, and have a lot of information through linkages (e.g. tree structures) are much more appropriate for graph storage."><y>#</y><d>2022-12-05</d><h>20:34</h><r>quoll</r>UML and relational are orthogonal, IMO.

Data that are regular and well defined is appropriate for relational storage. This is especially the case when the most common access mode refers to entire records at once.

Data that evolve in structure, are less record-oriented, and have a lot of information through linkages (e.g. tree structures) are much more appropriate for graph storage.</z><z id="t1670273312" t="quoll UML is a modeling language with a long history of modeling software. I don’t know if it prefers an OWA or CWA, though I think it is agnostic to that (because there are systems that convert between UML and OWL). It is typically used for documentation purposes, though there have been some attempts at automating processes with it, hence the development of OCL (Object Constraint Language). OWL is specifically designed for the OWA, which makes it inappropriate for software development, but well suited for data. This is particularly true for data on the web, where not all current data may be accessible at any time, and where data continue to grow. (side note: I hate that “data” is a plural word). It has significantly more descriptive capability around relationships than UML has, and consequently allows for better modeling. However, this is a double edged sword, as relatively fewer people have exposure to OWL, and are unaware of the these capabilities, meaning that they are not used as often as they could be. Importantly, OWL was designed from the outset to be reasoned over, and there are many automated systems for doing exactly this. This, combined with the greater expressivity of OWL, is what has allowed automated reasoning in multiple domains, including medical (SNOMED), pharmaceutical, and financial domains. There are many organizations who rely on these reasoning systems."><y>#</y><d>2022-12-05</d><h>20:48</h><r>quoll</r>UML is a modeling language with a long history of modeling software. I don’t know if it prefers an OWA or CWA, though I think it is agnostic to that (because there are systems that convert between UML and OWL). It is typically used for documentation purposes, though there have been some attempts at automating processes with it, hence the development of OCL (Object Constraint Language).
OWL is specifically designed for the OWA, which makes it inappropriate for software development, but well suited for data. This is particularly true for data on the web, where not all current data may be accessible at any time, and where data continue to grow. (side note: I hate that “data” is a plural word). It has significantly more descriptive capability around relationships than UML has, and consequently allows for better modeling. However, this is a double edged sword, as relatively fewer people have exposure to OWL, and are unaware of the these capabilities, meaning that they are not used as often as they could be.
Importantly, OWL was designed from the outset to be reasoned over, and there are many automated systems for doing exactly this. This, combined with the greater expressivity of OWL, is what has allowed automated reasoning in multiple domains, including medical (SNOMED), pharmaceutical, and financial domains. There are many organizations who rely on these reasoning systems.</z><z id="t1670273592" t="quoll As some examples, NASA uses RDF/OWL for inventory systems in building spacecraft, SNOMED uses it to automate relationships between medical concepts, Deutsche Bank uses it to automate money laundering and fraud detection, and every major pharmaceutical company uses it to identify candidates for drug trials"><y>#</y><d>2022-12-05</d><h>20:53</h><r>quoll</r>As some examples, NASA uses RDF/OWL for inventory systems in building spacecraft, SNOMED uses it to automate relationships between medical concepts, Deutsche Bank uses it to automate money laundering and fraud detection, and every major pharmaceutical company uses it to identify candidates for drug trials</z><z id="t1670273642" t="quoll I provided examples to demonstrate that it’s not all hype. It has significant utility."><y>#</y><d>2022-12-05</d><h>20:54</h><r>quoll</r>I provided examples to demonstrate that it’s not all hype. It has significant utility.</z><z id="t1670274309" t="Bart Kleijngeld Examples help me out here, thanks! And yes, it does feel awkward that data is plural 😆 . Sadly, I think the reasoning capabilities aren&apos;t we look to utilize any time soon. We are a large company and wish to model our (business) language, and all the data flowing through and being produced and stored in our systems. Note: even the data itself won&apos;t be in RDF, only the data models that needs to be conformed to. For now, at least. We like to take a decentralized approach here, a bit like the AAA slogan: anyone can say anything. Embracing OWA, this web-like approach really sounds like a match with RDFS/ OWL to me. It&apos;s just that I don&apos;t know well enough where UML is more limited in ways that matter to us. For instance: I don&apos;t know if one can express &quot;subPropertyOf&quot; in UML reasonably, let alone properties as first-class citizens to begin with. That&apos;s homework for me I guess, although be my guest if you have anything to say about that too. Thanks as always"><y>#</y><d>2022-12-05</d><h>21:05</h><r>Bart Kleijngeld</r>Examples help me out here, thanks! And yes, it does feel awkward that data is plural <b>😆</b>.

Sadly, I think the reasoning capabilities aren&apos;t we look to utilize any time soon. We are a large company and wish to model our (business) language, and all the data flowing through and being produced and stored in our systems.

Note: even the data itself won&apos;t be in RDF, only the data models that needs to be conformed to. For now, at least.

We like to take a decentralized approach here, a bit like the AAA slogan: anyone can say anything. Embracing OWA, this web-like approach really sounds like a match with RDFS/ OWL to me.

It&apos;s just that I don&apos;t know well enough where UML is more limited in ways that matter to us. For instance: I don&apos;t know if one can express &quot;subPropertyOf&quot; in UML reasonably, let alone properties as first-class citizens to begin with. That&apos;s homework for me I guess, although be my guest if you have anything to say about that too. Thanks as always</z><z id="t1670276550" t="quoll I didn’t think there was, but I looked it up and… it exists, but it’s ugly"><y>#</y><d>2022-12-05</d><h>21:42</h><r>quoll</r>I didn’t think there was, but I looked it up and… it exists, but it’s ugly</z><z id="t1670276576" t="quoll I don’t know that it’s part of the UML spec though"><y>#</y><d>2022-12-05</d><h>21:42</h><r>quoll</r>I don’t know that it’s part of the UML spec though</z><z id="t1670276781" t="quoll Oh, no, apparently it’s legal. You just don’t see it used much"><y>#</y><d>2022-12-05</d><h>21:46</h><r>quoll</r>Oh, no, apparently it’s legal. You just don’t see it used much</z><z id="t1670276790" t="quoll “Generalization between associations”"><y>#</y><d>2022-12-05</d><h>21:46</h><r>quoll</r>“Generalization between associations”</z><z id="t1670311154" t="jumar Im a complete noob, just watching the discussions here. After reading this https://clojurians.slack.com/archives/C09GHBXRC/p1670273592043609?thread_ts=1670270820.424829&amp;amp;channel=C09GHBXRC&amp;amp;message_ts=1670273592.043609 I’m wondering what are some more boring examples of its utility apart from NASA, fraud detection, and farmaceutical companies. All of that sounds a bit special…"><y>#</y><d>2022-12-06</d><h>07:19</h><r>jumar</r>Im a complete noob, just watching the discussions here.
After reading this <a href="https://clojurians.slack.com/archives/C09GHBXRC/p1670273592043609?thread_ts=1670270820.424829&amp;amp;channel=C09GHBXRC&amp;amp;message_ts=1670273592.043609" target="_blank">https://clojurians.slack.com/archives/C09GHBXRC/p1670273592043609?thread_ts=1670270820.424829&amp;amp;channel=C09GHBXRC&amp;amp;message_ts=1670273592.043609</a>
I’m wondering what are some more boring examples of its utility apart from NASA, fraud detection, and farmaceutical companies. All of that sounds a bit special…</z><z id="t1670332698" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U06BE1L6T&quot;}] RDF’s utility or OWL’s?"><y>#</y><d>2022-12-06</d><h>13:18</h><r>rickmoynihan</r><a>@U06BE1L6T</a> RDF’s utility or OWL’s?</z><z id="t1670342039" t="jumar I guess RDF but perhaps both?"><y>#</y><d>2022-12-06</d><h>15:53</h><r>jumar</r>I guess RDF but perhaps both?</z><z id="t1670342841" t="quoll I’m working with medical data right now. Using public files, which are mostly in tabular format. There are lots of ID codes that refer to foreign datasets. For instance, Vaccine manufacturers are provided in files from the CDC (Center for Disease Control). They also reference CVX codes, which are the codes for vaccines. There are also (separate) files that link CVX codes to National Drug Codes (NDC). There are also other systems that connect these codes into SNOMED (Systematized NOmenclature for MEDical data). I [:attrs nil] put all the data into tables, and create foreign keys between them. In fact, my company has done this in the past. The ELT process is difficult, because these systems tend to model their data differently, and will sometimes take another system’s code and append extra letters to it. You often find yourself having to link many tables, and it can be very difficult to learn the schema to traverse from one part of the dataset to another. It’s a painful mess, but it works."><y>#</y><d>2022-12-06</d><h>16:07</h><r>quoll</r>I’m working with medical data right now. Using public files, which are mostly in tabular format. There are lots of ID codes that refer to foreign datasets. For instance, Vaccine manufacturers are provided in files from the CDC (Center for Disease Control). They also reference CVX codes, which are the codes for vaccines. There are also (separate) files that link CVX codes to National Drug Codes (NDC). There are also other systems that connect these codes into SNOMED (Systematized NOmenclature for MEDical data). I <b>could</b> put all the data into tables, and create foreign keys between them. In fact, my company has done this in the past. The ELT process is difficult, because these systems tend to model their data differently, and will sometimes take another system’s code and append extra letters to it. You often find yourself having to link many tables, and it can be very difficult to learn the schema to traverse from one part of the dataset to another.  It’s a painful mess, but it works.</z><z id="t1670342960" t="quoll Putting the whole thing into RDF simplifies the process significantly. ELT still has to happen, but it’s simplified. If something has different codes in different systems, I can keep both, and just link them. I don’t need to figure out foreign keys. I can traverse across the graph quickly and easily. And change management has become a thing of the past."><y>#</y><d>2022-12-06</d><h>16:09</h><r>quoll</r>Putting the whole thing into RDF simplifies the process significantly. ELT still has to happen, but it’s simplified. If something has different codes in different systems, I can keep both, and just link them. I don’t need to figure out foreign keys. I can traverse across the graph quickly and easily. And change management has become a thing of the past.</z><z id="t1670343003" t="quoll There’s nothing particularly complex about this. We’re just gaining utility by using a graph shape for the data instead of tabular"><y>#</y><d>2022-12-06</d><h>16:10</h><r>quoll</r>There’s nothing particularly complex about this. We’re just gaining utility by using a graph shape for the data instead of tabular</z><z id="t1670343457" t="Bart Kleijngeld That&apos;s a nice example that might appeal to my colleagues."><y>#</y><d>2022-12-06</d><h>16:17</h><r>Bart Kleijngeld</r>That&apos;s a nice example that might appeal to my colleagues.</z><z id="t1670370238" t="curtosis When you say you’re using SHACL for logical models, can you give a (suitably anonymized) example?"><y>#</y><d>2022-12-06</d><h>23:43</h><r>curtosis</r>When you say you’re using SHACL for logical models, can you give a (suitably anonymized) example?</z><z id="t1670395869" t="Bart Kleijngeld I&apos;m reading your question in two ways, so I&apos;ll just answer it in both 🙂 . If you&apos;re looking for an example on how we use SHACL to obtain a logical model, it would look something like this: :CarShape a sh:NodeShape ; sh:targetClas vehicle:Car ; sh:property :idShape ; sh:property [ sh:path vehicle:tireCount sh:datatype xsd:int ; sh:minCount 1 ; ] . :idShape a sh:PropertyShape ; # ... Targeting the Car class from the vocabulary (conceptual model) you provide logical constraints this way, forming a logical model. If, on the other hand, you&apos;re looking for what we use such logical models for, let me try to answer that as well. The idea is to describe our data formally in a (large) conceptual model done in RDFS/OWL, so you can focus just on meaning and relationships under the Open World Assumption. This is great for modeling. From there, use cases in IT arise. Information is selected from the conceptual model, and logical constraints (like above) are added using SHACL. The resulting logical model can then be used to generate all sorts of target schemas (this is basically the project I work on), i.e. JSON Schema, OpenAPI specs, Pydantic models, SQL DDL, you name it (Work in progress!). I hope that clarifies it for you."><y>#</y><d>2022-12-07</d><h>06:51</h><r>Bart Kleijngeld</r>I&apos;m reading your question in two ways, so I&apos;ll just answer it in both <b>🙂</b>.

If you&apos;re looking for an example on how we use SHACL to obtain a logical model, it would look something like this:
<pre>:CarShape a sh:NodeShape ;
    sh:targetClas vehicle:Car ;
    sh:property :idShape ;
    sh:property [
        sh:path vehicle:tireCount
        sh:datatype xsd:int ;
        sh:minCount 1 ;
    ] .

:idShape a sh:PropertyShape ;
    # ...</pre>
Targeting the <code>Car</code> class from the vocabulary (conceptual model) you provide logical constraints this way, forming a logical model.

If, on the other hand, you&apos;re looking for what we use such logical models for, let me try to answer that as well. The idea is to describe our data formally in a (large) conceptual model done in RDFS/OWL, so you can focus just on meaning and relationships under the Open World Assumption. This is great for modeling.

From there, use cases in IT arise. Information is selected from the conceptual model, and logical constraints (like above) are added using SHACL. The resulting logical model can then be used to generate all sorts of target schemas (this is basically the project I work on), i.e. JSON Schema, OpenAPI specs, Pydantic models, SQL DDL, you name it (Work in progress!). I hope that clarifies it for you.</z><z id="t1670422610" t="curtosis I was thinking mostly the first, but the second was also super helpful. Thanks!!"><y>#</y><d>2022-12-07</d><h>14:16</h><r>curtosis</r>I was thinking mostly the first, but the second was also super helpful. Thanks!!</z><z id="t1670422853" t="curtosis That’s actually quite relevant to the work that I’m doing, though in some cases I could see it making sense to “generate” (modulo a lot of human knowledge) in the other direction: given a bunch of possibly-overlapping (primarily-)SQL logical models, generate SHACL to describe their relationship to a conceptual model/ontology, possibly expanding/refining the conceptual model as needed."><y>#</y><d>2022-12-07</d><h>14:20</h><r>curtosis</r>That’s actually quite relevant to the work that I’m doing, though in some cases I could see it making sense to “generate” (modulo a lot of human knowledge) in the other direction: given a bunch of possibly-overlapping (primarily-)SQL logical models, generate SHACL to describe their relationship to a conceptual model/ontology, possibly expanding/refining the conceptual model as needed.</z><z id="t1670423333" t="Bart Kleijngeld Never considered that way around. interesting. Could you elaborate on your use case/work/project perhaps? Some context might make me appreciate what you&apos;re doing more"><y>#</y><d>2022-12-07</d><h>14:28</h><r>Bart Kleijngeld</r>Never considered that way around. interesting. Could you elaborate on your use case/work/project perhaps? Some context might make me appreciate what you&apos;re doing more</z><z id="t1670423963" t="curtosis Without getting too specific 😉 sure… A common problem in a lot of large government agencies, especially the “boring” ones, is that they cover several major programs that are kind of related, but have some significant differences in approaches to data, only partially due to simple organizational boundaries. As a hypothetical example, there may be several programs that provide certain benefits or support to households, but because of the way the programs are designed (from a policy perspective) they define “household” quite differently. So there’s a nontrivial challenge in being able to identify which elements of those models are equivalent (and thus commensurable) and those that are not. So you want to be able to enable users (primarily but not exclusively) analysts to be able to find the right data and use it correctly, but you can’t realistically do much from the top down to standardize things. We’ve had some success building conceptual models in RDF/OWL, but the connection back to the logical models has always been fairly gauzy."><y>#</y><d>2022-12-07</d><h>14:39</h><r>curtosis</r>Without getting too specific <b>😉</b> sure…

A common problem in a lot of large government agencies, especially the “boring” ones, is that they cover several major programs that are kind of related, but have some significant differences in approaches to data, only partially due to simple organizational boundaries. As a hypothetical example, there may be several programs that provide certain benefits or support to households, but because of the way the programs are designed (from a policy perspective) they define “household” quite differently. So there’s a nontrivial challenge in being able to identify which elements of those models are equivalent (and thus commensurable) and those that are not.

So you want to be able to enable users (primarily but not exclusively) analysts to be able to find the right data and use it correctly, but you can’t realistically do much from the top down to standardize things. We’ve had some success building conceptual models in RDF/OWL, but the connection back to the logical models has always been fairly gauzy.</z><z id="t1670424210" t="Bart Kleijngeld Haha, good to be careful. Interesting. There&apos;s definitely seems to be some overlap in our use cases. Do I understand correctly that you wish to have all the data represented in RDF ultimately? So that data integration and federated querying (is that what you call it? Still learning) can be done?"><y>#</y><d>2022-12-07</d><h>14:43</h><r>Bart Kleijngeld</r>Haha, good to be careful. Interesting. There&apos;s definitely seems to be some overlap in our use cases. Do I understand correctly that you wish to have all the data represented in RDF ultimately? So that data integration and federated querying (is that what you call it? Still learning) can be done?</z><z id="t1670425251" t="curtosis I don’t think there’s any appetite to put all the data in RDF — for starters a lot of it really is transactional (and there is a LOT of it*) and it’s not clear** how much of it would benefit from a graph perspective — but having the metadata all integrated in one catalog would be extremely valuable. That said, there are subdomains where the relationship graph would actually be super useful. * ~1Bn complex actions (dozens of txs per action) per year At least to the business-value folks. Demonstrating it at scale is part of the challenge."><y>#</y><d>2022-12-07</d><h>15:00</h><r>curtosis</r>I don’t think there’s any appetite to put all the data in RDF — for starters a lot of it really is transactional (and there is a LOT of it*) and it’s not clear** how much of it would benefit from a graph perspective — but having the metadata all integrated in one catalog would be extremely valuable. That said, there are subdomains where the relationship graph would actually be super useful.
* ~1Bn complex actions (dozens of txs per action) per year
 At least to the business-value folks. Demonstrating it at scale is part of the challenge.</z><z id="t1670425426" t="curtosis We did a demonstration several years back on one of the natually-graphy domains (~6M primary subjects) and that graph alone was somewhere around 1.5Bn triples."><y>#</y><d>2022-12-07</d><h>15:03</h><r>curtosis</r>We did a demonstration several years back on one of the natually-graphy domains (~6M primary subjects) and that graph alone was somewhere around 1.5Bn triples.</z><z id="t1670425599" t="quoll Yes… triples grow quickly 🙂"><y>#</y><d>2022-12-07</d><h>15:06</h><r>quoll</r>Yes… triples grow quickly <b>🙂</b></z><z id="t1670426755" t="curtosis we had a Cray graph analytics machine at the time 🙂"><y>#</y><d>2022-12-07</d><h>15:25</h><r>curtosis</r>we had a Cray graph analytics machine at the time <b>🙂</b></z><z id="t1670432244" t="quoll Well, 1.5B triples should fit in main memory 🙂"><y>#</y><d>2022-12-07</d><h>16:57</h><r>quoll</r>Well, 1.5B triples should fit in main memory <b>🙂</b></z><z id="t1670432279" t="quoll A Cray graph machine should barely notice 🙂"><y>#</y><d>2022-12-07</d><h>16:57</h><r>quoll</r>A Cray graph machine should barely notice <b>🙂</b></z><z id="t1670432786" t="curtosis welllll…. IIRC it was complicated 🙂 . Also their triple/sparql implementation was distinctly weird, for performance reasons. (Basically everything was materialized as shared-memory pointers, so it was super fast once you loaded. Also very odd processors — slow clock but rotated through 128 thread slots with zero context switch overhead. https://en.wikipedia.org/wiki/Cray_XMT#Threadstorm4 ) Intriguing for low-level implementors, interesting performance properties for users."><y>#</y><d>2022-12-07</d><h>17:06</h><r>curtosis</r>welllll…. IIRC it was complicated  <b>🙂</b> . Also their triple/sparql implementation was distinctly weird, for performance reasons. (Basically everything was materialized as shared-memory pointers, so it was super fast once you loaded. Also very odd processors — slow clock but rotated through 128 thread slots with zero context switch overhead. <a href="https://en.wikipedia.org/wiki/Cray_XMT#Threadstorm4" target="_blank">https://en.wikipedia.org/wiki/Cray_XMT#Threadstorm4</a>) Intriguing for low-level implementors, interesting performance properties for users.</z><z id="t1670432819" t="curtosis We also had a team that was using it in non-RDF mode for some genomics work. It was fun to have access to."><y>#</y><d>2022-12-07</d><h>17:06</h><r>curtosis</r>We also had a team that was using it in non-RDF mode for some genomics work. It was fun to have access to.</z><z id="t1670433373" t="quoll I interviewed with them about working with one of these back in 2010, but opted for another opportunity instead. I was definitely curious about it"><y>#</y><d>2022-12-07</d><h>17:16</h><r>quoll</r>I interviewed with them about working with one of these back in 2010, but opted for another opportunity instead. I was definitely curious about it</z><z id="t1670433446" t="curtosis I wish you had, their early RDF implementation was terribad. 😄"><y>#</y><d>2022-12-07</d><h>17:17</h><r>curtosis</r>I wish you had, their early RDF implementation was terribad. <b>😄</b></z><z id="t1670433513" t="curtosis but I grew up with/on AllegroGraph, so that’s where I am most comfortable."><y>#</y><d>2022-12-07</d><h>17:18</h><r>curtosis</r>but I grew up with/on AllegroGraph, so that’s where I am most comfortable.</z><z id="t1670434926" t="quoll Well that makes sense, since it’s all in CL!"><y>#</y><d>2022-12-07</d><h>17:42</h><r>quoll</r>Well that makes sense, since it’s all in CL!</z><z id="t1670333637" t="rickmoynihan [:attrs {:href &quot;/_/_/users/U03BYUDJLJF&quot;}] : For me one of the biggest benefits is that RDF is property centric; where as the relational model is entity or class centric. So it’s much more flexible; you can attach a property to anything, whether you own it or not. In a relational database you need to add a new column to the entities table. I spoke about this in a presentation I gave recently called unpopular ways to make better APIs; which was really about information modelling… though I’ve just seen that the organisers of the groups have deleted their youtube account and all the recordings 😞 This is basically why it’s more suited to data mesh architectures. Decentralised is always harder than centralised. So I suspect most of the push back is due to that, rather than anything else. The slides are here: https://drive.google.com/file/d/11CgMZbLxHUxVrx-l5gk9dIah5l-TcrFH/view (though I think I shared this presentation with you before). Other benefits are reusing existing ontologies/vocabularies… depending on what you’re doing you can often just pick up and specialise models that others have developed."><y>#</y><d>2022-12-06</d><h>13:33</h><w>rickmoynihan</w><a>@bartkl</a>:

For me one of the biggest benefits is that RDF is property centric; where as the relational model is entity or class centric.  So it’s much more flexible; you can attach a property to anything, whether you own it or not.  In a relational database you need to add a new column to the entities table.  I spoke about this in a presentation I gave recently called unpopular ways to make better APIs; which was really about information modelling… though I’ve just seen that the organisers of the groups have deleted their youtube account and all the recordings <b>😞</b>

This is basically why it’s more suited to data mesh architectures.  Decentralised is always harder than centralised.  So I suspect most of the push back is due to that, rather than anything else.

The slides are here: <a href="https://drive.google.com/file/d/11CgMZbLxHUxVrx-l5gk9dIah5l-TcrFH/view" target="_blank">https://drive.google.com/file/d/11CgMZbLxHUxVrx-l5gk9dIah5l-TcrFH/view</a> (though I think I shared this presentation with you before).

Other benefits are reusing existing ontologies/vocabularies… depending on what you’re doing you can often just pick up and specialise models that others have developed.</z><z id="t1670337895" t="Bart Kleijngeld Hey Rick, thanks for sharing. Yes, I watched that presentation before, love it! The property-first perspective is what really compels me to prefer RDF as well. And indeed, we are already using all sorts of existing ontologies, such as DCAT, Dublin Core standards and QUDT. It seems to me this is a lot easier to link (it&apos;s for a reason it&apos;s called &quot;Linked Data&quot; I suppose 😉 ) than it would&apos;ve been with something like UML. Do you have any thoughts on that/experiences with that?"><y>#</y><d>2022-12-06</d><h>14:44</h><r>Bart Kleijngeld</r>Hey Rick, thanks for sharing.

Yes, I watched that presentation before, love it! The property-first perspective is what really compels me to prefer RDF as well.

And indeed, we are already using all sorts of existing ontologies, such as DCAT, Dublin Core standards and QUDT. It seems to me this is a lot easier to link (it&apos;s for a reason it&apos;s called &quot;Linked Data&quot; I suppose <b>😉</b>) than it would&apos;ve been with something like UML. Do you have any thoughts on that/experiences with that?</z><z id="t1670338128" t="rickmoynihan I’m not sure I understand the question; can you reframe it?"><y>#</y><d>2022-12-06</d><h>14:48</h><r>rickmoynihan</r>I’m not sure I understand the question; can you reframe it?</z><z id="t1670338408" t="Bart Kleijngeld Sure. I guess I&apos;m asking: do you know how one would achieve such reuse of existing models/ontologies/standards with UML? And if it&apos;s perhaps limited in what you can do there? I don&apos;t have much experience with it, and I&apos;m trying to really pin down at what point(s) RDF is simply more suited for data modeling than UML."><y>#</y><d>2022-12-06</d><h>14:53</h><r>Bart Kleijngeld</r>Sure. I guess I&apos;m asking: do you know how one would achieve such reuse of existing models/ontologies/standards with UML? And if it&apos;s perhaps limited in what you can do there? I don&apos;t have much experience with it, and I&apos;m trying to really pin down at what point(s) RDF is simply more suited for data modeling than UML.</z><z id="t1670339817" t="rickmoynihan Sure I can have a go, but I’m not really a UML expert… Also I’m not sure exactly what dimensions you’re comparing to UML; or what sense of modelling you’re referring to, when you’re comparing RDF/UML. The first thing that comes to mind is that it seems the comparisons are a bit vague and hand-wavy… i.e. UML as I’ve tended to understand it is mainly about discussing and designing systems and to some lesser extent requirements, use-cases and their constraints… essentially coming up with blueprints so an architect at the top of the pyramid can instruct people further down what they need to do to implement things. So blue prints are really communication tools, not software artifacts. Now when UML was formed, there was a consortium of companies led by the likes of Rational (now IIRC part of IBM), who wanted to take those semi-formal descriptions and use them to do code generation… e.g. convert a class diagram into a bunch of skeleton class and interface definitions. I toyed with some of those tools as a student in the late 90&apos;s, but I’ve never used them for real; nor have I ever been on a software project where that sort of workflow was used (or had any actual benefits). The main exception is the relational database schema development tools, where they are used and definitely do have some utility. So I think you might be referring to modelling in two senses: 1. a document/design tool which inspires/guides the development, but isn’t directly used. 2. something more like a schema or a real software artifact which is maintained alongside code etc. RDF can be used for either or both. It can be augmented by UML, for example many RDF vocabularies use things like class diagrams to describe their use… e.g. dcat and the rdf data cube. RDF is also obviously a data standard; where as UML is a standard notation for pictures — so in UML you can share pictures; but sharing beyond that I suspect will just be sharing rational rose files, or plant UML snippets."><y>#</y><d>2022-12-06</d><h>15:16</h><r>rickmoynihan</r>Sure I can have a go, but I’m not really a UML expert…

Also I’m not sure exactly what dimensions you’re comparing to UML; or what sense of modelling you’re referring to, when you’re comparing RDF/UML.

The first thing that comes to mind is that it seems the comparisons are a bit vague and hand-wavy…

i.e. UML as I’ve tended to understand it is mainly about discussing and designing systems and to some lesser extent requirements, use-cases and their constraints… essentially coming up with blueprints so an architect at the top of the pyramid can instruct people further down what they need to do to implement things.  So blue prints are really communication tools, not software artifacts.

Now when UML was formed, there was a consortium of companies led by the likes of Rational (now IIRC part of IBM), who wanted to take those semi-formal descriptions and use them to do code generation… e.g. convert a class diagram into a bunch of skeleton class and interface definitions.  I toyed with some of those tools as a student in the late 90&apos;s, but I’ve never used them for real; nor have I ever been on a software project where that sort of workflow was used (or had any actual benefits).  The main exception is the relational database schema development tools, where they are used and definitely do have some utility.

So I think you might be referring to modelling in two senses:

1. a document/design tool which inspires/guides the development, but isn’t directly used.
2. something more like a schema or a real software artifact which is maintained alongside code etc.
RDF can be used for either or both.  It can be augmented by UML, for example many RDF vocabularies use things like class diagrams to describe their use… e.g. dcat and the rdf data cube.

RDF is also obviously a data standard; where as UML is a standard notation for pictures — so in UML you can share pictures; but sharing beyond that I suspect will just be sharing rational rose files, or plant UML snippets.</z><z id="t1670339852" t="rickmoynihan Again I suspect the most important difference depends on how much you need centralisation/decentralisation."><y>#</y><d>2022-12-06</d><h>15:17</h><r>rickmoynihan</r>Again I suspect the most important difference depends on how much you need centralisation/decentralisation.</z><z id="t1670340050" t="rickmoynihan If you can make one schema to rule them all; then stick with a relational database. If the requirements push you to supporting many perspectives simultaneously; and extensibility etc with the ability to interoperate with some effort then RDF gives you a technology which you can hang political solutions off. For example if you want to achieve a compromise between people; and aspire to levels interoperability; vs tight integration."><y>#</y><d>2022-12-06</d><h>15:20</h><r>rickmoynihan</r>If you can make one schema to rule them all; then stick with a relational database.

If the requirements push you to supporting many perspectives simultaneously; and extensibility etc with the ability to interoperate with some effort then RDF gives you a technology which you can hang political solutions off.

For example if you want to achieve a compromise between people; and aspire to levels interoperability; vs tight integration.</z><z id="t1670340364" t="Bart Kleijngeld Sorry for being vague before. Nonetheless I&apos;m learning a lot from your response. &gt; If you can make one schema to rule them all; then stick with a relational database. This is what we don&apos;t want anymore. &gt; If the requirements push you to supporting many perspectives simultaneously; and extensibility etc with the ability to interoperate with some effort then RDF gives you a technology which you can hang political solutions off. This is what we are trying to implement. To clarify: we are a large company with the desire for a decentralized view on our data. Different departments should have some freedom to contribute to the totality of our (distributed) data model. Open world, AAA, very much Semantic Web candidate, right? 🙂 So, I&apos;m on board on why RDF is a perfect fit here. I&apos;m virtually breathing it throughout this story. What I&apos;m looking for is insights in why UML is not a good choice for this use case. I feel convinced by this, but I want to have a proper justification, which is rather hard given my experience with UML is very limited."><y>#</y><d>2022-12-06</d><h>15:26</h><r>Bart Kleijngeld</r>Sorry for being vague before. Nonetheless I&apos;m learning a lot from your response.

&gt;  If you can make one schema to rule them all; then stick with a relational database.
This is what we don&apos;t want anymore.

&gt; If the requirements push you to supporting many perspectives simultaneously; and extensibility etc with the ability to interoperate with some effort then RDF gives you a technology which you can hang political solutions off.
This is what we are trying to implement.

To clarify: we are a large company with the desire for a decentralized view on our data. Different departments should have some freedom to contribute to the totality of our (distributed) data model. Open world, AAA, very much Semantic Web candidate, right? <b>🙂</b> So, I&apos;m on board on why RDF is a perfect fit here. I&apos;m virtually breathing it throughout this story.

What I&apos;m looking for is insights in why UML is not a good choice for this use case. I feel convinced by this, but I want to have a proper justification, which is rather hard given my experience with UML is very limited.</z><z id="t1670341060" t="rickmoynihan &gt;&gt; If you can make one schema to rule them all; then stick with a relational database. &gt; This is what we don’t want anymore. &gt;&gt; … &gt; This is what we are trying to implement. Hence I suspect the backlash is around this transition — rather than RDF as such. I mean sure the RDF tools often aren’t as mature as elsewhere; but assuming this decision is the correct one, I don’t know where else you can go… You need to accept that the trade offs are worth it; and that there may be localised downsides which you’re trading for systemic wins. Also, if you need meta-schemas you might be pushed towards RDF. For example the work I’m doing (helping government connect statistics) means that you can’t dictate a single schema; as every dataset essentially requires its own schema — but the datasets can share, and we can target tooling around a meta-schema (for us the RDF data cube). &gt; What I’m looking for is insights in why UML is not a good choice for this use case. This is why I said all that stuff about UML 🙂 — for me UML is primarily just a communication diagramming tool, and it’s orthogonal to RDF/Relational etc… i.e you can still use it in combination with RDF. I suspect when people on your team say UML what they mean is some kind of entity-relationship-diagraming schema generator; that writes the DDL for them…"><y>#</y><d>2022-12-06</d><h>15:37</h><r>rickmoynihan</r>&gt;&gt; If you can make one schema to rule them all; then stick with a relational database.
&gt; This is what we don’t want anymore.
&gt;&gt; …
&gt; This is what we are trying to implement.
Hence I suspect the backlash is around this transition — rather than RDF as such.  I mean sure the RDF tools often aren’t as mature as elsewhere; but assuming this decision is the correct one, I don’t know where else you can go…  You need to accept that the trade offs are worth it; and that there may be localised downsides which you’re trading for systemic wins.

Also, if you need meta-schemas you might be pushed towards RDF.

For example the work I’m doing (helping government connect statistics) means that you can’t dictate a single schema; as every dataset essentially requires its own schema — but the datasets can share, and we can target tooling around a meta-schema (for us the RDF data cube).

&gt; What I’m looking for is insights in why UML is not a good choice for this use case.
This is why I said all that stuff about UML <b>🙂</b> — for me UML is primarily just a communication diagramming tool, and it’s orthogonal to RDF/Relational etc… i.e you can still use it in combination with RDF.

I suspect when people on your team say UML what they mean is some kind of entity-relationship-diagraming schema generator; that writes the DDL for them…</z><z id="t1670341231" t="Bart Kleijngeld Ahh right. Things are starting to make sense now. Paula also mentioned UML and relational are orthogonal, probably meaning to tell me the same thing."><y>#</y><d>2022-12-06</d><h>15:40</h><r>Bart Kleijngeld</r>Ahh right. Things are starting to make sense now. Paula also mentioned UML and relational are orthogonal, probably meaning to tell me the same thing.</z><z id="t1670341344" t="rickmoynihan Yeah; UML is huge - it’s something like 15 different types of diagram. Which bit do people mean?"><y>#</y><d>2022-12-06</d><h>15:42</h><r>rickmoynihan</r>Yeah; UML is huge - it’s something like 15 different types of diagram.  Which bit do people mean?</z><z id="t1670341452" t="Bart Kleijngeld Right, so, it&apos;s certainly clear to me that I can generate PlantUML from my RDF graphs (RDFS/OWL in particular, or any schema-providing language). It&apos;s just that people are afraid they can&apos;t use visual modeling tools anymore once we move to OWL/RDFS. Perhaps this assumption is wrong? I don&apos;t have the answer. Many of our architects are afraid to write Turtle, and love to draw diagrams in tools such as Enterprise Architect, Bizzdesign (I believe), and TopBraid&apos;s Composer"><y>#</y><d>2022-12-06</d><h>15:44</h><r>Bart Kleijngeld</r>Right, so, it&apos;s certainly clear to me that I can generate PlantUML from my RDF graphs (RDFS/OWL in particular, or any schema-providing language).

It&apos;s just that people are afraid they can&apos;t use visual modeling tools anymore once we move to OWL/RDFS. Perhaps this assumption is wrong? I don&apos;t have the answer.

Many of our architects are afraid to write Turtle, and love to draw diagrams in tools such as Enterprise Architect, Bizzdesign (I believe), and TopBraid&apos;s Composer</z><z id="t1670342479" t="Bart Kleijngeld Ah right, so I think what you were getting at is that you can do your UML modeling just fine in some visual tool, and then have it be serialized in RDF? (if the tool supports it) Of course, OWL is another story, although there seem to be mappings from UML to OWL"><y>#</y><d>2022-12-06</d><h>16:01</h><r>Bart Kleijngeld</r>Ah right, so I think what you were getting at is that you can do your UML modeling just fine in some visual tool, and then have it be serialized in RDF? (if the tool supports it) Of course, OWL is another story, although there seem to be mappings from UML to OWL</z><z id="t1670412800" t="rickmoynihan It’s almost but not quite what I’m saying, though the conclusion may be the same… I’m saying that as I understand it UML is really just a standardised notation for diagrams… so you can use UML on a whiteboard and much of it will work just fine for RDF too. Those software tools aren’t really “UML”, they’re specific tools with their own features that can draw diagrams in the UML notation. They may let you export as RDF; but I suspect most won’t. If people are attached to those specific products and features, and are used to generating code from diagrams etc, then they may need to change their expectations… though there may be some products and tools in the ecosystem that can help here, I wouldn’t really know — as I personally use primitive tools — prose, turtle and occasionally protege / pellet for this sort of thing. Ultimately though OWL and UML are very different. OWL is a description logic, and it’s about deriving entailments and satisfiability constraints. So you’d be missing out on all that if you treat UML and OWL as isomorphic to each other. That said, I’d question whether you really need OWL at all… and also whether you’re fighting the right battle. i.e. as [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] said you should persue the low hanging fruit first. So perhaps imposing OWL/RDF on people who want to use other systems isn’t the right way to go. What problems do you expect to solve? And is a more data-meshy approach not to simply let people do what they do already, with perhaps even more independence — and then only mandate that when you merge data that you interpret it as RDF via for example JSON-LD contexts etc… Then you can build generalised vocabularies for the domain that create the correspondences across localised (non RDF) vocabularies; and use that to integrate. i.e. I’d urge you not to start with OWL; but instead with simple mappings, namespaces and assignment of identifiers. Then move to common properties in shared vocabularies — and developing those."><y>#</y><d>2022-12-07</d><h>11:33</h><r>rickmoynihan</r>It’s almost but not quite what I’m saying, though the conclusion may be the same…

I’m saying that as I understand it UML is really just a standardised notation for diagrams… so you can use UML on a whiteboard and much of it will work just fine for RDF too.

Those software tools aren’t really “UML”, they’re specific tools with their own features that can draw diagrams in the UML notation.  They may let you export as RDF; but I suspect most won’t.

If people are attached to those specific products and features, and are used to generating code from diagrams etc, then they may need to change their expectations… though there may be some products and tools in the ecosystem that can help here, I wouldn’t really know — as  I personally use primitive tools — prose, turtle and occasionally protege / pellet for this sort of thing.

Ultimately though OWL and UML are very different.  OWL is a description logic, and it’s about deriving entailments and satisfiability constraints.  So you’d be missing out on all that if you treat UML and OWL as isomorphic to each other.

That said, I’d question whether you really need OWL at all… and also whether you’re fighting the right battle.

i.e. as <a>@U051N6TTC</a> said you should persue the low hanging fruit first.  So perhaps imposing OWL/RDF on people who want to use other systems isn’t the right way to go.

What problems do you expect to solve?  And is a more data-meshy approach not to simply let people do what they do already, with perhaps even more independence — and then only mandate that when you merge data that you interpret it as RDF via for example JSON-LD contexts etc…  Then you can build generalised vocabularies for the domain that create the correspondences across localised (non RDF) vocabularies; and use that to integrate.

i.e. I’d urge you not to start with OWL; but instead with simple mappings, namespaces and assignment of identifiers.

Then move to common properties in shared vocabularies — and developing those.</z><z id="t1670414246" t="rickmoynihan http://vowl.visualdataweb.org/webvowl.html"><y>#</y><d>2022-12-07</d><h>11:57</h><r>rickmoynihan</r><a href="http://vowl.visualdataweb.org/webvowl.html" target="_blank">http://vowl.visualdataweb.org/webvowl.html</a></z><z id="t1670415338" t="rickmoynihan A colleague suggested this: https://chowlk.linkeddata.es/ You can load a diagram template for OWL into googles http://draw.io ; then do your diagram there and save the diagram as XML and convert it into RDF/OWL. https://chowlk.linkeddata.es/notation.html"><y>#</y><d>2022-12-07</d><h>12:15</h><r>rickmoynihan</r>A colleague suggested this:

<a href="https://chowlk.linkeddata.es/" target="_blank">https://chowlk.linkeddata.es/</a>

You can load a diagram template for OWL into googles <a href="http://draw.io" target="_blank">http://draw.io</a>; then do your diagram there and save the diagram as XML and convert it into RDF/OWL.

<a href="https://chowlk.linkeddata.es/notation.html" target="_blank">https://chowlk.linkeddata.es/notation.html</a></z><z id="t1670418322" t="quoll This deserves another thread, but I&apos;ll say it here to keep it in context... From a practical perspective, UML does define a diagram spec, however it is actually a formal modeling language. There have also been efforts to describe OWL in UML, and more importantly, there has been quite a bit of research on automatically converting between the two. This is useful, because there is no good way to diagram out OWL descriptions except via UML. Unfortunately, while I&apos;ve seen tools that will draw OWL as UML, I only ever once saw an open source tool that could take systems modeled in UML and export them as OWL (that one was a plugin for a UML tool and became defunct nearly 20 year ago). Some commercial systems exist, but bleh. I&apos;m interested to see what might be possible with chowlk though, so thank you for that pointer!"><y>#</y><d>2022-12-07</d><h>13:05</h><r>quoll</r>This deserves another thread, but I&apos;ll say it here to keep it in context...
From a practical perspective, UML does define a diagram spec, however it is actually a formal modeling language. There have also been efforts to describe OWL in UML, and more importantly, there has been quite a bit of research on automatically converting between the two. This is useful, because there is no good way to diagram out OWL descriptions except via UML.
Unfortunately, while I&apos;ve seen tools that will draw OWL as UML, I only ever once saw an open source tool that could take systems modeled in UML and export them as OWL (that one was a plugin for a UML tool and became defunct nearly 20 year ago). Some commercial systems exist, but bleh. I&apos;m interested to see what might be possible with chowlk though, so thank you for that pointer!</z><z id="t1670418868" t="rickmoynihan Oh they’ve also just pointed me at: http://owlgred.lumii.lv/"><y>#</y><d>2022-12-07</d><h>13:14</h><r>rickmoynihan</r>Oh they’ve also just pointed me at:

<a href="http://owlgred.lumii.lv/" target="_blank">http://owlgred.lumii.lv/</a></z><z id="t1670418948" t="quoll I guess I&apos;m more interested in a UML-style diagram. Chowlk looks awesome"><y>#</y><d>2022-12-07</d><h>13:15</h><r>quoll</r>I guess I&apos;m more interested in a UML-style diagram. Chowlk looks awesome</z><z id="t1670418974" t="quoll Otherwise, people tend to prefer protege"><y>#</y><d>2022-12-07</d><h>13:16</h><r>quoll</r>Otherwise, people tend to prefer protege</z><z id="t1670419022" t="quoll Personally... I use Turtle, which I think makes me a little strange. But my friend and mentor (who has their name on many of the OWL specs) uses RDF/XML"><y>#</y><d>2022-12-07</d><h>13:17</h><r>quoll</r>Personally... I use Turtle, which I think makes me a little strange. But my friend and mentor (who has their name on many of the OWL specs) uses RDF/XML</z><z id="t1670419045" t="quoll I cringe at this every time 😖"><y>#</y><d>2022-12-07</d><h>13:17</h><r>quoll</r>I cringe at this every time <b>😖</b></z><z id="t1670419056" t="rickmoynihan RDF/XML!? 😮"><y>#</y><d>2022-12-07</d><h>13:17</h><r>rickmoynihan</r>RDF/XML!? <b>😮</b></z><z id="t1670419082" t="quoll She&apos;s a masochist, and I tell her so :rolling_on_the_floor_laughing:"><y>#</y><d>2022-12-07</d><h>13:18</h><r>quoll</r>She&apos;s a masochist, and I tell her so <b>:rolling_on_the_floor_laughing:</b></z><z id="t1670419125" t="quoll She doesn&apos;t care. She&apos;s been doing this for longer than I have, and she&apos;s [:attrs nil] at it"><y>#</y><d>2022-12-07</d><h>13:18</h><r>quoll</r>She doesn&apos;t care. She&apos;s been doing this for longer than I have, and she&apos;s <b>good</b> at it</z><z id="t1670419326" t="rickmoynihan Re: UML, Yeah I’ve always thought of it as semi-formal… for example state diagrams clearly embody the formalism of state machines. However other diagram types don’t seem to be quite so grounded… though I don’t doubt they make claims to formalism. For example class diagrams were always described as being independent of any particular language implementation; but then what does it mean when you introduce multiple inheritance; or treat classes like RDF/OWL does as set intersections, and partial orders."><y>#</y><d>2022-12-07</d><h>13:22</h><r>rickmoynihan</r>Re: UML,

Yeah I’ve always thought of it as semi-formal… for example state diagrams clearly embody the formalism of state machines.  However other diagram types don’t seem to be quite so grounded… though I don’t doubt they make claims to formalism.

For example class diagrams were always described as being independent of any particular language implementation; but then what does it mean when you introduce multiple inheritance; or treat classes like RDF/OWL does as set intersections, and partial orders.</z><z id="t1670419414" t="quoll UML can be mapped to other languages, though some features aren&apos;t available in those languages, yes"><y>#</y><d>2022-12-07</d><h>13:23</h><r>quoll</r>UML can be mapped to other languages, though some features aren&apos;t available in those languages, yes</z><z id="t1670419454" t="rickmoynihan I get you can convert between different semantics but people will naturally target a UML diagram for the langauge of their choice… or assume UML’s semantics… its use is often fuzzy and informal"><y>#</y><d>2022-12-07</d><h>13:24</h><r>rickmoynihan</r>I get you can convert between different semantics but people will naturally target a UML diagram for the langauge of their choice… or assume UML’s semantics… its use is often fuzzy and informal</z><z id="t1670419683" t="quoll UML itself is very formal in its definition. There have been several attempts to build runnable modeling systems based on it (I was contracted by SAP Research to implement OCL for one such system in 2005). I was also brought in to consult on how the MOF (Meta Object Facility) might be expressed in RDF, so I got to see a lot more of how structured it is."><y>#</y><d>2022-12-07</d><h>13:28</h><r>quoll</r>UML itself is very formal in its definition. There have been several attempts to build runnable modeling systems based on it (I was contracted by SAP Research to implement OCL for one such system in 2005). I was also brought in to consult on how the MOF (Meta Object Facility) might be expressed in RDF, so I got to see a lot more of how structured it is.</z><z id="t1670419719" t="quoll However, most people use a subset, and use it informally. The full system does not seem to be as well known"><y>#</y><d>2022-12-07</d><h>13:28</h><r>quoll</r>However, most people use a subset, and use it informally. The full system does not seem to be as well known</z><z id="t1670419748" t="quoll That&apos;s because it was designed by a bunch of computer scientists, who weren&apos;t really experts at marketing 🙂"><y>#</y><d>2022-12-07</d><h>13:29</h><r>quoll</r>That&apos;s because it was designed by a bunch of computer scientists, who weren&apos;t really experts at marketing <b>🙂</b></z><z id="t1670419755" t="quoll They still work on it today!"><y>#</y><d>2022-12-07</d><h>13:29</h><r>quoll</r>They still work on it today!</z><z id="t1670419813" t="quoll In committees formed by the wonderfully named https://omg.org/"><y>#</y><d>2022-12-07</d><h>13:30</h><r>quoll</r>In committees formed by the wonderfully named <a href="https://omg.org/" target="_blank">https://omg.org/</a></z><z id="t1670420347" t="quoll On the https://www.omg.org/spec/ under ISO Adopted Specifications, you&apos;ll see that https://www.omg.org/spec/UML/2.4.1 , though I guess it&apos;s getting on a bit now, since it was published in 2011."><y>#</y><d>2022-12-07</d><h>13:39</h><r>quoll</r>On the <a href="https://www.omg.org/spec/" target="_blank">https://www.omg.org/spec/</a> under ISO Adopted Specifications, you&apos;ll see that <a href="https://www.omg.org/spec/UML/2.4.1" target="_blank">https://www.omg.org/spec/UML/2.4.1</a>, though I guess it&apos;s getting on a bit now, since it was published in 2011.</z><z id="t1670420525" t="rickmoynihan 👍 thanks for [:attrs nil] correcting me"><y>#</y><d>2022-12-07</d><h>13:42</h><r>rickmoynihan</r><b>👍</b> thanks for <del>clarifying</del> correcting me</z><z id="t1670420746" t="quoll I actually knew none of this. I had come across UML reasonably early in my career, and for years I used it as a way of documenting class structures. I had NO IDEA there was a formal spec... right up until I was asked to implement OCL :rolling_on_the_floor_laughing: (OCL is the Object Constraint Language, and it extends UML to provide a java-like syntax for providing extra constraints that UML is not able to express on its own)"><y>#</y><d>2022-12-07</d><h>13:45</h><r>quoll</r>I actually knew none of this. I had come across UML reasonably early in my career, and for years I used it as a way of documenting class structures. I had NO IDEA there was a formal spec... right up until I was asked to implement OCL <b>:rolling_on_the_floor_laughing:</b>
(OCL is the Object Constraint Language, and it extends UML to provide a java-like syntax for providing extra constraints that UML is not able to express on its own)</z><z id="t1670420777" t="quoll And then my friend Elisa started asking me to contribute to the OMG meetings on the MOF, because of the role I&apos;d played in RDF and SPARQL"><y>#</y><d>2022-12-07</d><h>13:46</h><r>quoll</r>And then my friend Elisa started asking me to contribute to the OMG meetings on the MOF, because of the role I&apos;d played in RDF and SPARQL</z><z id="t1670420798" t="quoll (MOF = Meta Object Facility)"><y>#</y><d>2022-12-07</d><h>13:46</h><r>quoll</r>(MOF = Meta Object Facility)</z><z id="t1670420894" t="quoll The MOF provides modeling for modeling languages. i.e. it&apos;s a meta-modeling language. And it uses UML to define that."><y>#</y><d>2022-12-07</d><h>13:48</h><r>quoll</r>The MOF provides modeling for modeling languages. i.e. it&apos;s a meta-modeling language. And it uses UML to define that.</z><z id="t1670420916" t="quoll Of course, then it needs to model the UML. So there&apos;s a layer that does meta-meta-modeling (I kid you not)"><y>#</y><d>2022-12-07</d><h>13:48</h><r>quoll</r>Of course, then it needs to model the UML. So there&apos;s a layer that does meta-meta-modeling (I kid you not)</z><z id="t1670420950" t="quoll I find it all very amusing, but it&apos;s actually useful stuff"><y>#</y><d>2022-12-07</d><h>13:49</h><r>quoll</r>I find it all very amusing, but it&apos;s actually useful stuff</z><z id="t1670421083" t="rickmoynihan TIL 🙂 Is the meta circular definition done as proof of completeness or coverage, or is there utility beyond that? What do people use it for?"><y>#</y><d>2022-12-07</d><h>13:51</h><r>rickmoynihan</r>TIL <b>🙂</b>

Is the meta circular definition done as proof of completeness or coverage, or is there utility beyond that?  What do people use it for?</z><z id="t1670421134" t="rickmoynihan shudders CORBA"><y>#</y><d>2022-12-07</d><h>13:52</h><r>rickmoynihan</r>shudders CORBA</z><z id="t1670421197" t="rickmoynihan TBH I can see the utility"><y>#</y><d>2022-12-07</d><h>13:53</h><r>rickmoynihan</r>TBH I can see the utility</z><z id="t1670421223" t="quoll a) good question. I&apos;m not one of those people, so I don&apos;t fully know 🙂 b) I believe it helps design modeling languages, identifying what they can and can&apos;t do, and formalizing them. c) Personally, working with it has helped me understand modeling much better, because people often step into meta-modeling without being aware of it"><y>#</y><d>2022-12-07</d><h>13:53</h><r>quoll</r>a) good question. I&apos;m not one of those people, so I don&apos;t fully know <b>🙂</b>
b) I believe it helps design modeling languages, identifying what they can and can&apos;t do, and formalizing them.
c) Personally, working with it has helped me understand modeling much better, because people often step into meta-modeling without being aware of it</z><z id="t1670421242" t="quoll And CORBA is [:attrs nil]"><y>#</y><d>2022-12-07</d><h>13:54</h><r>quoll</r>And CORBA is <b>awesome!</b></z><z id="t1670421253" t="quoll I don&apos;t want to use it, but it&apos;s so cool 🙂"><y>#</y><d>2022-12-07</d><h>13:54</h><r>quoll</r>I don&apos;t want to use it, but it&apos;s so cool <b>🙂</b></z><z id="t1670421266" t="rickmoynihan lol"><y>#</y><d>2022-12-07</d><h>13:54</h><r>rickmoynihan</r>lol</z><z id="t1670421318" t="quoll I used to do CORBA in C++, and it was a really nice library to work with. It taught me to write my C++ libraries in a way that made it very easy for others to use (or so they said)"><y>#</y><d>2022-12-07</d><h>13:55</h><r>quoll</r>I used to do CORBA in C++, and it was a really nice library to work with. It taught me to write my C++ libraries in a way that made it very easy for others to use (or so they said)</z><z id="t1670421345" t="rickmoynihan I did use it a tiny bit once, but only enough to superficially hate it, and shallowly agree with everyone who hated it 🙂 I was into Jini back in the early days of Java though 😁"><y>#</y><d>2022-12-07</d><h>13:55</h><r>rickmoynihan</r>I did use it a tiny bit once, but only enough to superficially hate it, and shallowly agree with everyone who hated it <b>🙂</b>

I was into Jini back in the early days of Java though <b>😁</b></z><z id="t1670421373" t="rickmoynihan Which I still think was cool 🙂 and obviously that shares a lot with CORBA"><y>#</y><d>2022-12-07</d><h>13:56</h><r>rickmoynihan</r>Which I still think was cool <b>🙂</b> and obviously that shares a lot with CORBA</z><z id="t1670421385" t="rickmoynihan except you have one language to rule them all — yay Java!"><y>#</y><d>2022-12-07</d><h>13:56</h><r>rickmoynihan</r>except you have one language to rule them all — yay Java!</z><z id="t1670421435" t="quoll CORBA let you do all of that, but with ANY language!"><y>#</y><d>2022-12-07</d><h>13:57</h><r>quoll</r>CORBA let you do all of that, but with ANY language!</z><z id="t1670421447" t="quoll Which, for the day, was pretty cool."><y>#</y><d>2022-12-07</d><h>13:57</h><r>quoll</r>Which, for the day, was pretty cool.</z><z id="t1670421589" t="quoll then we had SOAP 😖 Then HTTP RPC Then REST Then all the data moved to JSON, still over HTTP (REST never really won over RPC, because almost everyone saying they did REST wasn&apos;t) Now there&apos;s GraphQL. It&apos;s become less &quot;capable&quot;, but it&apos;s become far more accessible, which is what was really needed"><y>#</y><d>2022-12-07</d><h>13:59</h><r>quoll</r>then we had SOAP <b>😖</b>
Then HTTP RPC
Then REST
Then all the data moved to JSON, still over HTTP (REST never really won over RPC, because almost everyone saying they did REST wasn&apos;t)
Now there&apos;s GraphQL.
It&apos;s become less &quot;capable&quot;, but it&apos;s become far more accessible, which is what was really needed</z><z id="t1670422991" t="rickmoynihan As I understand it not quite all of that; but yes it let you do a large part of it (the distributed object systems SOA bit). For example I don’t think corba let you do the fancy aspects of Jini; where clients could dynamically discover new services and interact with them…. In jini, you’d search for services via a java interface; and discover remote services that implement the interface, then download a client (stub) that knew how to talk to that service. It was all built on the java security sandbox… pretty cool. I agree entirely on the technologies being less capable but more accessible."><y>#</y><d>2022-12-07</d><h>14:23</h><r>rickmoynihan</r>As I understand it not quite all of that; but yes it let you do a large part of it (the distributed object systems SOA bit).

For example I don’t think corba let you do the fancy aspects of Jini; where clients could dynamically discover new services and interact with them…. In jini, you’d search for services via a java interface; and discover remote services that implement the interface, then download a client (stub) that knew how to talk to that service.  It was all built on the java security sandbox… pretty cool.

I agree entirely on the technologies being less capable but more accessible.</z><z id="t1670423807" t="Bart Kleijngeld I&apos;m not yet at the level of experience where I can entirely appreciate everything you are discussing, but it&apos;s really informative. By all means keep going 😉 . [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] Re: the level of formalism of UML: I have been told that it&apos;s poorly standardized. Do you know more about that? Apparently exporting my UML diagram in tool A and importing it in tool B can be troublesome due to that."><y>#</y><d>2022-12-07</d><h>14:36</h><r>Bart Kleijngeld</r>I&apos;m not yet at the level of experience where I can entirely appreciate everything you are discussing, but it&apos;s really informative. By all means keep going <b>😉</b>.

<a>@U051N6TTC</a> Re: the level of formalism of UML: I have been told that it&apos;s poorly standardized. Do you know more about that? Apparently exporting my UML diagram in tool A and importing it in tool B can be troublesome due to that.</z><z id="t1670425035" t="quoll That may be based on standards of serialization. The standards of the data model are highly specified."><y>#</y><d>2022-12-07</d><h>14:57</h><r>quoll</r>That may be based on standards of serialization.
The standards of the data model are highly specified.</z><z id="t1670425267" t="quoll I think that it’s covered by XMI, but I’m not entirely certain about that. The only UML “descriptions” that I know of are in graphical format. But I suspect that XMI would describe the connections, and not the sizes and locations of graphical elements. I can see there being some big gaps there. But I’ve never looked at the spec closely enough to know"><y>#</y><d>2022-12-07</d><h>15:01</h><r>quoll</r>I think that it’s covered by XMI, but I’m not entirely certain about that.
The only UML “descriptions” that I know of are in graphical format. But I suspect that XMI would describe the connections, and not the sizes and locations of graphical elements. I can see there being some big gaps there. But I’ve never looked at the spec closely enough to know</z><z id="t1670425370" t="Bart Kleijngeld Well, I just spoke with our senior data architect and he tells me the same story. That yes, XMI is used, and that the graphical stuff is excluded. So good to hear the both of you suspect the same thing. And what gaps do you mean?"><y>#</y><d>2022-12-07</d><h>15:02</h><r>Bart Kleijngeld</r>Well, I just spoke with our senior data architect and he tells me the same story. That yes, XMI is used, and that the graphical stuff is excluded. So good to hear the both of you suspect the same thing.

And what gaps do you mean?</z><z id="t1670425741" t="quoll Well, if I have a tool that lets me draw UML, I might have a few classes, with subclass relationships between them, then properties referring to classes, etc. I could then translate these objects and relationships into XMI. Then I give them to another tool that sees the XMI, understands the relationships, and then plots everything at position 0,0 on the screen, because no one said anything about where to put anything"><y>#</y><d>2022-12-07</d><h>15:09</h><r>quoll</r>Well, if I have a tool that lets me draw UML, I might have a few classes, with subclass relationships between them, then properties referring to classes, etc. I could then translate these objects and relationships into XMI. Then I give them to another tool that sees the XMI, understands the relationships, and then plots everything at position 0,0 on the screen, because no one said anything about where to put anything</z><z id="t1670427374" t="Bart Kleijngeld Ahh got it"><y>#</y><d>2022-12-07</d><h>15:36</h><r>Bart Kleijngeld</r>Ahh got it</z><z id="t1670337889" t="Eric Scott I&apos;m glad I got a chance to see your talk before it disappeared. I enjoyed it a lot. Yeah Eric Evans has written a lot about Domain-driven architecture, and uses the term &apos;ubiquitous vocabulary&apos;, where a lot of effort is put into describing your domain in detail so that work spread over several teams is all aligned and everyone is on the same page. The fact that the &apos;D&apos; in RDF is for Description really points to one of its key value propositions. You&apos;re just describing things."><y>#</y><d>2022-12-06</d><h>14:44</h><w>Eric Scott</w>I&apos;m glad I got a chance to see your talk before it disappeared. I enjoyed it a lot.

Yeah Eric Evans has written a lot about Domain-driven architecture, and uses the term &apos;ubiquitous vocabulary&apos;, where a lot of effort is put into describing your domain in detail so that work spread over several teams is all  aligned and everyone is on the same page. The fact that the &apos;D&apos; in RDF is for Description really points to one of its key value propositions. You&apos;re just describing things.</z><z id="t1670338163" t="Bart Kleijngeld Our goal seems to closely resemble what Evans describes here. And yes, the &quot;just describing things&quot; character of RDF is great. You really get to focus on expressing what things are, with lots of freedom of expressivity."><y>#</y><d>2022-12-06</d><h>14:49</h><r>Bart Kleijngeld</r>Our goal seems to closely resemble what Evans describes here. And yes, the &quot;just describing things&quot; character of RDF is great. You really get to focus on expressing what things are, with lots of freedom of expressivity.</z></g><g id="s6"><z id="t1670358378" t="curtosis Can’t believe I hadn’t joined this channel previously. Oh well, I’m here now. 🙂"><y>#</y><d>2022-12-06</d><h>20:26</h><w>curtosis</w>Can’t believe I hadn’t joined this channel previously. Oh well, I’m here now. <b>🙂</b></z><z id="t1670396156" t="Bart Kleijngeld Welcome!"><y>#</y><d>2022-12-07</d><h>06:55</h><r>Bart Kleijngeld</r>Welcome!</z><z id="t1670887342" t="quoll I’ve been frustrated with some of the external parser libraries, and I wanted to see if I could do something faster than could be generated with Instaparse, so I made a Turtle parser https://clojurians.slack.com/archives/C06MAR553/p1670887256319439"><y>#</y><d>2022-12-12</d><h>23:22</h><w>quoll</w>I’ve been frustrated with some of the external parser libraries, and I wanted to see if I could do something faster than could be generated with Instaparse, so I made a Turtle parser
<a href="https://clojurians.slack.com/archives/C06MAR553/p1670887256319439" target="_blank">https://clojurians.slack.com/archives/C06MAR553/p1670887256319439</a></z><z id="t1671613375" t="Bart Kleijngeld That&apos;s a really great name!"><y>#</y><d>2022-12-21</d><h>09:02</h><r>Bart Kleijngeld</r>That&apos;s a really great name!</z><z id="t1671627985" t="quoll How many turtles are there that you can name a Turtle parser after? I figure I had limited choices: There’s the TMNTs. The Great A’Tuin. Yertle. Harriet (a tortoise, but I include her because of her historical significance) Are there other turtles I’ve missed? Note that I’d started on a turtle parser using 2 other techniques, and these already got the names Yertle and Myrtle 😜 "><y>#</y><d>2022-12-21</d><h>13:06</h><r>quoll</r>How many turtles are there that you can name a Turtle parser after? I figure I had limited choices:
There’s the TMNTs.
The Great A’Tuin.
Yertle.
Harriet (a tortoise, but I include her because of her historical significance)
Are there other turtles I’ve missed?

Note that I’d started on a turtle parser using 2 other techniques, and these already got the names Yertle and Myrtle <b>😜</b> </z><z id="t1671628205" t="Bart Kleijngeld There&apos;s bound to be more! I love the effort you put into this. I really like &quot;Raphael&quot; because it starts with an R just like RDF. And because I grew up a as a young kid on the TMNTs 😉"><y>#</y><d>2022-12-21</d><h>13:10</h><r>Bart Kleijngeld</r>There&apos;s bound to be more! I love the effort you put into this.

I really like &quot;Raphael&quot; because it starts with an R just like RDF. And because I grew up a as a young kid on the TMNTs <b>😉</b></z><z id="t1671628759" t="quoll I turned 51 yesterday, so I was clearly influenced by that cultural phenomenon 😊 "><y>#</y><d>2022-12-21</d><h>13:19</h><r>quoll</r>I turned 51 yesterday, so I was clearly influenced by that cultural phenomenon <b>😊</b> </z><z id="t1671628810" t="Bart Kleijngeld Congratulations Paula! 🙂"><y>#</y><d>2022-12-21</d><h>13:20</h><r>Bart Kleijngeld</r>Congratulations Paula! <b>🙂</b></z><z id="t1671628843" t="quoll I ought to have come up with a better tagline for the project, but I wanted to wrap it up, so I just wrote the first thing I could think of"><y>#</y><d>2022-12-21</d><h>13:20</h><r>quoll</r>I ought to have come up with a better tagline for the project, but I wanted to wrap it up, so I just wrote the first thing I could think of</z><z id="t1671628959" t="Bart Kleijngeld Ohh yeah the half-shell one 😂 . I enjoyed it. It&apos;s clever. But yeah, maybe someone can come up with one that references something in the original TMNT series"><y>#</y><d>2022-12-21</d><h>13:22</h><r>Bart Kleijngeld</r>Ohh yeah the half-shell one <b>😂</b> . I enjoyed it. It&apos;s clever. But yeah, maybe someone can come up with one that references something in the original TMNT series</z><z id="t1671629007" t="quoll There are 2 modifications I want to make for upcoming releases (adapt for use with a stream reader, and a protocol to emit triples to, rather than accumulate in a vector), and that’ll hopefully happen in coming days. I have been thinking about it as I swim laps 🙂 (Rich uses a hammock, I use a pool)"><y>#</y><d>2022-12-21</d><h>13:23</h><r>quoll</r>There are 2 modifications I want to make for upcoming releases (adapt for use with a stream reader, and a protocol to emit triples to, rather than accumulate in a vector), and that’ll hopefully happen in coming days. I have been thinking about it as I swim laps <b>🙂</b>  (Rich uses a hammock, I use a pool)</z><z id="t1671629089" t="Bart Kleijngeld Cool! I&apos;ll be keeping a close eye on your project. I&apos;m currently using RDF4j&apos;s Rio parse from native Java. It works fine, but it&apos;s always nice to replace it with a proper Clojure lib"><y>#</y><d>2022-12-21</d><h>13:24</h><r>Bart Kleijngeld</r>Cool! I&apos;ll be keeping a close eye on your project. I&apos;m currently using RDF4j&apos;s Rio parse from native Java. It works fine, but it&apos;s always nice to replace it with a proper Clojure lib</z><z id="t1671629128" t="Bart Kleijngeld And yeah, repetitive moving is great for thinking ^^"><y>#</y><d>2022-12-21</d><h>13:25</h><r>Bart Kleijngeld</r>And yeah, repetitive moving is great for thinking ^^</z><z id="t1671629171" t="quoll A Clojure impl will never be as fast, but I think it’s more flexible, and most importantly, it’s 100% compatible with ClojureScript"><y>#</y><d>2022-12-21</d><h>13:26</h><r>quoll</r>A Clojure impl will never be as fast, but I think it’s more flexible, and most importantly, it’s 100% compatible with ClojureScript</z><z id="t1671629219" t="Bart Kleijngeld That&apos;s a major advantage indeed. And for my use case speed is not an issue anyways (information models limited to a use case don&apos;t amount to many triples)"><y>#</y><d>2022-12-21</d><h>13:26</h><r>Bart Kleijngeld</r>That&apos;s a major advantage indeed. And for my use case speed is not an issue anyways (information models limited to a use case don&apos;t amount to many triples)</z><z id="t1671629319" t="quoll As for the swimming… it’s ideal because of the increased blood flow, and also because it’s boring. Your mind is FORCED to think. It used to be tricky for me to design and count laps, but then Apple Watches started counting laps in the pool 🙂 "><y>#</y><d>2022-12-21</d><h>13:28</h><r>quoll</r>As for the swimming… it’s ideal because of the increased blood flow, and also because it’s boring. Your mind is FORCED to think. It used to be tricky for me to design and count laps, but then Apple Watches started counting laps in the pool <b>🙂</b> </z><z id="t1671629388" t="Bart Kleijngeld The boring part is absolutely essential! I agree"><y>#</y><d>2022-12-21</d><h>13:29</h><r>Bart Kleijngeld</r>The boring part is absolutely essential! I agree</z><z id="t1671629402" t="quoll I also walk, but I listen to podcasts, or else someone comes with me and we talk. But swimming forces me to think problems through "><y>#</y><d>2022-12-21</d><h>13:30</h><r>quoll</r>I also walk, but I listen to podcasts, or else someone comes with me and we talk. But swimming forces me to think problems through </z><z id="t1671629455" t="Bart Kleijngeld Walking is my boring go-to for the deep dive into my brain 🙂 . Podcasts are for during doing the dishes and taking showers for me haha"><y>#</y><d>2022-12-21</d><h>13:30</h><r>Bart Kleijngeld</r>Walking is my boring go-to for the deep dive into my brain <b>🙂</b>. Podcasts are for during doing the dishes and taking showers for me haha</z><z id="t1670887372" t="quoll I have no idea what the speed comparison is. But it was fun to build"><y>#</y><d>2022-12-12</d><h>23:22</h><w>quoll</w>I have no idea what the speed comparison is. But it was fun to build</z><z id="t1671700448" t="simongray I have some data that is newly introduced into an existing dataset. What kind of relation/attribute can I use to mark the date it was added?"><y>#</y><d>2022-12-22</d><h>09:14</h><w>simongray</w>I have some data that is newly introduced into an existing dataset. What kind of relation/attribute can I use to mark the date it was added?</z><z id="t1671703252" t="kotlenik Maybe these could fit the purpose : https://www.dublincore.org/specifications/dublin-core/dcmi-terms/terms/issued/ , https://www.dublincore.org/specifications/dublin-core/dcmi-terms/terms/modified/ . We use them for tracking resources over time in one of the projects."><y>#</y><d>2022-12-22</d><h>10:00</h><r>kotlenik</r>Maybe these could fit the purpose : <a href="https://www.dublincore.org/specifications/dublin-core/dcmi-terms/terms/issued/" target="_blank">https://www.dublincore.org/specifications/dublin-core/dcmi-terms/terms/issued/</a> , <a href="https://www.dublincore.org/specifications/dublin-core/dcmi-terms/terms/modified/" target="_blank">https://www.dublincore.org/specifications/dublin-core/dcmi-terms/terms/modified/</a> . We use them for tracking resources over time in one of the projects.</z><z id="t1671703497" t="simongray Thank you [:attrs {:href &quot;/_/_/users/U02QE89BCPR&quot;}] ! yes, I think that’ll do. I had found dc:date, but that seemed too general to me."><y>#</y><d>2022-12-22</d><h>10:04</h><r>simongray</r>Thank you <a>@U02QE89BCPR</a>! yes, I think that’ll do. I had found dc:date, but that seemed too general to me.</z><z id="t1671703704" t="simongray how do you prefer to represent the dates? Just using ISO strings or used XSD Datetime?"><y>#</y><d>2022-12-22</d><h>10:08</h><r>simongray</r>how do you prefer to represent the dates? Just using ISO strings or used XSD Datetime?</z><z id="t1671703725" t="simongray leaning towards just using strings…"><y>#</y><d>2022-12-22</d><h>10:08</h><r>simongray</r>leaning towards just using strings…</z><z id="t1671703794" t="kotlenik It made us a headaches often, so we stick always to typed literals using XSD datetime. But, if you have single store and fixed usage scenarios, you may use strings. But some day it may hurt you, so to speak."><y>#</y><d>2022-12-22</d><h>10:09</h><r>kotlenik</r>It made us a headaches often, so we stick always to typed literals using XSD datetime. But, if you have single store and fixed usage scenarios, you may use strings. But some day it may hurt you, so to speak.</z><z id="t1671703818" t="simongray Hmmm…"><y>#</y><d>2022-12-22</d><h>10:10</h><r>simongray</r>Hmmm…</z><z id="t1671703884" t="simongray the issue I have with XSD Datetime is that it is too specific. I really just need a symbolic calendar date. It’s too bad this doesn’t seem to exist for RDF."><y>#</y><d>2022-12-22</d><h>10:11</h><r>simongray</r>the issue I have with XSD Datetime is that it is too specific. I really just need a symbolic calendar date. It’s too bad this doesn’t seem to exist for RDF.</z><z id="t1671704008" t="simongray Timezones screw everything up"><y>#</y><d>2022-12-22</d><h>10:13</h><r>simongray</r>Timezones screw everything up</z><z id="t1671704126" t="kotlenik It is hard almost in every storage, because of a lot of formats people all over the world use. Not only timezones, but also the historic way people note dates. Eventually, you end up with wrapper around your store for data import making sure you have at your side unified approach. This is important if you have data coming from i.e. Asia, EU and US. It may differ significantly and is worth of effort to unify."><y>#</y><d>2022-12-22</d><h>10:15</h><r>kotlenik</r>It is hard almost in every storage, because of a lot of formats people all over the world use. Not only timezones, but also the historic way people note dates. Eventually, you end up with wrapper around your store for data import making sure you have at your side unified approach. This is important if you have data coming from i.e. Asia, EU and US. It may differ significantly and is worth of effort to unify.</z><z id="t1671704173" t="simongray 😕"><y>#</y><d>2022-12-22</d><h>10:16</h><r>simongray</r><b>😕</b></z><z id="t1671710285" t="rickmoynihan There is xsd:date too, we tend to use this and xsd:dateTime for time literals"><y>#</y><d>2022-12-22</d><h>11:58</h><r>rickmoynihan</r>There is <code>xsd:date</code> too, we tend to use this and <code>xsd:dateTime</code> for time literals</z><z id="t1671710823" t="simongray Oh, I never noticed that"><y>#</y><d>2022-12-22</d><h>12:07</h><r>simongray</r>Oh, I never noticed that</z><z id="t1671710973" t="simongray Hmmm…. Jena seems to have a weird model-dependent relationship with datatypes https://jena.apache.org/documentation/notes/typed-literals.html"><y>#</y><d>2022-12-22</d><h>12:09</h><r>simongray</r>Hmmm…. Jena seems to have a weird model-dependent relationship with datatypes <a href="https://jena.apache.org/documentation/notes/typed-literals.html" target="_blank">https://jena.apache.org/documentation/notes/typed-literals.html</a></z><z id="t1671710992" t="simongray At least programmatically"><y>#</y><d>2022-12-22</d><h>12:09</h><r>simongray</r>At least programmatically</z><z id="t1671715857" t="Eric Scott I keep thinking about a reification scheme like this: gregorian:2022\/12\/22 a gregorian:Date; rdfs:label &quot;2022-12-22&quot;^xsd:date; gregorian:duringYear gregorian:2022; gregorian:duringMonth gregorian:2022\/12; gregorian:weeklyRecurrenceOf gregorian:DayOfWeek\/Thursday; gregorian:monthyRecurrenceOf gregorian:DayOfMonth\/22; gregorian:annualRecurrenceOf gregorian:DayOfYear\/12\/22; ... . Wouldn&apos;t be that hard to publish. Would that make anyone&apos;s life easier? Has something like this already been published somewhere?"><y>#</y><d>2022-12-22</d><h>13:30</h><r>Eric Scott</r>I keep thinking about a reification scheme like this:

<pre>gregorian:2022\/12\/22 a gregorian:Date;
  rdfs:label &quot;2022-12-22&quot;^xsd:date;
  gregorian:duringYear gregorian:2022;
  gregorian:duringMonth gregorian:2022\/12;
  gregorian:weeklyRecurrenceOf gregorian:DayOfWeek\/Thursday;
  gregorian:monthyRecurrenceOf gregorian:DayOfMonth\/22;
  gregorian:annualRecurrenceOf gregorian:DayOfYear\/12\/22;
...
.</pre>
Wouldn&apos;t be that hard to publish. Would that make anyone&apos;s life easier? Has something like this already been published somewhere?</z><z id="t1671716814" t="Eric Scott Come to think of it day of week might better be numbered, then labeled with &quot;Thursday&quot;@en"><y>#</y><d>2022-12-22</d><h>13:46</h><r>Eric Scott</r>Come to think of it day of week might better be numbered, then labeled with <code>&quot;Thursday&quot;@en</code></z><z id="t1671717864" t="Eric Scott Using &apos;-&apos; instead of &apos;\/&apos; would probably be less hassle."><y>#</y><d>2022-12-22</d><h>14:04</h><r>Eric Scott</r>Using &apos;-&apos; instead of &apos;\/&apos; would probably be less hassle.</z><z id="t1671719411" t="rickmoynihan [:attrs {:href &quot;/_/_/users/UB3R8UYA1&quot;}] : yes http://reference.data.gov.uk until very recently ran a time uri service, which would generate things like that when dereferenced. It was particularly useful for things like “government years” etc. Unfortunately they sunsetted the service a few months ago, and a bunch of our gov clients use it, we’re hoping they’ll reinstate it on the original domain soon. Regardless the code that used to run it is here: https://github.com/epimorphics/IntervalServer"><y>#</y><d>2022-12-22</d><h>14:30</h><r>rickmoynihan</r><a>@UB3R8UYA1</a>: yes <a href="http://reference.data.gov.uk" target="_blank">http://reference.data.gov.uk</a> until very recently ran a time uri service, which would generate things like that when dereferenced.

It was particularly useful for things like “government years” etc.

Unfortunately they sunsetted the service a few months ago, and a bunch of our gov clients use it, we’re hoping they’ll reinstate it on the original domain soon.

Regardless the code that used to run it is here:

<a href="https://github.com/epimorphics/IntervalServer" target="_blank">https://github.com/epimorphics/IntervalServer</a></z><z id="t1671719655" t="rickmoynihan &gt; Hmmm…. Jena seems to have a weird model-dependent relationship with datatypes https://jena.apache.org/documentation/notes/typed-literals.html [:attrs {:href &quot;/_/_/users/U4P4NREBY&quot;}] what do you think is weird? The difference between lexical form and its entailment? Or not leaving datatypes open to ontology definition?"><y>#</y><d>2022-12-22</d><h>14:34</h><r>rickmoynihan</r>&gt; Hmmm…. Jena seems to have a weird model-dependent relationship with datatypes <a href="https://jena.apache.org/documentation/notes/typed-literals.html" target="_blank">https://jena.apache.org/documentation/notes/typed-literals.html</a>
<a>@U4P4NREBY</a> what do you think is weird?  The difference between lexical form and its entailment?  Or not leaving datatypes open to ontology definition?</z><z id="t1671722800" t="simongray I would just have expected that I could do (XSDDate. &quot;2022-12-22&quot;) or something similar since it seems perfectly able to independently consume that literal if it’s read in via an RDF dataset file."><y>#</y><d>2022-12-22</d><h>15:26</h><r>simongray</r>I would just have expected that I could do <code>(XSDDate. &quot;2022-12-22&quot;)</code> or something similar since it seems perfectly able to independently consume that literal if it’s read in via an RDF dataset file.</z><z id="t1671722849" t="simongray So why is it suddenly tied to the model when doing it programmatically?"><y>#</y><d>2022-12-22</d><h>15:27</h><r>simongray</r>So why is it suddenly tied to the model when doing it programmatically?</z><z id="t1671722856" t="simongray Makes no sense to me."><y>#</y><d>2022-12-22</d><h>15:27</h><r>simongray</r>Makes no sense to me.</z><z id="t1671723764" t="rickmoynihan still not sure what you mean"><y>#</y><d>2022-12-22</d><h>15:42</h><r>rickmoynihan</r>still not sure what you mean</z><z id="t1671723911" t="simongray The way I bootstrap my database is by generating a bunch of triples (represented as sets of vectors in Aristotle). I then add these triples to my database graph contained in an Apache Jena Model instance. However, if I want to add triples with XSDDate literals to my model I have to have access to the model in order to create those literals, which suddenly couples the model to those literals. It turns my neat one-way flow of bootstrapping into a directed acyclic graph since the data that is imported into the model (apparently) needs to be created in the model before it can be added to the model. However, if I consume a .ttl file with some XSDDates in it, I don’t need to know the model in advance. They are somehow able to exist independently while the ones I create programmatically are not."><y>#</y><d>2022-12-22</d><h>15:45</h><r>simongray</r>The way I bootstrap my database is by generating a bunch of triples (represented as sets of vectors in Aristotle).

I then add these triples to my database graph contained in an Apache Jena Model instance.

However, if I want to add triples with XSDDate literals to my model I have to have access to the model in order to create those literals, which suddenly couples the model to those literals. It turns my neat one-way flow of bootstrapping into a directed acyclic graph since the data that is imported into the model (apparently) needs to be created in the model before it can be added to the model.

However, if I consume a .ttl file with some XSDDates in it, I don’t need to know the model in advance. They are somehow able to exist independently while the ones I create programmatically are not.</z><z id="t1671724351" t="rickmoynihan I’m still not sure I follow 🙂 Though I don’t have as much experience with jena, so I could be missing something… but a model (as I understand it) is just an object / handle onto an in memory graph. createLiteral is essentially just a factory / constructor function for literals (which can presumably include dates). If you don’t want to couple it to the database model; then can you not just make a different one?!"><y>#</y><d>2022-12-22</d><h>15:52</h><r>rickmoynihan</r>I’m still not sure I follow <b>🙂</b>

Though I don’t have as much experience with jena, so I could be missing something… but a model (as I understand it) is just an object / handle onto an in memory graph.

<code>createLiteral</code> is essentially just a factory / constructor function for literals (which can presumably include dates).

If you don’t want to couple it to the database model; then can you not just make a different one?!</z><z id="t1671724409" t="simongray It might just be a case of createLiteral being defined as an instance method when it should really be a static method."><y>#</y><d>2022-12-22</d><h>15:53</h><r>simongray</r>It might just be a case of <code>createLiteral</code> being defined as an instance method when it should really be a static method.</z><z id="t1671724493" t="simongray Wait, there is also NodeFactory/createLiteral . Maybe the documtation is just misleading/out of date."><y>#</y><d>2022-12-22</d><h>15:54</h><r>simongray</r>Wait, there is also <code>NodeFactory/createLiteral</code> . Maybe the documtation is just misleading/out of date.</z><z id="t1671724526" t="simongray Well, in that case, there’s no issue it seems!"><y>#</y><d>2022-12-22</d><h>15:55</h><r>simongray</r>Well, in that case, there’s no issue it seems!</z><z id="t1671724609" t="rickmoynihan I agree it should really be a static method"><y>#</y><d>2022-12-22</d><h>15:56</h><r>rickmoynihan</r>I agree it should really be a static method</z><z id="t1671724653" t="rickmoynihan Jena’s api’s are a bit weird in places; I’ve always found RDF4j to be much cleaner… though not quite as fully featured."><y>#</y><d>2022-12-22</d><h>15:57</h><r>rickmoynihan</r>Jena’s api’s are a bit weird in places; I’ve always found RDF4j to be much cleaner… though not quite as fully featured.</z><z id="t1671724711" t="simongray I’m deep in OWL land, so Jena was the best fit"><y>#</y><d>2022-12-22</d><h>15:58</h><r>simongray</r>I’m deep in OWL land, so Jena was the best fit</z><z id="t1671724730" t="rickmoynihan Yeah RDF4j has a lot less support on that front"><y>#</y><d>2022-12-22</d><h>15:58</h><r>rickmoynihan</r>Yeah RDF4j has a lot less support on that front</z><z id="t1672907702" t="simongray I am dumbfounded at how difficult it seems to be to remove a triple from an Apache Jena model…"><y>#</y><d>2023-01-05</d><h>08:35</h><w>simongray</w>I am dumbfounded at how difficult it seems to be to remove a triple from an Apache Jena model…</z><z id="t1672914234" t="simongray Removing a triple from an inference graph is also an interesting endeavour. It apparently needs to be removed from both the underlying graph as well as the inference graph—and in the right order!"><y>#</y><d>2023-01-05</d><h>10:23</h><r>simongray</r>Removing a triple from an inference graph is also an interesting endeavour. It apparently needs to be removed from both the underlying graph as well as the inference graph—and in the right order!</z><z id="t1672934120" t="quoll This makes sense. Otherwise the inference graph could just create it again, right?"><y>#</y><d>2023-01-05</d><h>15:55</h><r>quoll</r>This makes sense. Otherwise the inference graph could just create it again, right?</z><z id="t1672942216" t="simongray Yeah, I guess it does. 🙂 "><y>#</y><d>2023-01-05</d><h>18:10</h><r>simongray</r>Yeah, I guess it does. <b>🙂</b> </z><z id="t1672942455" t="quoll But I get it… making sense doesn’t make it less frustrating 🙂"><y>#</y><d>2023-01-05</d><h>18:14</h><r>quoll</r>But I get it… making sense doesn’t make it less frustrating <b>🙂</b></z><z id="t1672989983" t="Bart Kleijngeld http://skosmos.org/ &gt; Open source web-based SKOS browser and publishing tool I haven&apos;t looked at this piece of software yet, but I wanted to share only because of the great name 😂 ."><y>#</y><d>2023-01-06</d><h>07:26</h><w>Bart Kleijngeld</w><a href="http://skosmos.org/" target="_blank">http://skosmos.org/</a>
&gt; Open source web-based SKOS browser and publishing tool
I haven&apos;t looked at this piece of software yet, but I wanted to share only because of the great name <b>😂</b>.</z><z id="t1672994311" t="Bart Kleijngeld Fun fact: Raphael refused to allow my blank nodes be formatted like this: :AShape sh:property [ sh:path It doesn&apos;t like the ; after xsd:int . RDF4j is fine with this, and I&apos;m fairly confident some others were too. However, the https://www.w3.org/TR/turtle/#grammar-production-predicateObjectList specifices: [7] predicateObjectList ::= verb objectList (&apos;;&apos; (verb objectList)?)* From which I gather Raphael is simply &quot;more&quot; correct 😉 ."><y>#</y><d>2023-01-06</d><h>08:38</h><w>Bart Kleijngeld</w>Fun fact: Raphael refused to allow my blank nodes be formatted like this:

<pre>:AShape sh:property [
  sh:path </pre>
It doesn&apos;t like the  <code>;</code> after <code>xsd:int</code> . RDF4j is fine with this, and I&apos;m fairly confident some others were too.

However, the <a href="https://www.w3.org/TR/turtle/#grammar-production-predicateObjectList" target="_blank">https://www.w3.org/TR/turtle/#grammar-production-predicateObjectList</a> specifices:
<pre>[7]	predicateObjectList	::=	verb objectList (&apos;;&apos; (verb objectList)?)*</pre>
From which I gather Raphael is simply &quot;more&quot; correct <b>😉</b>.</z><z id="t1673013166" t="quoll I was about to respond, “But that’s not legal syntax!” 😊 "><y>#</y><d>2023-01-06</d><h>13:52</h><w>quoll</w>I was about to respond, “But that’s not legal syntax!” <b>😊</b> </z><z id="t1673013232" t="quoll I kept comparing to other parsers and found that a number of them have odd corners like this, where they fail on unusual things that should or should not pass"><y>#</y><d>2023-01-06</d><h>13:53</h><w>quoll</w>I kept comparing to other parsers and found that a number of them have odd corners like this, where they fail on unusual things that should or should not pass</z><z id="t1673013250" t="quoll That would be an easy one to let through with a warning "><y>#</y><d>2023-01-06</d><h>13:54</h><w>quoll</w>That would be an easy one to let through with a warning </z><z id="t1673248607" t="Bart Kleijngeld I&apos;ve finished reading Semantic Web for the Working Ontologist by James Hendler, Dean Allemang and Fabien Gandon. It&apos;s a great book on the Semantic Web and how to design/maintain ontologies. If someone&apos;s interested I&apos;ve written a https://www.goodreads.com/review/show/4853908945 ."><y>#</y><d>2023-01-09</d><h>07:16</h><w>Bart Kleijngeld</w>I&apos;ve finished reading Semantic Web for the Working Ontologist by James Hendler, Dean Allemang and Fabien Gandon. It&apos;s a great book on the Semantic Web and how to design/maintain ontologies.

If someone&apos;s interested I&apos;ve written a <a href="https://www.goodreads.com/review/show/4853908945" target="_blank">https://www.goodreads.com/review/show/4853908945</a>.</z><z id="t1673371561" t="curtosis I’ve been away from the (intersection rdf clojure) world for a while … what do y’all prefer to use these days for accessing SPARQL endpoints? (AGraph at the moment, but probably need to be adaptable to e.g. Neptune)"><y>#</y><d>2023-01-10</d><h>17:26</h><w>curtosis</w>I’ve been away from the (intersection rdf clojure) world for a while … what do y’all prefer to use these days for accessing SPARQL endpoints?
(AGraph at the moment, but probably need to be adaptable to e.g. Neptune)</z><z id="t1673437213" t="rickmoynihan Not sure what you’re asking… You say “accessing SPARQL endpoints”; but seem to then mention triplestores, so do you mean hosting SPARQL endpoints?"><y>#</y><d>2023-01-11</d><h>11:40</h><r>rickmoynihan</r>Not sure what you’re asking…

You say “accessing SPARQL endpoints”; but seem to then mention triplestores, so do you mean hosting SPARQL endpoints?</z><z id="t1673371590" t="curtosis the clojure curse of “is it stable or abandoned” 😛"><y>#</y><d>2023-01-10</d><h>17:26</h><w>curtosis</w>the clojure curse of “is it stable or abandoned” <b>😛</b></z><z id="t1673372070" t="osi i appreciate the discussions on RDF here. what i’m wondering is, is this channel the most-active discussion forum for RDF at the moment, or are there other venues i should hook into? (for myself, as well as my colleagues, who i’m bringing along a RDF journey)"><y>#</y><d>2023-01-10</d><h>17:34</h><w>osi</w>i appreciate the discussions on RDF here. what i’m wondering is, is this channel the most-active discussion forum for RDF at the moment, or are there other venues i should hook into? (for myself, as well as my colleagues, who i’m bringing along a RDF journey)</z><z id="t1673437411" t="rickmoynihan I’ve no idea… but this forum is probably the most active place for the intersection of clojure and RDF. For general RDF forums I’m not really aware of any good ones (beyond our own private work channels), there are of course the various w3c mailing lists etc, but they’re typically pretty quiet"><y>#</y><d>2023-01-11</d><h>11:43</h><r>rickmoynihan</r>I’ve no idea… but this forum is probably the most active place for the intersection of clojure and RDF.

For general RDF forums I’m not really aware of any good ones (beyond our own private work channels), there are of course the various w3c mailing lists etc, but they’re typically pretty quiet</z><z id="t1673372119" t="quoll [:attrs {:href &quot;/_/_/users/UFH6TKTKK&quot;}] I don’t actually know. For the Clojure space, I think so?"><y>#</y><d>2023-01-10</d><h>17:35</h><w>quoll</w><a>@peter.royal</a> I don’t actually know. For the Clojure space, I think so?</z><z id="t1673372211" t="osi agreed on Clojure; several of the recent discussions haven’t been terribly Clojure specific, which is part of what made me wonder"><y>#</y><d>2023-01-10</d><h>17:36</h><w>osi</w>agreed on Clojure; several of the recent discussions haven’t been terribly Clojure specific, which is part of what made me wonder</z><z id="t1673439062" t="rickmoynihan The channel was created back in 2015 by joelkuiper, after he and I were DMing a lot about RDF topics, which could have easily been public; though I’m not officially an admin or anything I’d consider general RDF (non clojure) discussion totally welcome and appropriate. Particularly given that there are few other active public places to discuss RDF"><y>#</y><d>2023-01-11</d><h>12:11</h><r>rickmoynihan</r>The channel was created back in 2015 by joelkuiper, after he and I were DMing a lot about RDF topics, which could have easily  been public; though I’m not officially an admin or anything I’d consider general RDF (non clojure) discussion totally welcome and appropriate.

Particularly given that there are few other active public places to discuss RDF</z><z id="t1673457901" t="osi thanks for the historical context!"><y>#</y><d>2023-01-11</d><h>17:25</h><r>osi</r>thanks for the historical context!</z><z id="t1673372325" t="quoll Most of what I’m doing at the moment is data oriented. So I’m using Clojure for parsing data and generating TTL, or for parsing TTL (that latter one isn’t for work… yet). When I’m doing SPARQL I’m usually using a dashboard (GraphDB or Stardog) or else curl"><y>#</y><d>2023-01-10</d><h>17:38</h><w>quoll</w>Most of what I’m doing at the moment is data oriented. So I’m using Clojure for parsing data and generating TTL, or for parsing TTL (that latter one isn’t for work… yet). When I’m doing SPARQL I’m usually using a dashboard (GraphDB or Stardog) or else curl</z><z id="t1673372503" t="osi 👍 good to know! i’ve been eying stardog, but GraphDB is a new one to me i’ll check out."><y>#</y><d>2023-01-10</d><h>17:41</h><w>osi</w><b>👍</b> good to know! i’ve been eying stardog, but GraphDB is a new one to me i’ll check out.</z><z id="t1673373277" t="curtosis [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] same for me, though my usage is more creating custom dashboards so I need more control. I guess I can always just write some wrappers around the SPARQL REST interface… which has the advantage of being easily testable using curl 😛"><y>#</y><d>2023-01-10</d><h>17:54</h><w>curtosis</w><a>@quoll</a> same for me, though my usage is more creating custom dashboards so I need more control. I guess I can always just write some wrappers around the SPARQL REST interface… which has the advantage of being easily testable using curl <b>😛</b></z><z id="t1673373798" t="Eric Scott There&apos;s this: https://github.com/ont-app/sparql-endpoint"><y>#</y><d>2023-01-10</d><h>18:03</h><w>Eric Scott</w>There&apos;s this: <a href="https://github.com/ont-app/sparql-endpoint" target="_blank">https://github.com/ont-app/sparql-endpoint</a></z><z id="t1673437819" t="rickmoynihan I still maintain https://github.com/Swirrl/grafter and the API is stable because it’s used in a lot of systems. It is about to get a significant update; to the latest RDF4j 4.x line in the next few weeks. It’s probably one of the oldest clojure RDF libraries, and provides access to the I/O facilities of RDF4j, and stuff for talking to sparql endpoints etc"><y>#</y><d>2023-01-11</d><h>11:50</h><w>rickmoynihan</w>I still maintain <a href="https://github.com/Swirrl/grafter" target="_blank">https://github.com/Swirrl/grafter</a> and the API is stable because it’s used in a lot of systems.

It is about to get a significant update; to the latest RDF4j 4.x line in the next few weeks.

It’s probably one of the oldest clojure RDF libraries, and provides access to the I/O facilities of RDF4j, and stuff for talking to sparql endpoints etc</z><z id="t1673892739" t="Mathias Picker Has anyone tried to use the jena libs with GraalVM? I&apos;m trying the first time, and getting a ClassNotFoundException. I&apos;ve seldomly used graal, and never with java libraries in a clojure project, I&apos;m running against a wall here. Pure clojure projects work fine with this setup. Demo project: https://github.com/virtual-earth-de/jena-graal"><y>#</y><d>2023-01-16</d><h>18:12</h><w>Mathias Picker</w>Has anyone tried to use the jena libs with GraalVM? I&apos;m trying the first time, and getting a ClassNotFoundException. I&apos;ve seldomly used graal, and never with java libraries in a clojure project, I&apos;m running against a wall here. Pure clojure projects work fine with this setup. Demo project: <a href="https://github.com/virtual-earth-de/jena-graal" target="_blank">https://github.com/virtual-earth-de/jena-graal</a></z><z id="t1673934868" t="quoll All love to Andy, but this is one of the reasons why I reimplement these things in Clojure 🙂"><y>#</y><d>2023-01-17</d><h>05:54</h><r>quoll</r>All love to Andy, but this is one of the reasons why I reimplement these things in Clojure <b>🙂</b></z><z id="t1673934918" t="quoll one of the big reasons for Graal to fail on Java libs is when reflection gets used. I can&apos;t tell you where that&apos;s happening, but that&apos;s a first thing to look for."><y>#</y><d>2023-01-17</d><h>05:55</h><r>quoll</r>one of the big reasons for Graal to fail on Java libs is when reflection gets used. I can&apos;t tell you where that&apos;s happening, but that&apos;s a first thing to look for.</z><z id="t1673954905" t="Mathias Picker …thanks, I got over the reflection bit for the queryparser but now I&apos;m seeing a new one with xerces :) It woud be great to have a working semantic web environment in clojure - it&apos;s on my todo list to check out asami for RDF. Right now I&apos;m testing / benchmarking commercial triple store &amp; found RDFox with it&apos;s main memory implementation quite impressive. &gt; 4 billon triples in 512Gb, not too shabby. And query performance is impressive, compared to it&apos;s on-disk contenders. I&apos;m wondering how much work it would be to import test data from e.g. the berlin sparql benchmark into clojure datalog stores, maybe I&apos;ll work on that, maybe I can even use fluree&apos;s sparql parser to run the tests…"><y>#</y><d>2023-01-17</d><h>11:28</h><r>Mathias Picker</r>…thanks, I got over the reflection bit for the queryparser but now I&apos;m seeing a new one with xerces :)

It woud be great to have a working semantic web environment in clojure - it&apos;s on my todo list to check out asami for RDF. 

Right now I&apos;m testing / benchmarking commercial triple store &amp; found RDFox with it&apos;s main memory implementation quite impressive. &gt; 4 billon triples in 512Gb, not too shabby. And query performance is impressive, compared to it&apos;s on-disk contenders.

I&apos;m wondering how much work it would be to import test data from e.g. the berlin sparql benchmark into clojure datalog stores, maybe I&apos;ll work on that, maybe I can even use fluree&apos;s sparql parser to run the tests…</z><z id="t1673966730" t="Kelvin Apologies if this sounds narcissistic for promoting my own libs, but looking at your code you might want to check out https://github.com/yetanalytics/flint , the Clojure -&gt; SPARQL compiler I made."><y>#</y><d>2023-01-17</d><h>14:45</h><r>Kelvin</r>Apologies if this sounds narcissistic for promoting my own libs, but looking at your code you might want to check out <a href="https://github.com/yetanalytics/flint" target="_blank">https://github.com/yetanalytics/flint</a>, the Clojure -&gt; SPARQL compiler I made.</z><z id="t1673966816" t="Kelvin I also wrote https://github.com/yetanalytics/flint-jena , which is a version that builds Jena Query/UpdateRequest instances directly instead of strings."><y>#</y><d>2023-01-17</d><h>14:46</h><r>Kelvin</r>I also wrote <a href="https://github.com/yetanalytics/flint-jena" target="_blank">https://github.com/yetanalytics/flint-jena</a>, which is a version that builds Jena Query/UpdateRequest instances directly instead of strings.</z><z id="t1673966880" t="Kelvin I’m curious how they would actually work in GraalVM. Vanilla Flint should work just fine since that’s 100% Clojure, but flint-jena might run into issues since it calls a lot of low-level Jena methods directly"><y>#</y><d>2023-01-17</d><h>14:48</h><r>Kelvin</r>I’m curious how they would actually work in GraalVM. Vanilla Flint should work just fine since that’s 100% Clojure, but flint-jena might run into issues since it calls a lot of low-level Jena methods directly</z><z id="t1673967363" t="rickmoynihan Have you traced reflective calls with the -agentlib:native-image-agent ?"><y>#</y><d>2023-01-17</d><h>14:56</h><r>rickmoynihan</r>Have you traced reflective calls with the <code>-agentlib:native-image-agent</code>?</z><z id="t1673967499" t="rickmoynihan I’ve not done this with Jena, but have done it with RDF4j and using the agent to trace the reflective calls, generated the configs I needed just fine. IIRC it does tend to give you much more config than you need (which IIRC can bloat the binaries a bit) but you can then go through them by hand and clean up the config you don’t require easily enough."><y>#</y><d>2023-01-17</d><h>14:58</h><r>rickmoynihan</r>I’ve not done this with Jena, but have done it with RDF4j and using the agent to trace the reflective calls, generated the configs I needed just fine.

IIRC it does tend to give you much more config than you need (which IIRC can bloat the binaries a bit) but you can then go through them by hand and clean up the config you don’t require easily enough.</z><z id="t1673968486" t="quoll [:attrs {:href &quot;/_/_/users/U039VREN8F5&quot;}] Asami is not there for RDF yet. I have started on this, but other solutions will be better"><y>#</y><d>2023-01-17</d><h>15:14</h><r>quoll</r><a>@U039VREN8F5</a> Asami is not there for RDF yet. I have started on this, but other solutions will be better</z><z id="t1673968621" t="Kelvin Speaking of Asami I wish there was a “Korra” library that would allow you to use Asami with SPARQL"><y>#</y><d>2023-01-17</d><h>15:17</h><r>Kelvin</r>Speaking of Asami I wish there was a “Korra” library that would allow you to use Asami with SPARQL</z><z id="t1673968635" t="Kelvin &gt; I have started on this Though this might be a good sign"><y>#</y><d>2023-01-17</d><h>15:17</h><r>Kelvin</r>&gt; I have started on this
Though this might be a good sign</z><z id="t1673971635" t="quoll I am trying to create a transform library that takes SPARQL and generates Asami queries. Why do it that way? Because Asami queries are data structures, and the structures are executed directly"><y>#</y><d>2023-01-17</d><h>16:07</h><r>quoll</r>I am trying to create a transform library that takes SPARQL and generates Asami queries.
Why do it that way? Because Asami queries are data structures, and the structures are executed directly</z><z id="t1673971664" t="quoll Asami does not have 100% coverage of SPARQL functionality, but it has a LOT of it"><y>#</y><d>2023-01-17</d><h>16:07</h><r>quoll</r>Asami does not have 100% coverage of SPARQL functionality, but it has a LOT of it</z><z id="t1673971683" t="Kelvin Yes that sort of SPARQL -&gt; Asami compilation was exactly what I was envisioning"><y>#</y><d>2023-01-17</d><h>16:08</h><r>Kelvin</r>Yes that sort of SPARQL -&gt; Asami compilation was exactly what I was envisioning</z><z id="t1673971730" t="quoll I spend my days doing SPARQL queries now, and it seems ridiculous that I don’t have it on Asami. And embarrassing"><y>#</y><d>2023-01-17</d><h>16:08</h><r>quoll</r>I spend my days doing SPARQL queries now, and it seems ridiculous that I don’t have it on Asami. And embarrassing</z><z id="t1673973557" t="rickmoynihan That would be a pretty cute addition to asami for sure What I’d really like though would be a something to go from SPARQL to flint (though IIRC it doesn’t have 100% coverage of the SPARQL grammar) or an intermediate data representation of SPARQL. We currently rewrite SPARQL queries by parsing the AST with Jena, munging it as the AST and then printing it back to SPARQL, for execution against the database. All written in clojure; but the Jena APIs for this aren’t ideal. That said if such a thing existed I probably wouldn’t replace what we have, as it’s battle tested and works in a bunch of production settings… However if I were to do something again and were in less of a hurry I’d probably go down this route 🙂"><y>#</y><d>2023-01-17</d><h>16:39</h><r>rickmoynihan</r>That would be a pretty cute addition to asami for sure

What I’d really like though would be a something to go from SPARQL to flint (though IIRC it doesn’t have 100% coverage of the SPARQL grammar) or an intermediate data representation of SPARQL.

We currently rewrite SPARQL queries by parsing the AST with Jena, munging it as the AST and then printing it back to SPARQL, for execution against the database.  All written in clojure; but the Jena APIs for this aren’t ideal.

That said if such a thing existed I probably wouldn’t replace what we have, as it’s battle tested and works in a bunch of production settings…  However if I were to do something again and were in less of a hurry I’d probably go down this route <b>🙂</b></z><z id="t1673973670" t="Kelvin &gt; though IIRC it doesn’t have 100% coverage of the SPARQL grammar Yeah unfortunately it doesn’t, which is why I never made a SPARQL -&gt; flint parser"><y>#</y><d>2023-01-17</d><h>16:41</h><r>Kelvin</r>&gt; though IIRC it doesn’t have 100% coverage of the SPARQL grammar
Yeah unfortunately it doesn’t, which is why I never made a SPARQL -&gt; flint parser</z><z id="t1673973684" t="Kelvin I’d say it’s like 90-95% coverage though"><y>#</y><d>2023-01-17</d><h>16:41</h><r>Kelvin</r>I’d say it’s like 90-95% coverage though</z><z id="t1673973696" t="Kelvin And as an aside I would love to see Flint work with Asami"><y>#</y><d>2023-01-17</d><h>16:41</h><r>Kelvin</r>And as an aside I would love to see Flint work with Asami</z><z id="t1673973749" t="quoll Asami’s functionality has been driven by SPARQL. There were a couple of occasions when Asami implemented a query option before Datomic did. However, there are a couple of minor semantic differences between SPARQL and Datomic. Whenever that happened, I always took the SPARQL route"><y>#</y><d>2023-01-17</d><h>16:42</h><r>quoll</r>Asami’s functionality has been driven by SPARQL. There were a couple of occasions when Asami implemented a query option before Datomic did. However, there are a couple of minor semantic differences between SPARQL and Datomic. Whenever that happened, I always took the SPARQL route</z><z id="t1673973803" t="quoll (I had lots of reasons for this, but probably the real reason was laziness… I already understood the SPARQL semantics)"><y>#</y><d>2023-01-17</d><h>16:43</h><r>quoll</r>(I had lots of reasons for this, but probably the real reason was laziness… I already understood the SPARQL semantics)</z><z id="t1673974069" t="quoll Going back to the original statement that started this thread… My biggest reason for reimplementing things from scratch is platform neutrality. My company has a big investment in .net, and I don’t want to write C# 🙂"><y>#</y><d>2023-01-17</d><h>16:47</h><r>quoll</r>Going back to the original statement that started this thread…
My biggest  reason for reimplementing things from scratch is platform neutrality. My company has a big investment in .net, and I don’t want to write C# <b>🙂</b></z><z id="t1673974091" t="quoll Also, embedding in web pages can be an important use case"><y>#</y><d>2023-01-17</d><h>16:48</h><r>quoll</r>Also, embedding in web pages can be an important use case</z><z id="t1673974128" t="quoll This is also why I try to build each component as a library, rather than a monolithic project. The smaller the dependencies for a web page, the better"><y>#</y><d>2023-01-17</d><h>16:48</h><r>quoll</r>This is also why I try to build each component as a library, rather than a monolithic project. The smaller the dependencies for a web page, the better</z><z id="t1673974177" t="Kelvin Also because multiple libraries are easier to maintain than a big monolithic one"><y>#</y><d>2023-01-17</d><h>16:49</h><r>Kelvin</r>Also because multiple libraries are easier to maintain than a big monolithic one</z><z id="t1673974203" t="quoll That’s not been my experience"><y>#</y><d>2023-01-17</d><h>16:50</h><r>quoll</r>That’s not been my experience</z><z id="t1673974239" t="Kelvin Ok maybe “maintain” was the wrong word - I should say “easier to navigate each lib”"><y>#</y><d>2023-01-17</d><h>16:50</h><r>Kelvin</r>Ok maybe “maintain” was the wrong word - I should say “easier to navigate each lib”</z><z id="t1673974257" t="Kelvin I’m just basing off my experiences of having to trawl through all of Jena"><y>#</y><d>2023-01-17</d><h>16:50</h><r>Kelvin</r>I’m just basing off my experiences of having to trawl through all of Jena</z><z id="t1673974281" t="quoll That’s true. The boundaries are far clearer"><y>#</y><d>2023-01-17</d><h>16:51</h><r>quoll</r>That’s true. The boundaries are far clearer</z><z id="t1673974309" t="quoll Nothing worse than going through module A only to see it reach into the internals of something else"><y>#</y><d>2023-01-17</d><h>16:51</h><r>quoll</r>Nothing worse than going through module A only to see it reach into the internals of something else</z><z id="t1673974348" t="Kelvin Also doesn’t help that Jena is a real-life version of Enterprise FizzBuzz"><y>#</y><d>2023-01-17</d><h>16:52</h><r>Kelvin</r>Also doesn’t help that Jena is a real-life version of Enterprise FizzBuzz</z><z id="t1673974383" t="quoll I’m guessing that you didn’t see the Jena codebase circa 2002"><y>#</y><d>2023-01-17</d><h>16:53</h><r>quoll</r>I’m guessing that you didn’t see the Jena codebase circa 2002</z><z id="t1673974400" t="quoll It was… 😖"><y>#</y><d>2023-01-17</d><h>16:53</h><r>quoll</r>It was… <b>😖</b></z><z id="t1673974431" t="quoll I appreciate that they describe their queries plans in a lisp-like syntax, but that’s it"><y>#</y><d>2023-01-17</d><h>16:53</h><r>quoll</r>I appreciate that they describe their queries plans in a lisp-like syntax, but that’s it</z><z id="t1673974621" t="quoll Even as late as 2004, they did BGP matching against triples using a filter. Yes, a filter. If you inserted 1000 triples, in which you have 10 instances of my:Type , and you did a basic query of: SELECT ?y WHERE { ?x rdf:type my:Type . ?x rdf:value ?y } Then you would iterate through that data with 10,000 tests"><y>#</y><d>2023-01-17</d><h>16:57</h><r>quoll</r>Even as late as 2004, they did BGP matching against triples using a filter.
Yes, a filter.
If you inserted 1000 triples, in which you have 10 instances of <code>my:Type</code>, and you did a basic query of:
<pre>SELECT ?y
WHERE { ?x rdf:type my:Type . ?x rdf:value ?y }</pre>
Then you would iterate through that data with 10,000 tests</z><z id="t1673974713" t="rickmoynihan yeah I always much preferred the code quality and cleanliness of RDF4j to Jena… Though it’s not quite as fully featured as Jena."><y>#</y><d>2023-01-17</d><h>16:58</h><r>rickmoynihan</r>yeah I always much preferred the code quality and cleanliness of RDF4j to Jena…  Though it’s not quite as fully featured as Jena.</z><z id="t1673974752" t="quoll Actually, I guess it was 11000 tests. 1000 to match the type, and then for each of the 10 matches you’d iterate through 1000 statements"><y>#</y><d>2023-01-17</d><h>16:59</h><r>quoll</r>Actually, I guess it was 11000 tests. 1000 to match the type, and then for each of the 10 matches you’d iterate through 1000 statements</z><z id="t1673974785" t="quoll Sesame was much, much better. Hmmm, I wonder what happened to them? I should ask"><y>#</y><d>2023-01-17</d><h>16:59</h><r>quoll</r>Sesame was much, much better. Hmmm, I wonder what happened to them? I should ask</z><z id="t1673974801" t="rickmoynihan Sesame was renamed RDF4j when it moved to eclipse"><y>#</y><d>2023-01-17</d><h>17:00</h><r>rickmoynihan</r>Sesame was renamed RDF4j when it moved to eclipse</z><z id="t1673974860" t="rickmoynihan it’s still basically the same — though they repackaged all the classes under org.eclipse and have made a few breaking changes along the way"><y>#</y><d>2023-01-17</d><h>17:01</h><r>rickmoynihan</r>it’s still basically the same — though they repackaged all the classes under <code>org.eclipse</code> and have made a few breaking changes along the way</z><z id="t1673974918" t="rickmoynihan Jeen Broekstra is still very active"><y>#</y><d>2023-01-17</d><h>17:01</h><r>rickmoynihan</r>Jeen Broekstra is still very active</z><z id="t1673974957" t="rickmoynihan It is of course still a huge java project with lots of modules etc"><y>#</y><d>2023-01-17</d><h>17:02</h><r>rickmoynihan</r>It is of course still a huge java project with lots of modules etc</z><z id="t1674468512" t="simongray Hey SPARQL people, I need to find all resources of a specific type that share the same set of predicate-objects, i.e. they are identical. I have no real idea how to construct a query to return that…"><y>#</y><d>2023-01-23</d><h>10:08</h><w>simongray</w>Hey SPARQL people, I need to find all resources of a specific type that share the same set of predicate-objects, i.e. they are identical. I have no real idea how to construct a query to return that…</z><z id="t1674468632" t="simongray The goal is to remove duplicates"><y>#</y><d>2023-01-23</d><h>10:10</h><w>simongray</w>The goal is to remove duplicates</z><z id="t1674470409" t="simongray Currently just matching on some attributes and trying to reduce the result set in Clojure. but would be nice to have a single, generalised query for finding duplicates."><y>#</y><d>2023-01-23</d><h>10:40</h><w>simongray</w>Currently just matching on some attributes and trying to reduce the result set in Clojure. but would be nice to have a single, generalised query for finding duplicates.</z><z id="t1674485577" t="Kelvin Would SELECT DISTINCT be helpful here?"><y>#</y><d>2023-01-23</d><h>14:52</h><r>Kelvin</r>Would <code>SELECT DISTINCT</code> be helpful here?</z><z id="t1674485679" t="Kelvin i.e. if you’re trying to remove dupes (as opposed to specifically querying for dupes) of a certain type, would the following query be helpful? SELECT DISTINCT ?s WHERE { ?s a my:Type }"><y>#</y><d>2023-01-23</d><h>14:54</h><r>Kelvin</r>i.e. if you’re trying to remove dupes (as opposed to specifically querying for dupes) of a certain type, would the following query be helpful?
<pre>SELECT DISTINCT ?s
WHERE {
  ?s a my:Type
}</pre></z><z id="t1674485784" t="Kelvin If on the other hand you’re trying to specifically find duplicates I think you need to use the COUNT aggregate and use filters to return when the count &gt; 1"><y>#</y><d>2023-01-23</d><h>14:56</h><r>Kelvin</r>If on the other hand you’re trying to specifically find duplicates I think you need to use the <code>COUNT</code> aggregate and use filters to return when the count &gt; 1</z><z id="t1674486188" t="Kelvin Ah I think HAVING would be your friend here (not sure if this query actually works but could be worth a shot): SELECT ?s WHERE { ?s a my:Type ?s ?p ?o } GROUP BY ?p ?o HAVING (COUNT(?s) &gt; 1)"><y>#</y><d>2023-01-23</d><h>15:03</h><r>Kelvin</r>Ah I think <code>HAVING</code> would be your friend here (not sure if this query actually works but could be worth a shot):
<pre>SELECT ?s
WHERE {
  ?s a my:Type
  ?s ?p ?o
}
GROUP BY ?p ?o
HAVING (COUNT(?s) &gt; 1)</pre></z><z id="t1674548135" t="simongray Thanks, [:attrs {:href &quot;/_/_/users/U02FU7RMG8M&quot;}] !"><y>#</y><d>2023-01-24</d><h>08:15</h><r>simongray</r>Thanks, <a>@U02FU7RMG8M</a>!</z><z id="t1674510988" t="quoll I don’t have data to play with here, but I’m thinking you could do something like this (in thread)…"><y>#</y><d>2023-01-23</d><h>21:56</h><w>quoll</w>I don’t have data to play with here, but I’m thinking you could do something like this (in thread)…</z><z id="t1674510997" t="quoll select ?s where{ ?s a mytype . ?s ?p ?o . ?s2 a mytype . ?s2 ?p ?o . FILTER (?s != ?s2) MINUS{ ?s ?px ?ox . NOT EXISTS { ?s2 ?px ?ox } } }"><y>#</y><d>2023-01-23</d><h>21:56</h><r>quoll</r><pre>select ?s
where{
  ?s a mytype .
  ?s ?p ?o .
  ?s2 a mytype .
  ?s2 ?p ?o .
  FILTER (?s != ?s2)
  MINUS{
    ?s ?px ?ox .
    NOT EXISTS { ?s2 ?px ?ox }
  } 
}</pre></z><z id="t1674511116" t="quoll This matches ?s and ?s2 where they are both mytype and they share any properties at all. Then remove any instance of ?s where there is a property/value and the matching ?s2 does not have that property/value"><y>#</y><d>2023-01-23</d><h>21:58</h><r>quoll</r>This matches <code>?s</code> and <code>?s2</code> where they are both <code>mytype</code> and they share any properties at all.
Then remove any instance of <code>?s</code> where there is a property/value and the matching  <code>?s2</code> does not have that property/value</z><z id="t1674511331" t="quoll if ?s has fewer properties than ?s2 then it will be taken away via the MINUS, but then when the bindings are the other way around ?s will have more properties than ?s2 and so it won’t be removed via the minus. Which I think works here"><y>#</y><d>2023-01-23</d><h>22:02</h><r>quoll</r>if <code>?s</code> has fewer properties than <code>?s2</code> then it will be taken away via the MINUS, but then when the bindings are the other way around <code>?s</code> will have more properties than <code>?s2</code> and so it won’t be removed via the minus. Which I think works here</z><z id="t1674511372" t="quoll The problem is that the above works via a nested loop in the query engine. It doesn’t scale"><y>#</y><d>2023-01-23</d><h>22:02</h><r>quoll</r>The problem is that the above works via a nested loop in the query engine. It doesn’t scale</z><z id="t1674548160" t="simongray Yeah…"><y>#</y><d>2023-01-24</d><h>08:16</h><r>simongray</r>Yeah…</z><z id="t1674548175" t="simongray Thanks [:attrs {:href &quot;/_/_/users/U051N6TTC&quot;}] !"><y>#</y><d>2023-01-24</d><h>08:16</h><r>simongray</r>Thanks <a>@U051N6TTC</a>!</z><z id="t1674744618" t="cldwalker Mornin. Sharing a script that converts a subset of a datascript db to rdf via a friendly forked #nbb (node and cljs) in case folks are interested - https://github.com/logseq/nbb-logseq/tree/main/examples/linked-data"><y>#</y><d>2023-01-26</d><h>14:50</h><w>cldwalker</w>Mornin. Sharing a script that converts a subset of a datascript db to rdf via a friendly forked #nbb (node and cljs) in case folks are interested - <a href="https://github.com/logseq/nbb-logseq/tree/main/examples/linked-data" target="_blank">https://github.com/logseq/nbb-logseq/tree/main/examples/linked-data</a></z><z id="t1674744675" t="cldwalker nbb-logseq is the name of the friendly fork that adds datascript to nbb"><y>#</y><d>2023-01-26</d><h>14:51</h><r>cldwalker</r>nbb-logseq is the name of the friendly fork that adds datascript to nbb</z><z id="t1674809270" t="simongray I really like that you&apos;re leaning into the triplestore aspect of logseq. Other than the local first, open source qualities of the tool, it is my favourite reason to use it over all of the other solutions in this space."><y>#</y><d>2023-01-27</d><h>08:47</h><r>simongray</r>I really like that you&apos;re leaning into the triplestore aspect of logseq. Other than the local first, open source qualities of the tool, it is my favourite reason to use it over all of the other solutions in this space.</z><z id="t1674833274" t="cldwalker Glad to hear. We plan to make triples even easier with tags like https://tana.inc/ has done. Their &quot;supertag&quot; is just a #tag that behaves like a type"><y>#</y><d>2023-01-27</d><h>15:27</h><r>cldwalker</r>Glad to hear. We plan to make triples even easier with tags like <a href="https://tana.inc/" target="_blank">https://tana.inc/</a> has done. Their &quot;supertag&quot; is just a <code>#tag</code> that behaves like a type</z><z id="t1674831997" t="Kelvin Working with Jena again…is there no way to turn an XSDDateTime (or another other XSD date/time instance) into a Java Instant?"><y>#</y><d>2023-01-27</d><h>15:06</h><w>Kelvin</w>Working with Jena again…is there no way to turn an XSDDateTime (or another other XSD date/time instance) into a Java Instant?</z><z id="t1674833933" t="Kelvin At least not directly…you can do it this roundabout way: (.toInstant (.asCalendar date-time))"><y>#</y><d>2023-01-27</d><h>15:38</h><r>Kelvin</r>At least not directly…you can do it this roundabout way:
<pre>(.toInstant (.asCalendar date-time))</pre></z><z id="t1674833938" t="Kelvin But it seems like an oversight regardless"><y>#</y><d>2023-01-27</d><h>15:38</h><r>Kelvin</r>But it seems like an oversight regardless</z><z id="t1674837854" t="quoll It’s the Java way 🙂"><y>#</y><d>2023-01-27</d><h>16:44</h><r>quoll</r>It’s the Java way <b>🙂</b></z><z id="t1674845879" t="Eric Scott Shouldn&apos;t there be some kind of factory class here?"><y>#</y><d>2023-01-27</d><h>18:57</h><r>Eric Scott</r>Shouldn&apos;t there be some kind of factory class here?</z><z id="t1674845972" t="Kelvin Eh you can assume that I defined date-time elsewhere"><y>#</y><d>2023-01-27</d><h>18:59</h><r>Kelvin</r>Eh you can assume that I defined <code>date-time</code> elsewhere</z><z id="t1674846020" t="Eric Scott Sorry, should&apos;ve marked that as a joke. 🙂"><y>#</y><d>2023-01-27</d><h>19:00</h><r>Eric Scott</r>Sorry, should&apos;ve marked that as a joke. <b>🙂</b></z><z id="t1674846700" t="quoll &gt; Shouldn’t there be some kind of factory class here? You get that from the FactoryFactory class"><y>#</y><d>2023-01-27</d><h>19:11</h><r>quoll</r>&gt; Shouldn’t there be some kind of factory class here?
You get that from the FactoryFactory class</z><z id="t1675021582" t="curtosis You could also just reify AbstractFactoryAbstractFactory."><y>#</y><d>2023-01-29</d><h>19:46</h><r>curtosis</r>You could also just reify AbstractFactoryAbstractFactory.</z><z id="t1675087562" t="Kelvin …and that’s how Enterprise FizzBuzz is born"><y>#</y><d>2023-01-30</d><h>14:06</h><r>Kelvin</r>…and that’s how Enterprise FizzBuzz is born</z><z id="t1675090528" t="quoll I feel like I’m being nerd sniped into an implementation that uses a FizzBuzzFactoryFactory to dispatch to either a FizzFactory, a BuzzFactory, or a FizzBuzzFactory, which then instantiates a FizzInterface, a BuzzInterface, or an extension of both that implements both interfaces, and then calls the parent AbstractFizz or AbstractBuzz…"><y>#</y><d>2023-01-30</d><h>14:55</h><r>quoll</r>I feel like I’m being nerd sniped into an implementation that uses a FizzBuzzFactoryFactory to dispatch to either a FizzFactory, a BuzzFactory, or a FizzBuzzFactory, which then instantiates a FizzInterface, a BuzzInterface, or an extension of both that implements both interfaces, and then calls the parent AbstractFizz or AbstractBuzz…</z><z id="t1675100596" t="Eric Scott https://clojurians.slack.com/archives/C09GHBXRC/p1674837854959379?thread_ts=1674831997.613949&amp;amp;cid=C09GHBXRC"><y>#</y><d>2023-01-30</d><h>17:43</h><r>Eric Scott</r><a href="https://clojurians.slack.com/archives/C09GHBXRC/p1674837854959379?thread_ts=1674831997.613949&amp;amp;cid=C09GHBXRC" target="_blank">https://clojurians.slack.com/archives/C09GHBXRC/p1674837854959379?thread_ts=1674831997.613949&amp;amp;cid=C09GHBXRC</a></z><z id="t1675101033" t="Bart Kleijngeld I&apos;ve been gone for a few weeks, and I just wanted to share that I&apos;m really happy with the activity here 🙂 . Interesting conversations, too!"><y>#</y><d>2023-01-30</d><h>17:50</h><w>Bart Kleijngeld</w>I&apos;ve been gone for a few weeks, and I just wanted to share that I&apos;m really happy with the activity here <b>🙂</b>. Interesting conversations, too!</z><z id="t1675982124" t="Eric Scott • Hi all - I&apos;ve had a chance to go back and tighten up the screws on ont-app/igraph-jena and its supporting library ont-app/rdf (BTW I should mention I&apos;m looking for work). Thanks to [:attrs {:href &quot;/_/_/users/U4P4NREBY&quot;}] for his contribution. There is now better testing, and the rdf module has better i/o support. There&apos;s a breaking change (sorry). • What before was called read-rdf is now load-rdf to reflect the terminology in Jena. This creates a new graph from an RDF file. • read-rdf now, like jena, reads a file into an existing graph https://github.com/ont-app/igraph-jena https://github.com/ont-app/rdf"><y>#</y><d>2023-02-09</d><h>22:35</h><w>Eric Scott</w>• Hi all -
I&apos;ve had a chance to go back and tighten up the screws on ont-app/igraph-jena and its
supporting library ont-app/rdf (BTW I should mention I&apos;m looking for work).

Thanks to <a>@simongray</a> for his contribution.

There is now better testing, and the rdf module has better i/o support.

There&apos;s a breaking change (sorry).
• What before was called <code>read-rdf</code> is now <code>load-rdf</code> to reflect the terminology in Jena. This creates a new graph from an RDF file.
• <code>read-rdf</code> now, like jena, reads a file into an existing graph
<a href="https://github.com/ont-app/igraph-jena" target="_blank">https://github.com/ont-app/igraph-jena</a>
<a href="https://github.com/ont-app/rdf" target="_blank">https://github.com/ont-app/rdf</a></z><z id="t1676138753" t="Eric Scott New release (0.2.0) of https://github.com/ont-app/sparql-endpoint project.clj -&gt; deps.edn Added tests for update predicated on ONT_APP_TEST_UPDATE_ENDPOINT envar"><y>#</y><d>2023-02-11</d><h>18:05</h><w>Eric Scott</w>New release (0.2.0) of <a href="https://github.com/ont-app/sparql-endpoint" target="_blank">https://github.com/ont-app/sparql-endpoint</a>


    project.clj -&gt; deps.edn
    Added tests for update predicated on ONT_APP_TEST_UPDATE_ENDPOINT envar</z></g></div></body>